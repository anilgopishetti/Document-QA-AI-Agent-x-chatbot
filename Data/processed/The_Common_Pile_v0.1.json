{
  "doc_id": "4f1ba2cc-f474-4745-9646-6e67a29f7887",
  "filename": "The_Common_Pile_v0.1.pdf",
  "title": "The Common Pile v0.1: An 8TB Dataset of Public Domain and Openly Licensed Text",
  "metadata": {
    "title": "The Common Pile v0.1: An 8TB Dataset of Public Domain and Openly Licensed Text",
    "author": "Nikhil Kandpal; Brian Lester; Colin Raffel; Sebastian Majstorovic; Stella Biderman; Baber Abbasi; Luca Soldaini; Enrico Shippole; A. Feder Cooper; Aviya Skowron; John Kirchenbauer; Shayne Longpre; Lintang Sutawika; Alon Albalak; Zhenlin Xu; Guilherme Penedo; Loubna Ben Allal; Elie Bakouch; John David Pressman; Honglu Fan; Dashiell Stander; Guangyu Song; Aaron Gokaslan; Tom Goldstein; Brian R. Bartoldson; Bhavya Kailkhura; Tyler Murray",
    "subject": "",
    "producer": "pikepdf 8.15.1",
    "creationDate": "",
    "modDate": "",
    "num_pages": 55
  },
  "sections": [
    {
      "section_id": "51cefad4-ea1b-450b-be15-7720ab321c53",
      "heading": "arXiv:2506.05209v1  [cs.CL]  5 Jun 2025",
      "text": "",
      "start_page": 1,
      "end_page": 1
    },
    {
      "section_id": "88118c3f-6c51-4948-8d18-89a9f6897060",
      "heading": "The Common Pile v0.1: An 8TB Dataset of PublicDomain and Openly Licensed Text",
      "text": "Nikhil Kandpal∗1,2Brian Lester∗1,2Colin Raffel∗1,2,3Sebastian Majstorovic4Stella Biderman4Baber Abbasi4Luca Soldaini5Enrico Shippole6A. Feder Cooper†7Aviya Skowron4John Kirchenbauer8Shayne Longpre9LintangSutawika4,10Alon Albalak‡11Zhenlin Xu12Guilherme Penedo3Loubna Ben Allal3ElieBakouch3John David Pressman4Honglu Fan4,13Dashiell Stander4Guangyu Song4AaronGokaslan7Tom Goldstein8Brian R. Bartoldson14Bhavya Kailkhura14Tyler Murray5\n\n1University of Toronto2Vector Institute3Hugging Face4EleutherAI5The Allen Institute forArtificial Intelligence6Teraflop AI7Cornell University8University of Maryland, College Park9MIT10CMU11Lila Sciences12Independent13poolside14Lawrence Livermore NationalLaboratory",
      "start_page": 1,
      "end_page": 1
    },
    {
      "section_id": "93cb46d5-c273-492e-afd8-906b2135a613",
      "heading": "Abstract",
      "text": "",
      "start_page": 1,
      "end_page": 1
    },
    {
      "section_id": "ae5a0220-913b-4d88-8fe2-cee41e91abc3",
      "heading": "Large language models (LLMs) are typically trained on enormous quantities ofunlicensed text, a practice that has led to scrutiny due to possible intellectualproperty infringement and ethical concerns. Training LLMs on openly licensedtext presents a first step towards addressing these issues, but prior data collectionefforts have yielded datasets too small or low-quality to produce performant LLMs.To address this gap, we collect, curate, and release the Common Pile v0.1, aneight terabyte collection of openly licensed text designed for LLM pretraining.The Common Pile comprises content from 30 sources that span diverse domainsincluding research papers, code, books, encyclopedias, educational materials,audio transcripts, and more. Crucially, we validate our efforts by training two7 billion parameter LLMs on text from the Common Pile: Comma v0.1-1T andComma v0.1-2T, trained on 1 and 2 trillion tokens respectively. Both modelsattain competitive performance to LLMs trained on unlicensed text with similarcomputational budgets, such as Llama 1 and 2 7B. In addition to releasing theCommon Pile v0.1 itself, we also release the code used in its creation as well asthe training mixture and checkpoints for the Comma v0.1 models.",
      "text": "",
      "start_page": 1,
      "end_page": 1
    },
    {
      "section_id": "b0653c0f-8a6a-46ad-a995-676d2739f58d",
      "heading": "1Introduction",
      "text": "A critical stage of large language model (LLM) development is pretraining [72, 136, 142], where anLLM is trained to predict the next token (i.e., word or subword unit) in a corpus of unstructured text.Pretraining is widely regarded as the foundation for strong downstream performance, as it enablesLLMs to learn the structure of natural language [32, 110, 154] and accumulate a broad base of worldknowledge [133, 152]. In an effort to push the capabilities of LLMs, pre-training datasets have grownsteadily over time [143], with modern datasets containing trillions of tokens [132, 167, 193]. To meetthis increasing demand for pre-training data, the de facto approach has been to leverage the publicInternet as a source of text [57, 95, 108, 132, 142].\n\nWhile the web provides a diverse and continuously growing supply of text, much of this content—under most legal frameworks—is protected by copyright. Yet, this text is routinely used to pretrain\n\n∗Equal contribution. For a list of author contributions, see Appendix A. †Work done while a graduatestudent at Cornell University. ‡Work done while at SynthLabs.\n\nPreprint.\n\nStack V2\n\nPEPs\n\nUSPTO\n\nCAP\n\nUSGPO\n\nUK Hansard\n\nRegulations.gov\n\nWikiteam\n\nWikimedia\n\nCCCC\n\nNews\n\nFoodista\n\nPDR\n\npeS2o\n\nPubMed\n\nArXiv Papers\n\nArXiv Abstracts\n\nStack Exchange\n\nGitHub Archive\n\nUbuntu IRC\n\nBHL\n\nPre-1929 Books\n\nLibrary of Congress\n\nProject Gutenberg\n\nCC YouTube\n\nDPI\n\nDOAB\n\nPressBooks\n\nLibreTexts\n\nOER Commons\n\n1MB\n\n100MB\n\n10GB\n\n1TB\n\nDataset Size\n\nCode(4775 GB) Government & Legal\n\n(1172 GB)Wikis(528 GB)Web(260 GB)\n\nAcademic\n\nPapers(370 GB)Online Forums\n\n(165 GB)\n\nPublic Domain\n\nBooks(244 GB)Other(29 GB)\n\nEducational\n\nResources\n\n(15 GB)\n\nFigure 1: The Common Pile is an 8TB dataset of openly licensed text curated from 30 diversesources. The sources comprising the Common Pile are shown above, categorized by textual domain.",
      "start_page": 1,
      "end_page": 2
    },
    {
      "section_id": "0ae43c48-1154-4d84-8367-f03b1051039b",
      "heading": "LLMs, often without compensation to the creators of this content. Recent estimates suggest thatcompensating the authors of pre-training data, even at conservatively low wage rates, would costbillions of US dollars [82]. While copyright exemptions for text and data mining exist in somejurisdictions [69, 79, 92, 130, 156], many rights holders have objected to the uncompensated use oftheir work, resulting in numerous lawsuits against LLM developers [24, 191] that could carry financialdamages in the billions [40, 96, 159]. Beyond questions of intellectual property (IP) law, the use ofweb-scraped data also raises ethical concerns [9], as content creators rarely explicitly consent to thedownstream use of their work for LLM training. In fact, recent evidence suggests that many contentowners may not consent to its use as LLM training data, as shown by a sharp mid-2023 increase inwebsites blocking AI crawlers [107], following growing awareness of web data being used to trainmodels. Finally, while open models trained on publicly released pre-training datasets [18, 64, 103]support research into the study of learning dynamics [50, 76, 84], memorization [17, 22], dataauditing [47, 128, 145], and more, the use of unlicensed training data heavily limits the ability ofmodel trainers to share their datasets, and has previously resulted in DMCA takedowns of datasetssuch as the Pile [57].",
      "text": "",
      "start_page": 2,
      "end_page": 2
    },
    {
      "section_id": "217a04f5-3552-4ab7-92d2-59acc0c9a731",
      "heading": "The current landscape reflects a growing divide between LLM developers and content creators. Wesubmit that a natural first step toward resolving this tension is to ask: Is it possible to train performantlanguage models using only public domain and openly licensed text? We define “openly licensed”text as content that follows the Open Knowledge Foundation’s Open Definition 2.1 (further detailedin section 2 and Appendix C), which refers to content where the copyright holder has granted explicitpermission for the content to be freely accessed, used, modified, and shared for any purpose. Ourprimary contribution in this paper is to demonstrate that this is indeed possible by collecting, curating,and releasing the Common Pile v0.1, an 8TB dataset that—to our knowledge—constitutes the largestcollection of openly licensed text to date. The Common Pile comprises 30 text sources (detailed insection 3), covering diverse domains including research publications, open-source code, governmentdocuments, historical books, educational resources, audio transcripts, and more. Crucially, wedemonstrate that after appropriate filtering, deduplication, and reweighting, the Common Pile v0.1can be used as the foundation for competitive LLMs. Specifically, we train Comma v0.1-1T andComma v0.1-2T, a pair of 7-billion-parameter models with comparable performance to budget-matched models trained on unlicensed datasets such as Llama 1 and 2 7B. In the spirit of opennessand transparency, we release the Common Pile v0.1, both Comma v0.1 models and their filtered anddeduplicated pre-training dataset, and all data collection and processing code.",
      "text": "",
      "start_page": 2,
      "end_page": 2
    },
    {
      "section_id": "b1a84f6b-8da2-4cdf-9401-f272effa2aec",
      "heading": "2What do we mean by “openly licensed”?",
      "text": "Copyright law grants content creators certain rights, such as exclusive rights (with certain exceptions)to reproduce, distribute, and create derivatives of their original works. Although copyright laws varyacross jurisdictions, original, creative works (that are “fixed” in a tangible medium, such as physicallyor digitally [see, e.g., 1]) typically fall within the scope of copyright. Works in the public domain [38]have had their copyrights expire (after a legally dictated time period), were never eligible for copyrightprotection due to specific carve-outs (e.g., government documents in the U.S. [2]), or were otherwisededicated to the public domain by their copyright owners (e.g., with a CC0 license [35]). Copyright\n\n2",
      "start_page": 2,
      "end_page": 2
    },
    {
      "section_id": "abd8cf64-8794-40c5-a0c1-d7dda7fcff69",
      "heading": "owners can license their protected works, allowing others to adapt and reuse them under specifiedterms. For example, Creative Commons (CC) Licenses (except CC0) grant the right to “reproduceand Share the Licensed Material, in whole or in part; and produce, reproduce, and Share AdaptedMaterial” [36]. For a more in-depth and accessible discussion about licenses and generative AI, seeLee et al. [96, Parts II.I–II.J].",
      "text": "",
      "start_page": 3,
      "end_page": 3
    },
    {
      "section_id": "e8ab2c56-debf-4db1-b862-8327b58fec1a",
      "heading": "For the Common Pile, we collect and curate public domain and openly licensed text, where weconsider “openly licensed” to mean any license that meets the Open Knowledge Foundation’s OpenDefinition 2.1. Some prominent examples of licenses that are considered to be “open” under thisdefinition include CC BY [37], CC BY-SA [39], and software licenses certified by the Blue OakCouncil (e.g., the MIT license) [20]. We note that CC NC (non-commercial) and ND (no derivatives)licenses are not considered open under this definition and we therefore do not include contentdistributed under these licenses. While the use of an open license does not necessarily imply that therights holder has specifically contemplated use of their content to train LLMs, most open licensesinclude text like “the above rights may be exercised in all media and formats whether now known orhereafter devised” [37]. Overall, we consider our use of openly licensed data to be a substantial firststep towards ethical pre-training dataset curation.",
      "text": "2.1License due diligence",
      "start_page": 3,
      "end_page": 3
    },
    {
      "section_id": "6916fb56-759d-455a-aee5-8ac7a699b69e",
      "heading": "License launderingThere is a large quantity of data on the internet with incorrect, ambiguous, ormissing licensing metadata [96, 106]. A common pitfall is “license laundering,” where a copyrightedwork is redistributed (typically by a non-rights holder) with an incorrect license. License launderingcan undermine our ability to confidently source openly licensed content since it implies that wecannot always trust the license distributed with a piece of content. To address this issue, we setstrict standards for data sourcing, only including data from sources where we were confident thatthe licensing information was provided by the copyright holder, which ultimately led us to excludecertain sources such as OpenAlex [80, 126], YouTube Commons [74], and the Hacker News dataseton Kaggle.",
      "text": "",
      "start_page": 3,
      "end_page": 3
    },
    {
      "section_id": "1e58eb9d-b933-4962-ac97-851aa932b384",
      "heading": "Use of collection licensesA related issue is the licensing status of compilations of existing works.Many training corpora are released under open licenses, but these licenses do not necessarily alignwith the licensing status of the underlying documents [96, Part II.A]. As an example, the ODC-Bylicense has been commonly used for large-scale web corpora such as Dolma [167], FineWeb [132],and TxT360 [173]. ODC-By, by definition, does not extend to individual documents within thecorpus; therefore, the copyright of documents in these collections is still controlled by the documentauthors, and does not imply that the text itself is openly licensed.",
      "text": "",
      "start_page": 3,
      "end_page": 3
    },
    {
      "section_id": "4200fe97-5655-4a7a-80e2-d320766866a0",
      "heading": "LLM-generated synthetic datasetsDatasets containing text generated by LLMs trained on un-licensed data have been released under open licenses [e.g. 195]. It has not yet been establishedwhether it is permissible to apply arbitrary licenses to the generations of an LLM that was trained onunlicensed data [96]. We therefore take a conservative stance and avoid synthetic content that wasgenerated by an LLM.",
      "text": "",
      "start_page": 3,
      "end_page": 3
    },
    {
      "section_id": "04b98c49-3158-4765-aaef-22260fdd83a1",
      "heading": "CaveatsDespite our best efforts at due diligence, data that falls outside of our curatorial principlesand choices may have still ended up in our dataset. License laundering is a notoriously hardproblem to identify exhaustively in practice [96]. Copyright owners may also change the license theyassociate with their content. Since we collected and curated the Common Pile v0.1 in late 2024, thelicensing information we include and rely on may not be completely aligned with more recent updates.Further, some documents that we collect that are in the public domain or are openly licensed maycontain material with unclear status (e.g., quoted snippets of in-copyright books in public domainU.S. government publications). Finally, we note that while it is relatively straightforward to obeyattribution requirements when redistributing data, attributing model predictions back to the trainingdata points that influenced them remains an active area of research [129, 28].",
      "text": "2.2Comparisons with related work",
      "start_page": 3,
      "end_page": 3
    },
    {
      "section_id": "e51e4b4e-925a-4e47-a63c-2b2525da42fe",
      "heading": "Our work is not the first that aims to construct a dataset of openly licensed and/or public domain datafor the purposes of training machine learning models. Past efforts include CommonCanvas [61], acollection of approximately 70 million Creative Commons-licensed images designed for trainingimage generation models, the PG19 dataset [140] of public domain novels sourced from Project",
      "text": "3",
      "start_page": 3,
      "end_page": 3
    },
    {
      "section_id": "493755f7-0d06-4713-acce-5284760bb0ac",
      "heading": "Gutenberg used for benchmarking language models, the C4Corpus tools for sourcing CreativeCommons text from Common Crawl snapshots [68], and many datasets comprising CC BY-SA-licensed text from Wikipedia [66, 115].",
      "text": "",
      "start_page": 4,
      "end_page": 4
    },
    {
      "section_id": "4d9fa387-34c3-4791-b36f-200e77eed2bf",
      "heading": "More relevant to our work are the recent Open License Corpus (OLC) [119], Common Corpus [74, 91],and KL3M [78] datasets, which were constructed for use as LLM pre-training data. On the whole,OLC uses similar selection criteria to ours, including text that is in the public domain or is openlylicensed. However, OLC also includes conversations scraped from Hacker News, which does not havean open license. Additionally, OLC is considerably smaller than the Common Pile v0.1, comprisingdata from 12 sources (vs. 30 for Common Pile) totaling 0.85 TB of text (vs. 7.6 TB for CommonPile). Common Corpus also uses a similar set of allowable licenses/copyright statuses (e.g. CCBY, CC BY-SA, public domain, MIT-style, etc.) although the specific licenses/statuses are notclear because Common Corpus does not retain full per-document licensing information across allsources. Additionally, Common Corpus incorporates data from OpenAlex [126] which is knownto provide inaccurate licensing information [e.g., 80]. Furthermore, while the Common Pile andCommon Corpus are similar in size (7.6 TB vs. 7.4 TB), Common Corpus targets a broader set oflanguages and therefore contains significantly less English text. Conversely, KL3M does not considerCC BY-SA to be acceptable and, as a result, almost exclusively consists of government documents.Accordingly, the Common Pile is much larger than KL3M (3 TB), and is built from significantlymore diverse data sources (Figure 1 & Section 3). In subsection 4.3, we compare the Common Pilev0.1 to these datasets in a controlled setting, ultimately showing that it produces substantially moreperformant LLMs.",
      "text": "",
      "start_page": 4,
      "end_page": 4
    },
    {
      "section_id": "ae025885-ccef-4a78-a87b-99a1cd02a08b",
      "heading": "3Composition of the Common Pile",
      "text": "",
      "start_page": 4,
      "end_page": 4
    },
    {
      "section_id": "6ffb3d91-2a36-444b-8ae5-7995ca03b679",
      "heading": "The Common Pile comprises content drawn from a wide range of domains, including scholarlypublications, government documents, online discussions, books, open educational resources, andmore. In this section, we provide an overview of each of the domains contained in the Common Pileand briefly discuss their constituent data sources. In-depth discussion of each source is provided inAppendix B.",
      "text": "",
      "start_page": 4,
      "end_page": 4
    },
    {
      "section_id": "f9062916-3cad-42b0-aeea-20bfd458ce0d",
      "heading": "Scientific and scholarly texts, which are often distributed under open licenses due to open accessmandates, appear in many LLM pre-training datasets [e.g. 57, 167, 185] since they expose models totechnical terminology, formal reasoning, and long-range document structure. To attain broad coverageof scholarly text, we filter peS2o [166] (a collection text extracted from open-access scientific PDFsbased on S2ORC [104]) to only retain openly licensed research papers. For medical-domain text,we collect text from openly licensed articles in the U.S. National Institutes of Health’s NationalLibrary of Medicine’s PubMed Central archive. Additionally, we collect data from ArXiv, whichcontains over 2.4 million articles in the quantitative sciences, most of which are uploaded as LaTeXsource and may be distributed under various licenses chosen by a given article’s author. We includeopenly licensed articles sourced from ArXiv’s bulk-access S3 bucket and parsed using LATEXMLand Trafilatura [10]. Furthermore, according to ArXiv’s licensing policy, all metadata (includingabstracts) of articles posted to ArXiv are distributed under the CC0 license; we therefore include theabstracts for all ArXiv papers in the Common Pile, regardless of the paper’s full-text license.",
      "text": "",
      "start_page": 4,
      "end_page": 4
    },
    {
      "section_id": "717826ad-0399-4353-ae3d-b9fcb553cb25",
      "heading": "Online discussion forums comprise multi-turn question-answer pairs and discussions and thereforecan be useful for training language models to follow conversational structure as well as for improvingperformance on question answering and dialogue-centric tasks. StackExchange is a collection ofwebsites that host user-provided questions and answers and allow their redistribution under a CCBY-SA license. We leverage the user-provided StackExchange dumps from the Internet Archive andformat questions/answers in the same order they appear on StackExchange, using PyMarkdown toconvert each comment into plain text. Additionally, we collect text from issues, pull requests, andcomments on GitHub, which, according to GitHub’s terms of service, inherit the license of theirassociated repository. We extract this content from repositories with Blue Oak Council-approvedlicenses from the GitHub Archive. Finally, we include logs of all discussions on the Ubuntu-hostedInternet Relay Chat (IRC) since 2004, which are released into public domain.",
      "text": "",
      "start_page": 4,
      "end_page": 4
    },
    {
      "section_id": "756b6f60-1a04-4d3d-9f73-2ac441abf1a3",
      "heading": "Government and legal texts are often published directly into the public domain or under openlicenses. For example, in the US, text written by federal government employees is considered to be inthe public domain. We therefore include all plain-text documents made available through the United",
      "text": "4",
      "start_page": 4,
      "end_page": 4
    },
    {
      "section_id": "730b4f8c-fe8b-45b0-8a2d-abbd6b509d51",
      "heading": "States Government Publishing Office (USGPO)’s GovInfo.gov developer API. Additionally, weinclude all plain text regulatory documents published by U.S. federal agencies from Regulations.gov,an online platform that hosts newly proposed rules and regulations from federal agencies. TheCommon Pile also incorporates US Patents and Trademark Office (USPTO) patent documentssourced from the Google Patents Public Data dataset [77], containing millions of public domainpatents and published patent applications dating back to 1782. Similarly, the Hansard (the officialrecord of parliamentary proceedings) of the United Kingdom is distributed under the Open ParliamentLicense, which stipulates similar terms to the CC BY license. We source UK Hansard data fromParlParse [131], covering Commons debates from 1918 forward and Lords proceedings from the1999 reform. For legal text, we leverage the Caselaw Access Project (comprising 40 million pages ofU.S. federal and state court decisions and judges’ opinions from the last 365 years) and Court Listener(including 900 thousand cases scraped from 479 courts). Only legal texts in the public domain wereselected for the Common Pile.",
      "text": "",
      "start_page": 5,
      "end_page": 5
    },
    {
      "section_id": "9766f6f6-2163-483e-9b54-05665827d38d",
      "heading": "Curated task datasets are typically designed for fine-tuning on specific downstream tasks such asquestion answering, summarization, or text classification. To source datasets that are distributedunder an open license and only contain content owned by the dataset’s rights holder (to avoid licenselaundering), we use metadata and redistributed datasets from the Data Provenance Initiative [106, 109].Full details on the datasets we include are available in Appendix D.",
      "text": "",
      "start_page": 5,
      "end_page": 5
    },
    {
      "section_id": "a7041935-e18c-4d5b-b210-91541cc2e75b",
      "heading": "Books, particularly historic text, can fall into the public domain due to copyright expiration—forexample, in the United States, books published prior to 1929 are currently in the public domain. Wesource public domain books from various sources, including the Biodiversity Heritage Library (BHL),an open-access digital library for biodiversity literature and archives; pre-1929 books digitized by theInternet Archive on behalf of HathiTrust member libraries; the collection of public domain bookscalled “Selected Digitized Books” released by the Library of Congress; and select books from ProjectGutenberg, an online collection of over 75,000 digitized books, most of which are in the publicdomain.",
      "text": "",
      "start_page": 5,
      "end_page": 5
    },
    {
      "section_id": "64bf502f-a8c2-4ce1-9ffa-191789afb8d7",
      "heading": "Open Educational Resources (OERs) are educational materials (e.g. textbooks, lecture notes, lessonplans, etc.), typically published under Creative Commons licenses that support free and equitableaccess to education. We collect data from multiple OER repositories, including the Directory ofOpen Access Books (DOAB), an online index of over 94,000 peer-reviewed books curated fromtrusted open-access publishers; PressBooks, a searchable catalog of over 8,000 open access books;OERCommons, an online platform where educators share open-access instructional materials; andLibreTexts, a catalog of over 3,000 open-access textbooks.",
      "text": "",
      "start_page": 5,
      "end_page": 5
    },
    {
      "section_id": "0bcf9375-ca49-4ea0-8c6f-f91613201631",
      "heading": "Wikis are topic- or domain-specific encyclopedic websites that are collaboratively written, maintained,and moderated. Historical and cultural precedent has led many wikis to have an open license. Wedownloaded the official database dumps of wikitext (Mediawiki’s custom markup language) ofthe English-language wikis that are directly managed by the Wikimedia foundation and convertedwikitext to plain text using wtf_wikipedia. For wikis not managed by Wikimedia, we make use ofwikiteam’s unofficial database dumps and apply the same conversion process.",
      "text": "",
      "start_page": 5,
      "end_page": 5
    },
    {
      "section_id": "c3ae12f8-692a-4ea5-be23-78ae9f9119ca",
      "heading": "Source code has proven to be a useful part of LLM pre-training corpora, not only to support codingabilities but also to improve reasoning [7, 113, 120]. Due to the Free and Open Source Software(FOSS) movement, a great deal of source code is distributed with an open license. We leverageprior work done by the Software Heritage Foundation and BigCode to compile the openly licensedsubset of the Stack V2 [113], based on the license detection performed by the creators of StackV2. Additionally we collected all Python Enhancement Proposals (PEPs)—design documents thatgenerally provide a technical specification and rationale for new features of the Python programminglanguage—that were released into the public domain.",
      "text": "YouTube allows users to upload content under a CC BY license. We therefore sourced and transcribedspeech-heavy CC BY videos from YouTube. To avoid license laundering and focus on high-qualityspeech-based textual content, we manually curated a set of over 2,000 YouTube channels that releaseoriginal openly licensed content containing speech. From these channels, we retrieved and transcribed(using Whisper [138]) over 1.1 million openly licensed videos comprising more than 470,000 hoursof content.",
      "start_page": 5,
      "end_page": 5
    },
    {
      "section_id": "9df4f7db-ca1b-4602-bf4b-8fdabc2659bf",
      "heading": "Web text is a common source of LLM pre-training data. A small fraction of content on the webis distributed under open licenses. To recover a portion of this content, we process 52 Common",
      "text": "5",
      "start_page": 5,
      "end_page": 5
    },
    {
      "section_id": "5d6a176b-af55-4dee-997f-b7631da928e1",
      "heading": "Crawl snapshots using a regular expression (regex) adapted from the C4Corpus project [68] to retainpages that include a CC BY, CC BY-SA, or CC0 marker. This regex naturally results in many falsepositives (e.g., it would retain a page that included and provided attribution for a CC BY image butotherwise contained unlicensed content), so we manually verified the top 1000 domains by contentvolume, retaining only those for which all content was assigned a Creative Commons license. Textwas extracted using a pipeline similar to the one used for Dolma [167]. We provide more detailson the composition of our web-sourced text, called CCCC, in Appendix G. Apart from CCCC,we additionally manually collected data from a few select sites, including Foodista, a community-maintained site with recipes and food-related news as well as nutrition information; news sites thatpublish content under CC BY or CC BY-SA according to Open Newswire; and the Public DomainReview, an online journal dedicated to exploration of works of art and literature that have aged intothe public domain.",
      "text": "",
      "start_page": 6,
      "end_page": 6
    },
    {
      "section_id": "f75f662a-eeab-4992-8db0-c126e74cabe7",
      "heading": "4Assessing the Common Pile v0.1’s quality",
      "text": "",
      "start_page": 6,
      "end_page": 6
    },
    {
      "section_id": "2e49df4d-9772-40c2-b1ca-51e9b9060f51",
      "heading": "The utility of an LLM pre-training dataset is mostly assessed in terms of whether or not it can beused to train performant LLMs. To validate our efforts in curating the Common Pile, we use it asthe basis of an LLM pre-training dataset created through additional filtering (subsection 4.1) andrebalancing (subsection 4.2). Then, we perform a controlled data ablation study (subsection 4.3)where we train otherwise-identical LLMs on different pre-training datasets, including prior datasetscomprised of openly licensed text mentioned in subsection 2.2 as well as a selection of representativepre-training datasets of unlicensed text. Finally, we train Comma v0.1-1T and Comma v0.1-2T, 7billion parameter LLMs trained on 1 and 2 trillion tokens (respectively) of Common Pile-sourcedcontent, and compare them to models with a similar parameter count and training budget that weretrained on unlicensed text (subsection 4.4).",
      "text": "4.1Dataset preprocessing and filtering",
      "start_page": 6,
      "end_page": 6
    },
    {
      "section_id": "b37c354d-011f-43c5-b455-d42c484003ec",
      "heading": "Before training a language model, it is considered important to “clean” data in hopes of retaining onlyhigh-quality text under some notion of quality [4, 105]. Consequently, before training on data fromthe Common Pile (which is distributed in a relatively “raw” format), we independently preprocessedeach of the Common Pile’s non-code datasets using pipelines implemented with the Dolma dataprocessing toolkit [167].",
      "text": "",
      "start_page": 6,
      "end_page": 6
    },
    {
      "section_id": "00b9d15b-fa10-4595-92a8-65c462b10f18",
      "heading": "Since the Common Pile v0.1 focuses primarily on English content, we apply language identificationusing a FastText classifier [81] to filter out non-English text. When processing web text fromCCCC, we employ the text quality classifier adapted from DataComp-LM [99] with an extremelylow threshold to remove noisy text. We remove documents with pervasive OCR errors using thelikelihood-based filtering approach from [166], which removes documents that are assigned anexcessively low log-likelihood under a unigram language model constructed from the Trillion WordCorpus [117]. To reduce the prevalence of toxic or inappropriate content, we apply a pair of FastTexttoxicity classifiers implemented in Dolma [167] that were trained on the Jigsaw Toxic CommentClassification Challenge dataset [30]. We apply regex-based personally identifiable information(PII) redaction to remove email addresses, phone numbers, and IP addresses, and replace themwith <EMAIL_ADDRESS>, <PHONE_NUMBER>, and <IP_ADDRESS> respectively. Finally, we performsource-specific regex filtering to remove repetitive or boilerplate text (e.g., page numbers, documentpreambles, license statements, etc.). For a detailed breakdown of the pre-processing applied to eachdataset, see Table 5 (appendix).",
      "text": "",
      "start_page": 6,
      "end_page": 6
    },
    {
      "section_id": "fa8844c4-357d-4929-9255-0e98c6efcad8",
      "heading": "After filtering, we perform global document-level fuzzy deduplication across all sources, as excessivedata duplication is known to harm language modeling performance [94] and increase memorization[83]. We use the bloom filter-based deduplication functionality from Dolma [167] and deem twodocuments duplicates if they share more than 90% of their 20-grams.",
      "text": "",
      "start_page": 6,
      "end_page": 6
    },
    {
      "section_id": "d67a55b2-c599-43d8-870c-3294a1577e8a",
      "heading": "For code data from the Stack v2, we apply the Red Pajama V1 [185] code filtering heuristics.These include filters based on the mean and maximum line length in a document, the proportion ofalphanumeric characters, and the ratio of alphabetical characters to tokens. After this initial filter, weadopt the process used by SmolLM2 [5] where we keep only code in Python, C, C++, SQL, Java,PHP, Rust, Javascript, Typescript, Go, Ruby, Markdown, C#, Swift, or shell and filter this set usinglanguage-specific quality classifiers to retain only educational and well-documented code. We use",
      "text": "6",
      "start_page": 6,
      "end_page": 6
    },
    {
      "section_id": "c5fc0dce-b626-45c1-8342-fe92afc1cb5f",
      "heading": "Figure 2: The Common Pile consistently outperforms other openly licensed corpora as a pre-training dataset. Following the setup from Penedo et al. [132], we train and evaluate 1.7B parametermodels on 28B tokens of data from each dataset. Stars denote benchmarks on which the modeltrained using the Common Pile outperforms all other models.",
      "text": "",
      "start_page": 7,
      "end_page": 7
    },
    {
      "section_id": "fe21de77-7736-492b-b1a7-fca3d7b7b290",
      "heading": "a lower threshold to filter out low-quality code than was used for SmolLM2, resulting in a largerset of post-filtered text. Finally, we extract plaintext from HTML documents in the Stack V2 usingTrafilatura [10] and apply our standard plaintext filtering pipeline including language, length, toxicity,and PII filtering.",
      "text": "4.2Data mixing",
      "start_page": 7,
      "end_page": 7
    },
    {
      "section_id": "b2286127-9fa6-400c-bb24-909d4870cb91",
      "heading": "Recent work [3, 178, 189] has shown that up- or down-weighting pre-training data sources inaccordance with some notion of data quality can produce more performant models. Indeed, thesources in the Common Pile vary drastically in their characteristics, and we don’t necessarily expectthat our largest sources contain the highest quality text. For example, patent text sourced fromthe USPTO (our second-largest source) exhibits substantially different wording, terminology, andrepetition than typical natural language. Consequently, we anticipate that appropriately mixing thesources in the Common Pile (rather than simply combining all sources, i.e., mixing in proportion tosource size) is of particular importance. Additionally, while LLM pre-training datasets have beencontinuously scaled to avoid the diminishing returns that result from repeating data [94], recent workhas highlighted that repeating high-quality data can be preferable to avoiding repetition by trainingon low-quality data [120, 52].",
      "text": "",
      "start_page": 7,
      "end_page": 7
    },
    {
      "section_id": "314378ea-fa9c-4a99-8483-a7826f77f7f8",
      "heading": "To determine mixing weights, we first trained per-source language models using the procedureoutlined in subsection 4.3 below for 28 billion tokens on all sources that were sufficiently large tobe repeated less than four times at this data budget. Based on the performance of these per-sourcemodels, we heuristically set mixing weights to up- and down-weight high- and low-performancesources respectively while targeting a maximum of six repetitions over the course of a 1 trillion tokentraining run. Additionally, we assumed that our smaller sources were high quality and set their mixingrates such that they were also repeated six times over the course of 1 trillion tokens. The resultingmixture and per-source repetition rates are given in Table 7 (appendix). We also experimented withusing MixMin [178] to automatically determine mixing weights but found that it did not improveover our heuristically determined mixture.",
      "text": "",
      "start_page": 7,
      "end_page": 7
    },
    {
      "section_id": "204259cd-ab63-4de5-8620-2f2086abd79e",
      "heading": "Because we use this dataset mixture to train the Comma v0.1 models (subsection 4.4) and becauseit comprises a heavily filtered and remixed version of the Common Pile v0.1, we refer to it as the“Comma dataset” to distinguish it from the Common Pile itself.",
      "text": "4.3Controlled dataset quality experiments\n\nAs a preliminary measure of the Common Pile’s quality, we adopt the experimental setting of Penedoet al. [132] and identically train models on the Comma dataset and various preexisting datasets. Byusing a controlled setting across datasets, we can assert that differences in model performance stemprimarily from the quality of each dataset. Specifically, we train 1.7 billion parameter decoder-onlyTransformer models [182] that follow the Llama architecture [179] on 28 billion tokens of data fromeach dataset, tokenized using the GPT-2 tokenizer [137]. We follow the hyperparameters and setup\n\n7\n\nof Penedo et al. [132] exactly, except that we used a weight decay of 0.2 instead of 0.1 due to slightlyimproved performance (possibly due to the large amount of repetition in the Comma dataset).",
      "start_page": 7,
      "end_page": 8
    },
    {
      "section_id": "efb5ef4f-e900-4cf9-955c-2ea0fef79daf",
      "heading": "Each model was then evaluated using the set of “early signal” tasks identified by Penedo et al. [132]which cover commonsense reasoning and knowledge capabilities; specifically, we evaluate zero-shotperformance on ARC [33], MMLU [70], HellaSwag (HSwag) [192], OpenBookQA (OBQA) [118],CommonSenseQA (CSQA) [171], PIQA [19], and SocialIQA (SIQA) [160]. We omit Winogrande(which was used in Penedo et al. [132]) because it is included in the set of datasets we sourced fromthe Data Provenance Initiative; consequently all of the tasks we evaluate on are “unseen” by allmodels. We highlight that a significant portion of the Comma dataset is code, but none of the taskswe evaluate on measure code capabilities. While it is possible that we could improve performanceby omitting code data in this setting, we retained code for reliable reporting of the Comma dataset’sperformance.",
      "text": "",
      "start_page": 8,
      "end_page": 8
    },
    {
      "section_id": "2b152c17-af7c-4da7-9f18-1bdd108fbeb6",
      "heading": "As baselines, we compare to the prior datasets that aim to provide open licensed text discussed insubsection 2.2: OLC [119], Common Corpus [91], and KL3M [78]. We additionally compare to thePile, as it one of the only LLM pre-training datasets that contains a comparable number of diversesources to the Common Pile (22 vs. 30). Finally, we report the performance of two web text-basedunlicensed pre-training datasets: OSCAR [169], which incorporates relatively little filtering; andFineWeb [132], an recent dataset that reflects current best practice for LLM pre-training datasetcuration.",
      "text": "",
      "start_page": 8,
      "end_page": 8
    },
    {
      "section_id": "d6f9583a-d26c-4f20-a3e3-8dd57aeb8ec2",
      "heading": "The resulting performance of each model is shown in Figure 2, with detailed results in Table 9(appendix). Notably, the Comma dataset-based model outperforms the models trained OLC, CommonCorpus, and KL3M across all benchmarks and outperforms the Pile-based model on all but twobenchmarks. While the performance of the FineWeb-based model is the best on most benchmarks,the Comma dataset-based model performs best on the scientific and scholarly knowledge-basedbenchmarks MMLU and ARC, possibly due to the Common Pile’s large proportion of domain-relevant text. On the other hand, on the commonsense reasoning datasets HellaSwag, PIQA, andCommonSenseQA, the model trained on the Comma dataset has significantly worse performancethan models trained on the Pile, OSCAR, and FineWeb, possibly indicating a lack of relevant datain the Common Pile. We additionally note that recent work [188] highlights that performance onHellaSwag is most heavily influenced by coverage of certain domains and topics such as personalblogs, tutorials, hobbies, and sports, which are poorly represented in the Common Pile. Overall,these findings confirm that the Comma dataset performs best among datasets that aim to contain onlyopenly licensed data and is also a strong candidate in general, particularly when targeting scientificand scholarly applications.",
      "text": "We note that the Comma dataset is the only dataset we evaluate that explicitly includes task-like datadue to inclusion of data from the Data Provenance Initiative (DPI). To verify that this does not conferan unfair advantage, we trained an additional model on the Comma dataset with all sources retainedexcept for the DPI-sourced data. Removing this source had a minimal impact on model performance(full results in Table 9), with a notable decrease only on HellaSwag, possibly suggesting that the DPIdata contains domain-relevant data for this benchmark that other sources lack.\n\n4.4Comma v0.1",
      "start_page": 8,
      "end_page": 8
    },
    {
      "section_id": "3c9c6f25-bb3a-4eac-b88e-e7db9f830482",
      "heading": "Having established that Comma’s dataset produces models with competitive performance whencompared to other datasets, we now validate our efforts at larger, more realistic scales. Specifically,we train Comma v0.1-1T and Comma v0.1-2T, a pair of 7 billion-parameter LLMs trained on 1 and 2trillion tokens of text respectively, and compare with other models trained using similar computationalbudgets.",
      "text": "TokenizationWhile training a tokenizer on unlicensed text is less likely to raise ethical or IP-relatedissues than training an LLM, we nevertheless trained a custom tokenizer on the Comma dataset toensure that our entire modeling pipeline was based on openly licensed data. In addition, the differentcharacteristics of our dataset likely makes existing tokenizers (which are often trained on web text)suboptimal. We therefore trained a BPE-based [55] tokenizer using the Hugging Face tokenizerslibrary using a vocabulary size of 64,000. We follow the same splitting regex as Llama 3.2 [62] andthe Hugging Face ByteLevel preprocessor; no Unicode normalization was used. The tokenizer wastrained on a 600GB sample [150] of text from the Comma dataset.\n\n8\n\nARC-CARC-EMMLUBoolQHSwagOBQACSQAPIQASIQAHumEvalMBPP0\n\n20\n\n40\n\n60\n\n80\n\n100\n\nPerformance\n\nKnowledge/ReasoningCoding\n\nComma v0.1-1TLLaMAMPTRPJ-INCITEQwen3",
      "start_page": 8,
      "end_page": 9
    },
    {
      "section_id": "90eebb7e-9141-4ed5-bb5b-8511c727125b",
      "heading": "Figure 3: Compared to models trained with similar resources (7 billion parameters, 1 trilliontokens), Comma v0.1-1T is the strongest model on several standard benchmarks. To contextual-ize these results, we include Qwen3 8B (trained on 36 trillion tokens) as a “current best-practices”upper bound. Stars denote benchmarks on which Comma v0.1-1T outperforms all other compute-matched models (i.e., all models other than Qwen3). Full numerical results are provided in Table 10(appendix).",
      "text": "Training setupWe trained Comma v0.1-1T and -2T using the lingua framework [183]. We baseour model architecture and training hyperparameters on lingua’s Llama-7B configuration, whichclosely follows the conventions set by the Llama series of models [179, 62]. For Comma v0.1-1T, wetrained with an effective batch size of 512 length-4096 sequences using the AdamW [111] optimizerand a weight decay of 0.2. For the Comma v0.1 variant trained on 2 trillion tokens, we increased thebatch size to 2048 length-4096 sequences.",
      "start_page": 9,
      "end_page": 9
    },
    {
      "section_id": "2f521aba-6b3a-4c21-81c5-43f6b26d5335",
      "heading": "We performed two stage training, with a first stage following a cosine learning rate schedule and thesecond stage being a ‘cool-down” [73], where we train only on a subset of high-quality sources usingthe mixing weights provided in Table 8 (appendix) while decaying the learning rate linearly to 0.Comma v0.1-1T had a first stage of 460,000 steps with 2,000 steps of warmup, an initial learningrate of 1e−3, a minimum learning rate of 1e−9, a cosine schedule period of 500,000 steps, and18,000 steps of decay. For Comma v0.1-2T, the first stage instead had 230,000 steps with a maximumand minimum learning rate of 2e−3 and 2e−9 respectively and a period of 250,000 steps, with9,000 steps of decay in the second stage. For both models, we average together ten evenly spacedcheckpoints from the cool-down phase to produce a final model as suggested by Grattafiori et al.[62]. Apart from our main Comma v0.1-1T and -2T training runs, we completed several additionalruns to better understand how hyper-parameters impact the model, including using a different batchsize and following a three-stage (rather than two-stage) curriculum. Overall, the results of these runswere consistent with the findings from our main training runs. Additional details can be found inAppendix O.",
      "text": "",
      "start_page": 9,
      "end_page": 9
    },
    {
      "section_id": "307f242d-35d7-4fd2-9857-93790ad0314b",
      "heading": "EvaluationWe evaluate the Comma v0.1 models on the suite of benchmarks used by Groeneveldet al. [64] in addition to two additional code benchmarks. Specifically, we evaluate models onARC [33], MMLU [70], BoolQ [31], HellaSwag [192], OpenBookQA [118], CommonsenseQA [171],PIQA [19], and SIQA [160] to probe world knowledge and reasoning and HumanEval [25] andMBPP [8] to evaluate coding capabilities. Following Groeneveld et al. [64], we evaluate usingOLMES [65], using a zero-shot format for all tasks except MMLU, which uses a 5-shot format. Forthe coding tasks, we report pass@10 accuracies.",
      "text": "",
      "start_page": 9,
      "end_page": 9
    },
    {
      "section_id": "3dbcc4d6-66f9-44d2-8752-6ef3b0739b4d",
      "heading": "Baseline modelsFor fairness, we primarily compare to prior models with the same parameter countand token budget. Since we are not aware of any such models trained on openly licensed data, wecompare only to models trained on unlicensed data. For Comma v0.1-1T, we compare to Llama 17B [179], MPT-7B [175], RPJ-INCITE-7B [185], StableLM-7B [12], and OpenLLaMA-7B [58]. Forthe two trillion token variant, we compare to OLMo Twin (specifically OLMo-7B-Twin-2T) [64],Llama 2 7B [180], and DeepSeekLLM [16]. Over time, the token budgets of open pre-trained LLMshave continually grown [143], and current standard practice is to pretrain on significantly more than1 or 2 trillion tokens. Consequently, recent models tend to outperform our baselines, which werereleased in 2023 and 2024. To provide a state-of-the-art point of reference, we additionally includeresults for the recently released Qwen3 8B [176], which was trained for 36 trillion tokens. Weemphasize that we cannot reliably compare to a model with a 36× or 18× larger training budget andwe primarily include it as a point of reference.",
      "text": "9\n\nARC-CARC-EMMLUBoolQHSwagOBQACSQAPIQASIQAHumEvalMBPP0\n\n20\n\n40\n\n60\n\n80\n\n100\n\nPerformance\n\nKnowledge/ReasoningCoding\n\nComma v0.1-2TOLMo TwinLLaMA 2DeepSeekLLMQwen3\n\nFigure 4: Comma v0.1-2T is also competitive with budget-matched models (7 billion parameters,2 trillion tokens) trained on unlicensed data. We additionally include Qwen3 8B as a higher budgetupper bound. Stars denote benchmarks where Comma v0.1-2T outperforms budget-matched models.Full numerical results are provided in Table 11 (appendix).",
      "start_page": 9,
      "end_page": 10
    },
    {
      "section_id": "b9e34c90-1608-4006-988b-1450dad8efcb",
      "heading": "ResultsAs shown in Figure 3, Comma v0.1-1T outperforms budget-matched baseline models onover half of the benchmarks tested. In line with our results from subsection 4.3, we observe thatComma v0.1-1T excels on knowledge-based benchmarks like ARC-C and MMLU, but lags behindon HellaSwag and PIQA. Comma v0.1-1T is also particularly strong at code-related tasks where itoutperforms baseline models by a wide margin. Comparisons to StableLM and OpenLLama can befound in Table 10 (appendix), but show similar trends.",
      "text": "",
      "start_page": 10,
      "end_page": 10
    },
    {
      "section_id": "01d8c2b2-38f5-43bd-abd9-6af98f4cff04",
      "heading": "Our promising results from training on 1 trillion tokens motivated us to experiment with longertraining durations. To test whether the filtered dataset supports training durations beyond 1T, wetrained Comma v0.1-2T simply by repeating the same data mixture used for Comma v0.1-1Tapproximately twice. We note that this pre-training mixture involves repeating certain sources anexcessive number of times (up to 16 passes for some sources). Prior work suggests that these extremelevels of data repetition may result in diminishing returns [121]. However, these experiments stillgive us a preliminary picture of the performance achievable under a larger budget.",
      "text": "",
      "start_page": 10,
      "end_page": 10
    },
    {
      "section_id": "3503620d-0b7b-4105-8c19-235b3da9509d",
      "heading": "The performance of Comma v0.1-2T is compared with budget-matched models in Figure 4. Notably,we find that Comma v0.1-2T is competitive with OLMo, Llama 2, and DeepSeekLLM, with especiallystrong performance on MMLU, SIQA, ARC-E, and the coding tasks. We emphasize that the Commav0.1-2T result here is likely not a best-case 2T-token run using the Common Pile v0.1 due toexcessive repetition, and better performance could likely be attained through a 2T-specific mixtureand curriculum. Nevertheless, this result highlights the promise of larger scale training runs based onthe Common Pile. Qwen3 8B’s superior performance across most benchmarks confirms the benefitof larger training budgets and motivates future efforts on scaling up the Common Pile.",
      "text": "",
      "start_page": 10,
      "end_page": 10
    },
    {
      "section_id": "fcd84d23-cd76-428d-a233-c68217bdf948",
      "heading": "5Conclusion",
      "text": "",
      "start_page": 10,
      "end_page": 10
    },
    {
      "section_id": "e9fc6880-a034-43fd-ae41-08281cb56701",
      "heading": "We release Common Pile v0.1, an 8TB corpus that—to our knowledge—constitutes the largest datasetbuilt exclusively from openly licensed text. Alongside our dataset, we release Comma v0.1-1T and-2T, two performant 7-billion-parameter LLMs trained on text from the Common Pile, as well asthe filtered and rebalanced data mixture we used for training. Our results demonstrate that not onlyis the Common Pile the strongest dataset for pretraining under an open-license constraint, but alsothat it produces models comparable to those trained on an equivalent amount of unlicensed data.This positive result holds promise for future of open-license pretraining, especially if the researchcommunity invests in collecting larger quantities of openly licensed text data in the future. Ultimately,we believe that the Common Pile v0.1 represents the first step on the path towards a more ethicallanguage model ecosystem, where performance need not come at the cost of creator rights and legaltransparency.",
      "text": "",
      "start_page": 10,
      "end_page": 10
    },
    {
      "section_id": "bd603dcc-f30e-4417-97a1-41bafdf45ab5",
      "heading": "Acknowledgments",
      "text": "We thank Chris Maddison, Anvith Thudi, Pierre-Carl Langlais, Alec Radford, Adam Roberts, SewonMin, and Weijia Shi for fruitful discussions and constructive feedback. An early draft of this work\n\n10\n\nwas shared at the Dataset Convening hosted by the Mozilla Foundation and EleutherAI. We thank theparticipants for their discussion and feedback.",
      "start_page": 10,
      "end_page": 11
    },
    {
      "section_id": "5c782587-528e-435b-b116-cb3118fd936a",
      "heading": "This work was supported by funding from the Mozilla Foundation and Sutter Hill Ventures. Weacknowledge the support of the Natural Sciences and Engineering Research Council of Canada(NSERC). Researchers funded through the NSERC-CSE Research Communities Grants do notrepresent the Communications Security Establishment Canada or the Government of Canada. Anyresearch, opinions or positions they produce as part of this initiative do not represent the officialviews of the Government of Canada.",
      "text": "",
      "start_page": 11,
      "end_page": 11
    },
    {
      "section_id": "ca68188d-a420-4488-91c9-9ece13787437",
      "heading": "Parts of this work were performed under the auspices of the U.S. Department of Energy by LawrenceLivermore National Laboratory under Contract DE-AC52-07NA27344 and was supported by theLLNL-LDRD Program under Project No. 24-ERD-010 and Project No. 24-SI-008 (LLNL-CONF-2006420).",
      "text": "",
      "start_page": 11,
      "end_page": 11
    },
    {
      "section_id": "f8963776-fd0a-49e5-82c8-1a3826027d70",
      "heading": "References",
      "text": "[1] 17 U.S. Code § 102. Subject matter of copyright: In general, December 1990. URL https://www.law.cornell.edu/uscode/text/17/102.\n\n[2] 17 U.S. Code § 105. Subject matter of copyright: United States Government works, December2024. URL https://www.law.cornell.edu/uscode/text/17/105.\n\n[3] Alon Albalak, Liangming Pan, Colin Raffel, and William Yang Wang. Efficient online datamixing for language model pre-training. arXiv preprint arXiv:2312.02406, 2023.",
      "start_page": 11,
      "end_page": 11
    },
    {
      "section_id": "b478a943-5502-482d-856c-9dbf2e469463",
      "heading": "[4] Alon Albalak, Yanai Elazar, Sang Michael Xie, Shayne Longpre, Nathan Lambert, XinyiWang, Niklas Muennighoff, Bairu Hou, Liangming Pan, Haewon Jeong, et al. A survey ondata selection for language models. Transactions on Machine Learning Research, 2024.",
      "text": "",
      "start_page": 11,
      "end_page": 11
    },
    {
      "section_id": "dcc63efd-26fc-4b4d-87de-e49db0d038f3",
      "heading": "[5] Loubna Ben Allal, Anton Lozhkov, Elie Bakouch, Gabriel Martín Blázquez, GuilhermePenedo, Lewis Tunstall, Andrés Marafioti, Hynek Kydlíˇcek, Agustín Piqueres Lajarín, VaibhavSrivastav, Joshua Lochner, Caleb Fahlgren, Xuan-Son Nguyen, Clémentine Fourrier, BenBurtenshaw, Hugo Larcher, Haojun Zhao, Cyril Zakka, Mathieu Morlon, Colin Raffel, Leandrovon Werra, and Thomas Wolf. Smollm2: When smol goes big – data-centric training of asmall language model, 2025. URL https://arxiv.org/abs/2502.02737.",
      "text": "",
      "start_page": 11,
      "end_page": 11
    },
    {
      "section_id": "61d84028-53c4-41bb-8c92-ea20e96d14db",
      "heading": "[6] Mikel Artetxe, Sebastian Ruder, and Dani Yogatama. On the cross-lingual transferabilityof monolingual representations. In Annual Meeting of the Association for ComputationalLinguistics, pages 4623–4637, 2019.",
      "text": "",
      "start_page": 11,
      "end_page": 11
    },
    {
      "section_id": "72d637ca-83ef-4537-94f9-7881ae6c640a",
      "heading": "[7] Viraat Aryabumi, Yixuan Su, Raymond Ma, Adrien Morisot, Ivan Zhang, Acyr Locatelli,Marzieh Fadaee, Ahmet Üstün, and Sara Hooker. To code, or not to code? exploring impact ofcode in pre-training. arXiv preprint arXiv:2408.10914, 2024.",
      "text": "[8] Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, DavidDohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with largelanguage models. arXiv preprint arXiv:2108.07732, 2021.",
      "start_page": 11,
      "end_page": 11
    },
    {
      "section_id": "ee2e3cdd-9372-441c-888a-ff083f76a5bd",
      "heading": "[9] Stefan Baack, Stella Biderman, Kasia Odrozek, Aviya Skowron, Ayah Bdeir, Jillian Bom-marito, Jennifer Ding, Maximilian Gahntz, Paul Keller, Pierre-Carl Langlais, Greg Lindahl,Sebastian Majstorovic, Nik Marda, Guilherme Penedo, Maarten Van Segbroeck, Jennifer Wang,Leandro von Werra, Mitchell Baker, Julie Belião, Kasia Chmielinski, Marzieh Fadaee, LisaGutermuth, Hynek Kydlíˇcek, Greg Leppert, EM Lewis-Jong, Solana Larsen, Shayne Long-pre, Angela Oduor Lungati, Cullen Miller, Victor Miller, Max Ryabinin, Kathleen Siminyu,Andrew Strait, Mark Surman, Anna Tumadóttir, Maurice Weber, Rebecca Weiss, Lee White,and Thomas Wolf. Towards best practices for open datasets for llm training, 2025. URLhttps://arxiv.org/abs/2501.08365.",
      "text": "",
      "start_page": 11,
      "end_page": 11
    },
    {
      "section_id": "acb103ff-421b-4e1d-9b5b-72cb363d68d9",
      "heading": "[10] Adrien Barbaresi. Trafilatura: A Web Scraping Library and Command-Line Tool for TextDiscovery and Extraction. In Proceedings of the Joint Conference of the 59th Annual Meetingof the Association for Computational Linguistics and the 11th International Joint Conferenceon Natural Language Processing: System Demonstrations, pages 122–131. Association for",
      "text": "11",
      "start_page": 11,
      "end_page": 11
    },
    {
      "section_id": "3b065318-278b-4f05-ba9a-0e11fb452ff9",
      "heading": "Computational Linguistics, 2021. URL https://aclanthology.org/2021.acl-demo.15.",
      "text": "[11] Max Bartolo, A. Roberts, Johannes Welbl, Sebastian Riedel, and Pontus Stenetorp. Beat theai: Investigating adversarial human annotation for reading comprehension. Transactions of theAssociation for Computational Linguistics, 8:662–678, 2020.\n\n[12] Marco Bellagente, Jonathan Tow, Dakota Mahan, Duy Phung, Maksym Zhuravinskyi, ReshinthAdithyan, James Baicoianu, Ben Brooks, Nathan Cooper, Ashish Datta, et al. Stable lm 2 1.6b technical report. arXiv preprint arXiv:2402.17834, 2024.\n\n[13] Jonathan Berant, A. Chou, Roy Frostig, and Percy Liang. Semantic parsing on freebase fromquestion-answer pairs. In Conference on Empirical Methods in Natural Language Processing,pages 1533–1544, 2013.",
      "start_page": 12,
      "end_page": 12
    },
    {
      "section_id": "ab0d08c0-6c78-4084-a593-3bd82fdf3f51",
      "heading": "[14] Janek Bevendorff, Benno Stein, Matthias Hagen, and Martin Potthast. Elastic ChatNoir:Search Engine for the ClueWeb and the Common Crawl. In Leif Azzopardi, Allan Hanbury,Gabriella Pasi, and Benjamin Piwowarski, editors, Advances in Information Retrieval. 40thEuropean Conference on IR Research (ECIR 2018), Lecture Notes in Computer Science, BerlinHeidelberg New York, March 2018. Springer.",
      "text": "",
      "start_page": 12,
      "end_page": 12
    },
    {
      "section_id": "0a17e39c-6ee4-4de5-b91f-6177110aa1a9",
      "heading": "[15] Janek Bevendorff, Martin Potthast, and Benno Stein. FastWARC: Optimizing Large-ScaleWeb Archive Analytics. In Andreas Wagner, Christian Guetl, Michael Granitzer, and StefanVoigt, editors, 3rd International Symposium on Open Search Technology (OSSYM 2021).International Open Search Symposium, October 2021.",
      "text": "",
      "start_page": 12,
      "end_page": 12
    },
    {
      "section_id": "0e7d50de-c2a7-433b-8467-bc90f90852c7",
      "heading": "[16] Xiao Bi, Deli Chen, Guanting Chen, Shanhuang Chen, Damai Dai, Chengqi Deng, HonghuiDing, Kai Dong, Qiushi Du, Zhe Fu, et al. Deepseek llm: Scaling open-source languagemodels with longtermism. arXiv preprint arXiv:2401.02954, 2024.",
      "text": "[17] Stella Biderman, Usvsn Prashanth, Lintang Sutawika, Hailey Schoelkopf, Quentin Anthony,Shivanshu Purohit, and Edward Raff. Emergent and predictable memorization in large languagemodels. Advances in Neural Information Processing Systems, 36:28072–28090, 2023.\n\n[18] Stella Biderman, Hailey Schoelkopf, Quentin Anthony, Herbie Bradley, Kyle O’Brien, EricHallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff,Aviya Skowron, Lintang Sutawika, and Oskar van der Wal. Pythia: A suite for analyzing largelanguage models across training and scaling, 2023. URL https://arxiv.org/abs/2304.01373.",
      "start_page": 12,
      "end_page": 12
    },
    {
      "section_id": "29c73850-20c1-4577-bd15-1535315da21f",
      "heading": "[19] Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, and Yejin Choi. Piqa: Reasoningabout physical commonsense in natural language, 2019. URL https://arxiv.org/abs/1911.11641.",
      "text": "[20] Blue Oak Council. License List (version 15), 2025. URL https://blueoakcouncil.org/list.\n\n[21] Oana-Maria Camburu, Tim Rocktäschel, Thomas Lukasiewicz, and Phil Blunsom. e-snli: Nat-ural language inference with natural language explanations. In Neural Information ProcessingSystems, pages 9560–9572, 2018.",
      "start_page": 12,
      "end_page": 12
    },
    {
      "section_id": "dcfe8645-0c17-4838-93bf-7be6e705d9ff",
      "heading": "[22] Nicholas Carlini, Daphne Ippolito, Matthew Jagielski, Katherine Lee, Florian Tramer, andChiyuan Zhang. Quantifying memorization across neural language models. In The EleventhInternational Conference on Learning Representations, 2022.",
      "text": "[23] Ilias Chalkidis, Abhik Jana, D. Hartung, M. Bommarito, Ion Androutsopoulos, D. Katz, andNikolaos Aletras. Lexglue: A benchmark dataset for legal language understanding in english.In Annual Meeting of the Association for Computational Linguistics, pages 4310–4330, 2021.\n\n[24] Chat GPT Is Eating the World, 2024. URL https://chatgptiseatingtheworld.com.\n\n12",
      "start_page": 12,
      "end_page": 12
    },
    {
      "section_id": "376d6b33-be9e-4023-8e33-294e475b478f",
      "heading": "[25] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto,Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, RaulPuri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, BrookeChan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, MohammadBavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, MatthiasPlappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, AlexNichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain,William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra,Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer,Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and WojciechZaremba. Evaluating large language models trained on code, 2021.",
      "text": "",
      "start_page": 13,
      "end_page": 13
    },
    {
      "section_id": "d885823e-3615-42a6-9bef-3a539c4817d3",
      "heading": "[26] Wenhu Chen, Hanwen Zha, Zhiyu Chen, Wenhan Xiong, Hong Wang, and William YangWang. HybridQA: A dataset of multi-hop question answering over tabular and textual data. InTrevor Cohn, Yulan He, and Yang Liu, editors, Findings of the Association for ComputationalLinguistics: EMNLP 2020, pages 1026–1036, Online, November 2020. Association forComputational Linguistics.doi: 10.18653/v1/2020.findings-emnlp.91.URL https://aclanthology.org/2020.findings-emnlp.91/.",
      "text": "[27] Zhiyu Chen, Wenhu Chen, Hanwen Zha, Xiyou Zhou, Yunkai Zhang, Sairam Sundaresan, andWilliam Yang Wang. Logic2text: High-fidelity natural language generation from logical forms.ArXiv, abs/2004.14579, 2020.\n\n[28] Sang Keun Choe, Hwijeen Ahn, Juhan Bae, Kewen Zhao, Minsoo Kang, Youngseog Chung,Adithya Pratapa, Willie Neiswanger, Emma Strubell, Teruko Mitamura, Jeff Schneider, EduardHovy, Roger Grosse, and Eric Xing. What is your data worth to gpt? llm-scale data valuationwith influence functions, 2024. URL https://arxiv.org/abs/2405.13954.\n\n[29] Eunsol Choi, He He, Mohit Iyyer, Mark Yatskar, Wen tau Yih, Yejin Choi, Percy Liang, andLuke Zettlemoyer. Quac: Question answering in context. In Conference on Empirical Methodsin Natural Language Processing, pages 2174–2184, 2018.",
      "start_page": 13,
      "end_page": 13
    },
    {
      "section_id": "a39b1b7b-5fea-4d24-989b-eaecb7e903e0",
      "heading": "[30] cjadams, Jeffrey Sorensen, Julia Elliott, Lucas Dixon, Mark McDonald, nithum, and WillCukierski. Toxic comment classification challenge. https://kaggle.com/competitions/jigsaw-toxic-comment-classification-challenge, 2017. Kaggle.",
      "text": "",
      "start_page": 13,
      "end_page": 13
    },
    {
      "section_id": "cb651d9a-6024-43b7-8318-3eebf13e7951",
      "heading": "[31] Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, andKristina Toutanova. Boolq: Exploring the surprising difficulty of natural yes/no questions.arXiv preprint arXiv:1905.10044, 2019.",
      "text": "",
      "start_page": 13,
      "end_page": 13
    },
    {
      "section_id": "fb828c67-6169-4b8f-b2f5-72e64e247ee3",
      "heading": "[32] Kevin Clark, Urvashi Khandelwal, Omer Levy, and Christopher D Manning. What doesBERT look at? an analysis of BERT’s attention. In Proceedings of the 2019 ACL WorkshopBlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, 2019.",
      "text": "",
      "start_page": 13,
      "end_page": 13
    },
    {
      "section_id": "ceb9c33c-b57e-4542-8516-48c5cee17918",
      "heading": "[33] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick,and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoningchallenge. arXiv preprint arXiv:1803.05457, 2018.",
      "text": "",
      "start_page": 13,
      "end_page": 13
    },
    {
      "section_id": "c0f658f5-0c98-4d5f-bb51-43b4d2dbdb69",
      "heading": "[34] Karl Cobbe, V. Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser,Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and JohnSchulman. Training verifiers to solve math word problems. ArXiv, abs/2110.14168, 2021.",
      "text": "",
      "start_page": 13,
      "end_page": 13
    },
    {
      "section_id": "018ecf4e-2a55-4a2d-aace-f1d557f30e5b",
      "heading": "[35] Creative Commons. CC0 1.0 Universal (CC0 1.0) Public Domain Dedication, 2025. URL",
      "text": "https://creativecommons.org/publicdomain/zero/1.0/.",
      "start_page": 13,
      "end_page": 13
    },
    {
      "section_id": "f9e8c502-baa2-48fd-af7b-74e5360e2606",
      "heading": "[36] Creative Commons. Creative Commons Attribution 4.0 International License § 2(a)(1)(A),2025. URL https://creativecommons.org/licenses/by/4.0/legalcode.",
      "text": "[37] Creative Commons. Creative Commons Attribution 4.0 International License § 3(a)(1)(A)(i),2025. URL https://creativecommons.org/licenses/by/4.0/legalcode.",
      "start_page": 13,
      "end_page": 13
    },
    {
      "section_id": "9c3b7779-693c-4626-ab7e-9869c1c6aba7",
      "heading": "[38] Creative Commons. Public Domain Mark 1.0, 2025. URL https://creativecommons.org/publicdomain/mark/1.",
      "text": "13",
      "start_page": 13,
      "end_page": 13
    },
    {
      "section_id": "6c25beaa-cc3a-4ea1-9c0d-a8701137408c",
      "heading": "[39] Creative Commons. Creative Commons Attribution-ShareAlike 4.0 International License,2025. URL https://creativecommons.org/licenses/by-sa/4.0/.",
      "text": "",
      "start_page": 14,
      "end_page": 14
    },
    {
      "section_id": "49211b9a-6724-40df-a0d9-398ad44407b9",
      "heading": "[40] A. Feder Cooper and James Grimmelmann. The Files are in the Computer: Copyright,Memorization, and Generative AI. arXiv preprint arXiv:2404.12590, 2024.",
      "text": "",
      "start_page": 14,
      "end_page": 14
    },
    {
      "section_id": "c8eb049a-92cf-4fbf-9bc5-9153521329c1",
      "heading": "[41] A. Feder Cooper, Aaron Gokaslan, Amy B. Cyphert, Christopher De Sa, Mark A. Lemley,Daniel E. Ho, and Percy Liang. Extracting memorized pieces of (copyrighted) books fromopen-weight language models. arXiv preprint arXiv:2505.12546, 2025.",
      "text": "",
      "start_page": 14,
      "end_page": 14
    },
    {
      "section_id": "7dde433e-ab17-4301-bcd7-73cda9669753",
      "heading": "[42] Yiming Cui, Ting Liu, Li Xiao, Zhipeng Chen, Wentao Ma, Wanxiang Che, Shijin Wang,and Guoping Hu. A span-extraction dataset for chinese machine reading comprehension. InEMNLP-IJCNLP, pages 5882–5888, 2019.",
      "text": "",
      "start_page": 14,
      "end_page": 14
    },
    {
      "section_id": "e9f01235-31ec-4888-b400-3b85595549e4",
      "heading": "[43] Bhavana Dalvi, Lifu Huang, Niket Tandon, Wen tau Yih, and Peter Clark. Tracking statechanges in procedural text: a challenge dataset and models for process paragraph comprehen-sion. In North American Chapter of the Association for Computational Linguistics, pages1595–1604, 2018.",
      "text": "[44] Pradeep Dasigi, Nelson F. Liu, Ana Marasovi´c, Noah A. Smith, and Matt Gardner. Quoref: Areading comprehension dataset with questions requiring coreferential reasoning. In Conferenceon Empirical Methods in Natural Language Processing, volume abs/1908.05803, 2019.",
      "start_page": 14,
      "end_page": 14
    },
    {
      "section_id": "77436a55-1f36-468d-a911-b38a2850e9c2",
      "heading": "[45] Ning Ding, Guangwei Xu, Yulin Chen, Xiaobin Wang, Xu Han, Pengjun Xie, HaitaoZheng, and Zhiyuan Liu. Few-nerd: A few-shot named entity recognition dataset. ArXiv,abs/2105.07464, 2021.",
      "text": "",
      "start_page": 14,
      "end_page": 14
    },
    {
      "section_id": "1f1362da-2f92-4c96-ba4b-fe741d96b7a6",
      "heading": "[46] Dheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, and MattGardner. Drop: A reading comprehension benchmark requiring discrete reasoning overparagraphs. In North American Chapter of the Association for Computational Linguistics,pages 2368–2378, 2019.",
      "text": "[47] Michael Duan, Anshuman Suri, Niloofar Mireshghallah, Sewon Min, Weijia Shi, Luke Zettle-moyer, Yulia Tsvetkov, Yejin Choi, David Evans, and Hannaneh Hajishirzi. Do membershipinference attacks work on large language models? arXiv preprint arXiv:2402.07841, 2024.",
      "start_page": 14,
      "end_page": 14
    },
    {
      "section_id": "037a3bb5-542d-4865-a4d7-965e6359304a",
      "heading": "[48] S. Dumitrescu, Petru Rebeja, Beáta L˝orincz, Mihaela G˘aman, M. Ilie, Andrei Pruteanu,Adriana Stan, Luciana Morogan, Traian Rebedea, and Sebastian Ruder. Liro: Benchmark andleaderboard for romanian language tasks. In NeurIPS Datasets and Benchmarks, 2021.",
      "text": "",
      "start_page": 14,
      "end_page": 14
    },
    {
      "section_id": "399713e4-952f-490d-ac6f-0ed0b12f729d",
      "heading": "[49] Yanai Elazar and Yoav Goldberg. Where’s my head? definition, data set, and models fornumeric fused-head identification and resolution. Transactions of the Association for Compu-tational Linguistics, 7:519–535, 2019.",
      "text": "",
      "start_page": 14,
      "end_page": 14
    },
    {
      "section_id": "9b8be3f1-54c1-455c-b390-3840454ca088",
      "heading": "[50] Yanai Elazar, Nora Kassner, Shauli Ravfogel, Amir Feder, Abhilasha Ravichander, MariusMosbach, Yonatan Belinkov, Hinrich Schütze, and Yoav Goldberg. Measuring causal effectsof data statistics on language model’sfactual’predictions. arXiv preprint arXiv:2207.14251,2022.",
      "text": "",
      "start_page": 14,
      "end_page": 14
    },
    {
      "section_id": "2915ff8a-e656-4193-b2e5-a70bb24d8017",
      "heading": "[51] Denis Emelin, Ronan Le Bras, Jena D. Hwang, Maxwell Forbes, and Yejin Choi. Moralstories: Situated reasoning about norms, intents, actions, and their consequences. ArXiv,abs/2012.15738, 2020.",
      "text": "",
      "start_page": 14,
      "end_page": 14
    },
    {
      "section_id": "72bfb4e4-2d7c-4995-8204-4d97884da6ee",
      "heading": "[52] Alex Fang, Hadi Pouransari, Matt Jordan, Alexander Toshev, Vaishaal Shankar, LudwigSchmidt, and Tom Gunter. Datasets, documents, and repetitions: The practicalities of unequaldata quality. arXiv preprint arXiv:2503.07879, 2025.",
      "text": "[53] James Ferguson, Matt Gardner, Tushar Khot, and Pradeep Dasigi. Iirc: A dataset of incompleteinformation reading comprehension questions. In Conference on Empirical Methods in NaturalLanguage Processing, pages 1137–1147, 2020.",
      "start_page": 14,
      "end_page": 14
    },
    {
      "section_id": "cd544e1f-6812-4b3a-86b3-123b728f23d3",
      "heading": "[54] Nancy Fulda, Nathan Tibbetts, Zachary Brown, and D. Wingate. Harvesting common-sensenavigational knowledge for robotics from uncurated text corpora. In Conference on RobotLearning, pages 525–534, 2017.",
      "text": "14\n\n[55] Philip Gage. A new algorithm for data compression. The C Users Journal archive, 12:23–38,1994. URL https://api.semanticscholar.org/CorpusID:59804030.\n\n[56] N. Gale, G. Heath, E. Cameron, S. Rashid, and S. Redwood. Using the framework method forthe analysis of qualitative data in multi-disciplinary health research. BMC Medical ResearchMethodology, 13:117 – 117, 2013.",
      "start_page": 14,
      "end_page": 15
    },
    {
      "section_id": "0baceaa1-c939-4664-a1dd-cd3cccfb891e",
      "heading": "[57] Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster,Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy.The pile: An 800gb dataset of diverse text for language modeling, 2020. URL https://arxiv.org/abs/2101.00027.",
      "text": "",
      "start_page": 15,
      "end_page": 15
    },
    {
      "section_id": "6b524f76-421d-41cf-910b-da1f7676e82c",
      "heading": "[58] Xinyang Geng and Hao Liu. Openllama: An open reproduction of llama, May 2023. URL",
      "text": "https://github.com/openlm-research/open_llama.",
      "start_page": 15,
      "end_page": 15
    },
    {
      "section_id": "bb509141-bbe5-4587-803a-79c3a87a7ca0",
      "heading": "[59] Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, D. Roth, and Jonathan Berant. Didaristotle use a laptop? a question answering benchmark with implicit reasoning strategies.Transactions of the Association for Computational Linguistics, 9:346–361, 2021.",
      "text": "[60] Max Glockner, Vered Shwartz, and Yoav Goldberg. Breaking nli systems with sentences thatrequire simple lexical inferences. ArXiv, abs/1805.02266, 2018.",
      "start_page": 15,
      "end_page": 15
    },
    {
      "section_id": "63021a12-345b-463f-808e-b31a02ce96b7",
      "heading": "[61] Aaron Gokaslan, A. Feder Cooper, Jasmine Collins, Landan Seguin, Austin Jacobson, MihirPatel, Jonathan Frankle, Cory Stephenson, and Volodymyr Kuleshov. CommonCanvas: OpenDiffusion Models Trained on Creative-Commons Images. In Proceedings of the IEEE/CVFConference on Computer Vision and Pattern Recognition (CVPR), pages 8250–8260, June2024.",
      "text": "",
      "start_page": 15,
      "end_page": 15
    },
    {
      "section_id": "1dd10ae1-72e9-4063-be79-09843ea94baf",
      "heading": "[62] Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian,Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, Amy Yang,Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravanku-mar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, Aurelien Rodriguez, AustenGregerson, Ava Spataru, Baptiste Roziere, Bethany Biron, Binh Tang, Bobbie Chern, Char-lotte Caucheteux, Chaya Nayak, Chloe Bi, Chris Marra, Chris McConnell, Christian Keller,Christophe Touret, Chunyang Wu, Corinne Wong, Cristian Canton Ferrer, Cyrus Nikolaidis,Damien Allonsius, Daniel Song, Danielle Pintz, Danny Livshits, Danny Wyatt, David Esiobu,Dhruv Choudhary, Dhruv Mahajan, Diego Garcia-Olano, Diego Perino, Dieuwke Hupkes,Egor Lakomkin, Ehab AlBadawy, Elina Lobanova, Emily Dinan, Eric Michael Smith, FilipRadenovic, Francisco Guzmán, Frank Zhang, Gabriel Synnaeve, Gabrielle Lee, Georgia LewisAnderson, Govind Thattai, Graeme Nail, Gregoire Mialon, Guan Pang, Guillem Cucurell,Hailey Nguyen, Hannah Korevaar, Hu Xu, Hugo Touvron, Iliyan Zarov, Imanol Arrieta Ibarra,Isabel Kloumann, Ishan Misra, Ivan Evtimov, Jack Zhang, Jade Copet, Jaewon Lee, JanGeffert, Jana Vranes, Jason Park, Jay Mahadeokar, Jeet Shah, Jelmer van der Linde, JenniferBillock, Jenny Hong, Jenya Lee, Jeremy Fu, Jianfeng Chi, Jianyu Huang, Jiawen Liu, JieWang, Jiecao Yu, Joanna Bitton, Joe Spisak, Jongsoo Park, Joseph Rocca, Joshua Johnstun,Joshua Saxe, Junteng Jia, Kalyan Vasuden Alwala, Karthik Prasad, Kartikeya Upasani, KatePlawiak, Ke Li, Kenneth Heafield, Kevin Stone, Khalid El-Arini, Krithika Iyer, Kshitiz Malik,Kuenley Chiu, Kunal Bhalla, Kushal Lakhotia, Lauren Rantala-Yeary, Laurens van der Maaten,Lawrence Chen, Liang Tan, Liz Jenkins, Louis Martin, Lovish Madaan, Lubo Malo, LukasBlecher, Lukas Landzaat, Luke de Oliveira, Madeline Muzzi, Mahesh Pasupuleti, MannatSingh, Manohar Paluri, Marcin Kardas, Maria Tsimpoukelli, Mathew Oldham, Mathieu Rita,Maya Pavlova, Melanie Kambadur, Mike Lewis, Min Si, Mitesh Kumar Singh, Mona Hassan,Naman Goyal, Narjes Torabi, Nikolay Bashlykov, Nikolay Bogoychev, Niladri Chatterji, NingZhang, Olivier Duchenne, Onur Çelebi, Patrick Alrassy, Pengchuan Zhang, Pengwei Li, PetarVasic, Peter Weng, Prajjwal Bhargava, Pratik Dubal, Praveen Krishnan, Punit Singh Koura,Puxin Xu, Qing He, Qingxiao Dong, Ragavan Srinivasan, Raj Ganapathy, Ramon Calderer,Ricardo Silveira Cabral, Robert Stojnic, Roberta Raileanu, Rohan Maheswari, Rohit Gird-har, Rohit Patel, Romain Sauvestre, Ronnie Polidoro, Roshan Sumbaly, Ross Taylor, RuanSilva, Rui Hou, Rui Wang, Saghar Hosseini, Sahana Chennabasappa, Sanjay Singh, SeanBell, Seohyun Sonia Kim, Sergey Edunov, Shaoliang Nie, Sharan Narang, Sharath Raparthy,Sheng Shen, Shengye Wan, Shruti Bhosale, Shun Zhang, Simon Vandenhende, Soumya Batra,",
      "text": "15",
      "start_page": 15,
      "end_page": 15
    },
    {
      "section_id": "34037ca2-cf36-4b1f-b6cf-c32c4d0c5ddb",
      "heading": "Spencer Whitman, Sten Sootla, Stephane Collot, Suchin Gururangan, Sydney Borodinsky,Tamar Herman, Tara Fowler, Tarek Sheasha, Thomas Georgiou, Thomas Scialom, TobiasSpeckbacher, Todor Mihaylov, Tong Xiao, Ujjwal Karn, Vedanuj Goswami, Vibhor Gupta, Vi-gnesh Ramanathan, Viktor Kerkez, Vincent Gonguet, Virginie Do, Vish Vogeti, Vítor Albiero,Vladan Petrovic, Weiwei Chu, Wenhan Xiong, Wenyin Fu, Whitney Meers, Xavier Martinet,Xiaodong Wang, Xiaofang Wang, Xiaoqing Ellen Tan, Xide Xia, Xinfeng Xie, Xuchao Jia,Xuewei Wang, Yaelle Goldschlag, Yashesh Gaur, Yasmine Babaei, Yi Wen, Yiwen Song,Yuchen Zhang, Yue Li, Yuning Mao, Zacharie Delpierre Coudert, Zheng Yan, ZhengxingChen, Zoe Papakipos, Aaditya Singh, Aayushi Srivastava, Abha Jain, Adam Kelsey, AdamShajnfeld, Adithya Gangidi, Adolfo Victoria, Ahuva Goldstand, Ajay Menon, Ajay Sharma,Alex Boesenberg, Alexei Baevski, Allie Feinstein, Amanda Kallet, Amit Sangani, Amos Teo,Anam Yunus, Andrei Lupu, Andres Alvarado, Andrew Caples, Andrew Gu, Andrew Ho,Andrew Poulton, Andrew Ryan, Ankit Ramchandani, Annie Dong, Annie Franco, Anuj Goyal,Aparajita Saraf, Arkabandhu Chowdhury, Ashley Gabriel, Ashwin Bharambe, Assaf Eisenman,Azadeh Yazdan, Beau James, Ben Maurer, Benjamin Leonhardi, Bernie Huang, Beth Loyd,Beto De Paola, Bhargavi Paranjape, Bing Liu, Bo Wu, Boyu Ni, Braden Hancock, Bram Wasti,Brandon Spence, Brani Stojkovic, Brian Gamido, Britt Montalvo, Carl Parker, Carly Burton,Catalina Mejia, Ce Liu, Changhan Wang, Changkyu Kim, Chao Zhou, Chester Hu, Ching-Hsiang Chu, Chris Cai, Chris Tindal, Christoph Feichtenhofer, Cynthia Gao, Damon Civin,Dana Beaty, Daniel Kreymer, Daniel Li, David Adkins, David Xu, Davide Testuggine, DeliaDavid, Devi Parikh, Diana Liskovich, Didem Foss, Dingkang Wang, Duc Le, Dustin Holland,Edward Dowling, Eissa Jamil, Elaine Montgomery, Eleonora Presani, Emily Hahn, EmilyWood, Eric-Tuan Le, Erik Brinkman, Esteban Arcaute, Evan Dunbar, Evan Smothers, Fei Sun,Felix Kreuk, Feng Tian, Filippos Kokkinos, Firat Ozgenel, Francesco Caggioni, Frank Kanayet,Frank Seide, Gabriela Medina Florez, Gabriella Schwarz, Gada Badeer, Georgia Swee, GilHalpern, Grant Herman, Grigory Sizov, Guangyi, Zhang, Guna Lakshminarayanan, HakanInan, Hamid Shojanazeri, Han Zou, Hannah Wang, Hanwen Zha, Haroun Habeeb, HarrisonRudolph, Helen Suk, Henry Aspegren, Hunter Goldman, Hongyuan Zhan, Ibrahim Damlaj,Igor Molybog, Igor Tufanov, Ilias Leontiadis, Irina-Elena Veliche, Itai Gat, Jake Weissman,James Geboski, James Kohli, Janice Lam, Japhet Asher, Jean-Baptiste Gaya, Jeff Marcus, JeffTang, Jennifer Chan, Jenny Zhen, Jeremy Reizenstein, Jeremy Teboul, Jessica Zhong, JianJin, Jingyi Yang, Joe Cummings, Jon Carvill, Jon Shepard, Jonathan McPhie, Jonathan Torres,Josh Ginsburg, Junjie Wang, Kai Wu, Kam Hou U, Karan Saxena, Kartikay Khandelwal,Katayoun Zand, Kathy Matosich, Kaushik Veeraraghavan, Kelly Michelena, Keqian Li, KiranJagadeesh, Kun Huang, Kunal Chawla, Kyle Huang, Lailin Chen, Lakshya Garg, Lavender A,Leandro Silva, Lee Bell, Lei Zhang, Liangpeng Guo, Licheng Yu, Liron Moshkovich, LucaWehrstedt, Madian Khabsa, Manav Avalani, Manish Bhatt, Martynas Mankus, Matan Hasson,Matthew Lennie, Matthias Reso, Maxim Groshev, Maxim Naumov, Maya Lathi, MeghanKeneally, Miao Liu, Michael L. Seltzer, Michal Valko, Michelle Restrepo, Mihir Patel, MikVyatskov, Mikayel Samvelyan, Mike Clark, Mike Macey, Mike Wang, Miquel Jubert Hermoso,Mo Metanat, Mohammad Rastegari, Munish Bansal, Nandhini Santhanam, Natascha Parks,Natasha White, Navyata Bawa, Nayan Singhal, Nick Egebo, Nicolas Usunier, Nikhil Mehta,Nikolay Pavlovich Laptev, Ning Dong, Norman Cheng, Oleg Chernoguz, Olivia Hart, OmkarSalpekar, Ozlem Kalinli, Parkin Kent, Parth Parekh, Paul Saab, Pavan Balaji, Pedro Rittner,Philip Bontrager, Pierre Roux, Piotr Dollar, Polina Zvyagina, Prashant Ratanchandani, PritishYuvraj, Qian Liang, Rachad Alao, Rachel Rodriguez, Rafi Ayub, Raghotham Murthy, RaghuNayani, Rahul Mitra, Rangaprabhu Parthasarathy, Raymond Li, Rebekkah Hogan, RobinBattey, Rocky Wang, Russ Howes, Ruty Rinott, Sachin Mehta, Sachin Siby, Sai Jayesh Bondu,Samyak Datta, Sara Chugh, Sara Hunt, Sargun Dhillon, Sasha Sidorov, Satadru Pan, SaurabhMahajan, Saurabh Verma, Seiji Yamamoto, Sharadh Ramaswamy, Shaun Lindsay, Shaun Lind-say, Sheng Feng, Shenghao Lin, Shengxin Cindy Zha, Shishir Patil, Shiva Shankar, ShuqiangZhang, Shuqiang Zhang, Sinong Wang, Sneha Agarwal, Soji Sajuyigbe, Soumith Chintala,Stephanie Max, Stephen Chen, Steve Kehoe, Steve Satterfield, Sudarshan Govindaprasad,Sumit Gupta, Summer Deng, Sungmin Cho, Sunny Virk, Suraj Subramanian, Sy Choudhury,Sydney Goldman, Tal Remez, Tamar Glaser, Tamara Best, Thilo Koehler, Thomas Robinson,Tianhe Li, Tianjun Zhang, Tim Matthews, Timothy Chou, Tzook Shaked, Varun Vontimitta,Victoria Ajayi, Victoria Montanez, Vijai Mohan, Vinay Satish Kumar, Vishal Mangla, VladIonescu, Vlad Poenaru, Vlad Tiberiu Mihailescu, Vladimir Ivanov, Wei Li, Wenchen Wang,Wenwen Jiang, Wes Bouaziz, Will Constable, Xiaocheng Tang, Xiaojian Wu, Xiaolan Wang,",
      "text": "16",
      "start_page": 16,
      "end_page": 16
    },
    {
      "section_id": "ad014f87-e98f-42e8-9775-145ce1b12a92",
      "heading": "Xilun Wu, Xinbo Gao, Yaniv Kleinman, Yanjun Chen, Ye Hu, Ye Jia, Ye Qi, Yenda Li, YilinZhang, Ying Zhang, Yossi Adi, Youngjin Nam, Yu, Wang, Yu Zhao, Yuchen Hao, YundiQian, Yunlu Li, Yuzi He, Zach Rait, Zachary DeVito, Zef Rosnbrick, Zhaoduo Wen, ZhenyuYang, Zhiwei Zhao, and Zhiyu Ma. The Llama 3 Herd of Models, November 2024. URL",
      "text": "http://arxiv.org/abs/2407.21783. arXiv:2407.21783 [cs].\n\n[63] Grobid. Grobid. https://github.com/kermitt2/grobid, 2008–2025.",
      "start_page": 17,
      "end_page": 17
    },
    {
      "section_id": "d256b70d-450e-4a8e-b418-f1e822e24ecc",
      "heading": "[64] Dirk Groeneveld, Iz Beltagy, Pete Walsh, Akshita Bhagia, Rodney Kinney, Oyvind Tafjord,Ananya Harsh Jha, Hamish Ivison, Ian Magnusson, Yizhong Wang, Shane Arora, David Atkin-son, Russell Authur, Khyathi Raghavi Chandu, Arman Cohan, Jennifer Dumas, Yanai Elazar,Yuling Gu, Jack Hessel, Tushar Khot, William Merrill, Jacob Morrison, Niklas Muennighoff,Aakanksha Naik, Crystal Nam, Matthew E. Peters, Valentina Pyatkin, Abhilasha Ravichander,Dustin Schwenk, Saurabh Shah, Will Smith, Emma Strubell, Nishant Subramani, MitchellWortsman, Pradeep Dasigi, Nathan Lambert, Kyle Richardson, Luke Zettlemoyer, Jesse Dodge,Kyle Lo, Luca Soldaini, Noah A. Smith, and Hannaneh Hajishirzi. Olmo: Accelerating thescience of language models, 2024. URL https://arxiv.org/abs/2402.00838.",
      "text": "",
      "start_page": 17,
      "end_page": 17
    },
    {
      "section_id": "20b2a77d-7da2-4f3c-9b5b-71cfe9762ef5",
      "heading": "[65] Yuling Gu, Oyvind Tafjord, Bailey Kuehl, Dany Haddad, Jesse Dodge, and HannanehHajishirzi.Olmes: A standard for language model evaluations, 2025.URL https://arxiv.org/abs/2406.08446.",
      "text": "",
      "start_page": 17,
      "end_page": 17
    },
    {
      "section_id": "b95fc97d-d6ba-468f-9884-d2b8ec52d616",
      "heading": "[66] Mandy Guo, Zihang Dai, Denny Vrandeˇci´c, and Rami Al-Rfou. Wiki-40b: Multilinguallanguage model dataset. In Proceedings of the Twelfth Language Resources and EvaluationConference, pages 2440–2452, 2020.",
      "text": "",
      "start_page": 17,
      "end_page": 17
    },
    {
      "section_id": "8149fa2a-5b44-498f-9ade-cdf079368d44",
      "heading": "[67] Aditya Gupta, Jiacheng Xu, Shyam Upadhyay, Diyi Yang, and Manaal Faruqui.Disfl-qa: A benchmark dataset for understanding disfluencies in question answering.ArXiv,abs/2106.04016, 2021.",
      "text": "",
      "start_page": 17,
      "end_page": 17
    },
    {
      "section_id": "3d7f0eb9-3901-4a39-8b3d-8d4dbed10704",
      "heading": "[68] Ivan Habernal, Omnia Zayed, and Iryna Gurevych. C4Corpus: Multilingual web-size corpuswith free license. In Nicoletta Calzolari, Khalid Choukri, Thierry Declerck, Sara Goggi,Marko Grobelnik, Bente Maegaard, Joseph Mariani, Helene Mazo, Asuncion Moreno, JanOdijk, and Stelios Piperidis, editors, Proceedings of the Tenth International Conference onLanguage Resources and Evaluation (LREC‘16), pages 914–922, Portorož, Slovenia, May2016. European Language Resources Association (ELRA). URL https://aclanthology.org/L16-1146/.",
      "text": "",
      "start_page": 17,
      "end_page": 17
    },
    {
      "section_id": "ed07a42b-919f-4ecf-b5ec-456b1d019c6b",
      "heading": "[69] Seth Hays.AI Training and Copyright Infringement:Solutions from Asia, Octo-ber 2024.URL https://www.techpolicy.press/ai-training-and-copyright-infringement-solutions-from-asia/.",
      "text": "",
      "start_page": 17,
      "end_page": 17
    },
    {
      "section_id": "4a936a6a-8e79-428c-9c52-86c03514e621",
      "heading": "[70] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, andJacob Steinhardt. Measuring massive multitask language understanding. arXiv preprintarXiv:2009.03300, 2020.",
      "text": "[71] Dan Hendrycks, Collin Burns, Anya Chen, and Spencer Ball. Cuad: An expert-annotated nlpdataset for legal contract review. ArXiv, abs/2103.06268, 2021.",
      "start_page": 17,
      "end_page": 17
    },
    {
      "section_id": "b7dcd5ef-5541-47e5-a2c0-3a9b394bf561",
      "heading": "[72] Jeremy Howard and Sebastian Ruder. Universal language model fine-tuning for text classi-fication. In Proceedings of the 56th Annual Meeting of the Association for ComputationalLinguistics, 2018.",
      "text": "[73] Shengding Hu, Yuge Tu, Xu Han, Chaoqun He, Ganqu Cui, Xiang Long, Zhi Zheng, YeweiFang, Yuxiang Huang, Weilin Zhao, et al. Minicpm: Unveiling the potential of small languagemodels with scalable training strategies. arXiv preprint arXiv:2404.06395, 2024.",
      "start_page": 17,
      "end_page": 17
    },
    {
      "section_id": "0ef672a2-1c1b-4e1a-9144-1b10024b4cf7",
      "heading": "[74] HuggingFace: Common Corpus, 2025.URL https://huggingface.co/datasets/PleIAs/common_corpus.",
      "text": "",
      "start_page": 17,
      "end_page": 17
    },
    {
      "section_id": "67ac5e9d-b8e5-4649-9d19-59b434ddb615",
      "heading": "[75] Jena D. Hwang, Chandra Bhagavatula, Ronan Le Bras, Jeff Da, Keisuke Sakaguchi, AntoineBosselut, and Yejin Choi. Comet-atomic 2020: On symbolic and neural commonsenseknowledge graphs. In AAAI Conference on Artificial Intelligence, pages 6384–6392, 2020.",
      "text": "17",
      "start_page": 17,
      "end_page": 17
    },
    {
      "section_id": "76193580-ac56-4c35-8930-1154279e7972",
      "heading": "[76] Adam Ibrahim, Benjamin Thérien, Kshitij Gupta, Mats L Richter, Quentin Anthony, TimothéeLesort, Eugene Belilovsky, and Irina Rish. Simple and scalable strategies to continuallypre-train large language models. arXiv preprint arXiv:2403.08763, 2024.",
      "text": "",
      "start_page": 18,
      "end_page": 18
    },
    {
      "section_id": "e71062b8-1102-46ec-a482-0a0483d7701f",
      "heading": "[77] IFI CLAIMS Patent Services and Google. Google patents public data. https://patents.google.com/, 2023. Licensed under a Creative Commons Attribution 4.0 InternationalLicense.",
      "text": "[78] Michael J Bommarito II, Jillian Bommarito, and Daniel Martin Katz. The kl3m data project:Copyright-clean training resources for large language models, 2025. URL https://arxiv.org/abs/2504.07854.",
      "start_page": 18,
      "end_page": 18
    },
    {
      "section_id": "687cb75d-7cd1-4c59-a42f-045a418885d9",
      "heading": "[79] Infocomm Media Development Authority of Singapore (IMDA), Aicadium, and AI VerifyFoundation. Model AI Governance Framework for Generative AI: Fostering a Trusted Ecosys-tem, May 2024. URL https://aiverifyfoundation.sg/wp-content/uploads/2024/05/Model-AI-Governance-Framework-for-Generative-AI-May-2024-1-1.pdf.",
      "text": "",
      "start_page": 18,
      "end_page": 18
    },
    {
      "section_id": "805e27b2-9d32-462a-824f-4461838aef72",
      "heading": "[80] Najko Jahn, Nick Haupka, and Anne Hobert. Analysing and reclassifying open access informa-tion in OpenAlex, 2023. URL https://subugoe.github.io/scholcomm_analytics/posts/oalex_oa_status/?utm_source=chatgpt.com.",
      "text": "",
      "start_page": 18,
      "end_page": 18
    },
    {
      "section_id": "1c0d242e-1672-4cdc-84a4-1e5a89f127b0",
      "heading": "[81] Armand Joulin, Edouard Grave, Piotr Bojanowski, and Tomas Mikolov. Bag of tricks forefficient text classification. In Proceedings of the 15th Conference of the European Chapterof the Association for Computational Linguistics: Volume 2, Short Papers, pages 427–431.Association for Computational Linguistics, April 2017.",
      "text": "[82] Nikhil Kandpal and Colin Raffel. Position: The most expensive part of an llm should be itstraining data. arXiv preprint arXiv:2504.12427, 2025.\n\n[83] Nikhil Kandpal, Eric Wallace, and Colin Raffel. Deduplicating training data mitigates privacyrisks in language models, 2022. URL https://arxiv.org/abs/2202.06539.",
      "start_page": 18,
      "end_page": 18
    },
    {
      "section_id": "4d3b15f2-45eb-44e9-8aa9-f4c5a2478af0",
      "heading": "[84] Nikhil Kandpal, Haikang Deng, Adam Roberts, Eric Wallace, and Colin Raffel. Largelanguage models struggle to learn long-tail knowledge. In International Conference onMachine Learning, pages 15696–15707. PMLR, 2023.",
      "text": "[85] Pride Kavumba, Naoya Inoue, Benjamin Heinzerling, Keshav Singh, Paul Reisert, and KentaroInui. When choosing plausible alternatives, clever hans can be clever. ArXiv, abs/1911.00225,2019.\n\n[86] Tushar Khot, Ashish Sabharwal, and Peter Clark. Scitail: A textual entailment dataset fromscience question answering. In AAAI Conference on Artificial Intelligence, pages 5189–5197,2018.",
      "start_page": 18,
      "end_page": 18
    },
    {
      "section_id": "705a8832-2671-4f0f-823a-86f772bebf27",
      "heading": "[87] Rodney Michael Kinney, Chloe Anastasiades, Russell Authur, Iz Beltagy, Jonathan Bragg,Alexandra Buraczynski, Isabel Cachola, Stefan Candra, Yoganand Chandrasekhar, ArmanCohan, Miles Crawford, Doug Downey, Jason Dunkelberger, Oren Etzioni, Rob Evans, SergeyFeldman, Joseph Gorney, David W. Graham, F.Q. Hu, Regan Huff, Daniel King, SebastianKohlmeier, Bailey Kuehl, Michael Langan, Daniel Lin, Haokun Liu, Kyle Lo, Jaron Lochner,Kelsey MacMillan, Tyler C. Murray, Christopher Newell, Smita R Rao, Shaurya Rohatgi,Paul Sayre, Zejiang Shen, Amanpreet Singh, Luca Soldaini, Shivashankar Subramanian,A. Tanaka, Alex D Wade, Linda M. Wagner, Lucy Lu Wang, Christopher Wilhelm, CarolineWu, Jiangjiang Yang, Angele Zamarron, Madeleine van Zuylen, and Daniel S. Weld. TheSemantic Scholar Open Data Platform. ArXiv, abs/2301.10140, 2023. URL https://api.semanticscholar.org/CorpusID:256194545.",
      "text": "",
      "start_page": 18,
      "end_page": 18
    },
    {
      "section_id": "158a3b54-96c5-4299-bae6-9c4a38c0f78f",
      "heading": "[88] Andreas Kopf, Yannic Kilcher, Dimitri von Rutte, Sotiris Anagnostidis, Zhi Rui Tam,K. Stevens, Abdullah Barhoum, Nguyen Minh Duc, Oliver Stanley, Rich’ard Nagyfi,ES Shahul, Sameer Suri, David Glushkov, Arnav Dantuluri, Andrew Maguire, ChristophSchuhmann, Huu Nguyen, and A. Mattick. Openassistant conversations - democratizing largelanguage model alignment. ArXiv, abs/2304.07327, 2023.",
      "text": "18",
      "start_page": 18,
      "end_page": 18
    },
    {
      "section_id": "b558771c-ae5b-44c7-b175-425ece7dfce6",
      "heading": "[89] T. Kwiatkowski, J. Palomaki, Olivia Redfield, Michael Collins, Ankur P. Parikh, Chris Alberti,D. Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones,Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc V. Le, and SlavPetrov. Natural questions: A benchmark for question answering research. Transactions of theAssociation for Computational Linguistics, 7:453–466, 2019.",
      "text": "[90] Faisal Ladhak, Esin Durmus, Claire Cardie, and K. McKeown. Wikilingua: A new benchmarkdataset for multilingual abstractive summarization. ArXiv, abs/2010.03093, 2020.\n\n[91] Pierre-Carl Langlais. Releasing Common Corpus: the largest public domain dataset for trainingLLMs, 2024. URL https://huggingface.co/blog/Pclanglais/common-corpus.",
      "start_page": 19,
      "end_page": 19
    },
    {
      "section_id": "2f760a33-38f6-4da0-9ad0-16535a493ec2",
      "heading": "[92] LDP Headquarters for the Promotion of Digital Society and Project Team onthe Evolution and Implementation of AIs.AI White Paper 2024:New Strate-giesinStageII,Towardtheworld’smostAI-friendlycountry,April2024.URL https://aiverifyfoundation.sg/wp-content/uploads/2024/05/Model-AI-Governance-Framework-for-Generative-AI-May-2024-1-1.pdf.",
      "text": "",
      "start_page": 19,
      "end_page": 19
    },
    {
      "section_id": "90958bca-a9f5-4c72-9f4d-cbe0eda7342e",
      "heading": "[93] R. Lebret, David Grangier, and Michael Auli. Neural text generation from structured datawith application to the biography domain. In Conference on Empirical Methods in NaturalLanguage Processing, pages 1203–1213, 2016.",
      "text": "",
      "start_page": 19,
      "end_page": 19
    },
    {
      "section_id": "7dad361e-9eb2-45ec-9a3b-62e1e0211530",
      "heading": "[94] Katherine Lee, Daphne Ippolito, Andrew Nystrom, Chiyuan Zhang, Douglas Eck, ChrisCallison-Burch, and Nicholas Carlini. Deduplicating training data makes language modelsbetter, 2022. URL https://arxiv.org/abs/2107.06499.",
      "text": "",
      "start_page": 19,
      "end_page": 19
    },
    {
      "section_id": "ad187718-ea3f-4db6-be50-e068e61e4a4b",
      "heading": "[95] Katherine Lee, A. Feder Cooper, James Grimmelmann, and Daphne Ippolito. AI and Law:The Next Generation. SSRN, 2023. http://dx.doi.org/10.2139/ssrn.4580739.",
      "text": "",
      "start_page": 19,
      "end_page": 19
    },
    {
      "section_id": "4b96dc35-9505-4ca3-92de-32f799ba8a04",
      "heading": "[96] Katherine Lee, A. Feder Cooper, and James Grimmelmann. Talkin’ ’Bout AI Generation:Copyright and the Generative-AI Supply Chain. arXiv preprint arXiv:2309.08133, 2023.",
      "text": "",
      "start_page": 19,
      "end_page": 19
    },
    {
      "section_id": "6ba3fe7e-b3bb-4add-b6ad-017efe7eb740",
      "heading": "[97] Jens Lehmann, Robert Isele, Max Jakob, Anja Jentzsch, D. Kontokostas, Pablo N. Mendes,Sebastian Hellmann, M. Morsey, Patrick van Kleef, S. Auer, and Christian Bizer. Dbpedia - alarge-scale, multilingual knowledge base extracted from wikipedia. Semantic Web, 6:167–195,2015.",
      "text": "[98] H. Levesque, E. Davis, and L. Morgenstern. The winograd schema challenge. In AAAI SpringSymposium: Logical Formalizations of Commonsense Reasoning, 2011.",
      "start_page": 19,
      "end_page": 19
    },
    {
      "section_id": "f10470a2-510a-442c-8614-95a88939dc06",
      "heading": "[99] Jeffrey Li, Alex Fang, Georgios Smyrnis, Maor Ivgi, Matt Jordan, Samir Gadre, HritikBansal, Etash Guha, Sedrick Keh, Kushal Arora, Saurabh Garg, Rui Xin, Niklas Muennighoff,Reinhard Heckel, Jean Mercat, Mayee Chen, Suchin Gururangan, Mitchell Wortsman, AlonAlbalak, Yonatan Bitton, Marianna Nezhurina, Amro Abbas, Cheng-Yu Hsieh, Dhruba Ghosh,Josh Gardner, Maciej Kilian, Hanlin Zhang, Rulin Shao, Sarah Pratt, Sunny Sanyal, GabrielIlharco, Giannis Daras, Kalyani Marathe, Aaron Gokaslan, Jieyu Zhang, Khyathi Chandu,Thao Nguyen, Igor Vasiljevic, Sham Kakade, Shuran Song, Sujay Sanghavi, Fartash Faghri,Sewoong Oh, Luke Zettlemoyer, Kyle Lo, Alaaeldin El-Nouby, Hadi Pouransari, AlexanderToshev, Stephanie Wang, Dirk Groeneveld, Luca Soldaini, Pang Wei Koh, Jenia Jitsev, ThomasKollar, Alexandros G. Dimakis, Yair Carmon, Achal Dave, Ludwig Schmidt, and VaishaalShankar. Datacomp-lm: In search of the next generation of training sets for language models,2025. URL https://arxiv.org/abs/2406.11794.",
      "text": "",
      "start_page": 19,
      "end_page": 19
    },
    {
      "section_id": "90a44b61-bd84-4155-a8f5-4e0e2967acbe",
      "heading": "[100] Xin Li and D. Roth.Learning question classifiers.In International Conference onComputational Linguistics, pages 1–7, 2002.",
      "text": "[101] Stephanie C. Lin, Jacob Hilton, and Owain Evans. Truthfulqa: Measuring how models mimichuman falsehoods. In Annual Meeting of the Association for Computational Linguistics, pages3214–3252, 2021.",
      "start_page": 19,
      "end_page": 19
    },
    {
      "section_id": "e56eb050-4a9a-450b-95de-665c71dee81f",
      "heading": "[102] Emmy Liu, Chenxuan Cui, Kenneth Zheng, and Graham Neubig. Testing the ability oflanguage models to interpret figurative language. ArXiv, abs/2204.12632, 2022.",
      "text": "19",
      "start_page": 19,
      "end_page": 19
    },
    {
      "section_id": "db5fba69-8ee0-4141-ba69-90ad5dfba05a",
      "heading": "[103] Zhengzhong Liu, Aurick Qiao, Willie Neiswanger, Hongyi Wang, Bowen Tan, TianhuaTao, Junbo Li, Yuqi Wang, Suqi Sun, Omkar Pangarkar, Richard Fan, Yi Gu, Victor Miller,Yonghao Zhuang, Guowei He, Haonan Li, Fajri Koto, Liping Tang, Nikhil Ranjan, ZhiqiangShen, Xuguang Ren, Roberto Iriondo, Cun Mu, Zhiting Hu, Mark Schulze, Preslav Nakov,Tim Baldwin, and Eric P. Xing. Llm360: Towards fully transparent open-source llms, 2023.URL https://arxiv.org/abs/2312.06550.",
      "text": "",
      "start_page": 20,
      "end_page": 20
    },
    {
      "section_id": "18434f94-5709-463d-981c-070fadd4cd56",
      "heading": "[104] Kyle Lo, Lucy Lu Wang, Mark Neumann, Rodney Kinney, and Daniel Weld.S2ORC:The semantic scholar open research corpus. In Proceedings of the 58th Annual Meetingof the Association for Computational Linguistics, pages 4969–4983, Online, July 2020.Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.447. URLhttps://www.aclweb.org/anthology/2020.acl-main.447.",
      "text": "",
      "start_page": 20,
      "end_page": 20
    },
    {
      "section_id": "5aa2257a-e0e8-4a2b-af42-0bc4fb0e1d69",
      "heading": "[105] Shayne Longpre, Stella Biderman, Alon Albalak, Hailey Schoelkopf, Daniel McDuff, SayashKapoor, Kevin Klyman, Kyle Lo, Gabriel Ilharco, Nay San, et al. The responsible foundationmodel development cheatsheet: A review of tools & resources. Transactions on MachineLearning Research, 2024.",
      "text": "",
      "start_page": 20,
      "end_page": 20
    },
    {
      "section_id": "043d6046-0db5-485b-b690-daf6146b62e7",
      "heading": "[106] Shayne Longpre, Robert Mahari, Anthony Chen, Naana Obeng-Marnu, Damien Sileo, WilliamBrannon, Niklas Muennighoff, Nathan Khazam, Jad Kabbara, Kartik Perisetla, Xinyi (Alexis)Wu, Enrico Shippole, Kurt Bollacker, Tongshuang Wu, Luis Villa, Sandy Pentland, andSara Hooker. A large-scale audit of dataset licensing and attribution in AI. Nature MachineIntelligence, 6(8):975–987, August 2024. doi: 10/gt8f5p.",
      "text": "",
      "start_page": 20,
      "end_page": 20
    },
    {
      "section_id": "a47be6cf-cbb1-4531-acc1-924364bbbf9e",
      "heading": "[107] Shayne Longpre, Robert Mahari, Ariel Lee, Campbell Lund, Hamidah Oderinwale, WilliamBrannon, Nayan Saxena, Naana Obeng-Marnu, Tobin South, Cole Hunter, Kevin Klyman,Christopher Klamm, Hailey Schoelkopf, Nikhil Singh, Manuel Cherep, Ahmad Anis, An Dinh,Caroline Chitongo, Da Yin, Damien Sileo, Deividas Mataciunas, Diganta Misra, EmadAlghamdi, Enrico Shippole, Jianguo Zhang, Joanna Materzynska, Kun Qian, Kush Tiwary,Lester Miranda, Manan Dey, Minnie Liang, Mohammed Hamdy, Niklas Muennighoff,Seonghyeon Ye, Seungone Kim, Shrestha Mohanty, Vipul Gupta, Vivek Sharma, Vu MinhChien, Xuhui Zhou, Yizhi Li, Caiming Xiong, Luis Villa, Stella Biderman, Hanlin Li, DaphneIppolito, Sara Hooker, Jad Kabbara, and Sandy Pentland. Consent in crisis: The rapid declineof the AI data commons. Advances in Neural Information Processing Systems, 37, 2024.",
      "text": "[108] Shayne Longpre, Robert Mahari, Naana Obeng-Marnu, William Brannon, Tobin South, KatyGero, Sandy Pentland, and Jad Kabbara. Data authenticity, consent, & provenance for ai areall broken: what will it take to fix them? arXiv preprint arXiv:2404.12691, 2024.",
      "start_page": 20,
      "end_page": 20
    },
    {
      "section_id": "edc7277b-a657-4f05-847c-3548b5f24a01",
      "heading": "[109] Shayne Longpre, Nikhil Singh, Manuel Cherep, Kushagra Tiwary, Joanna Materzynska,William Brannon, Robert Mahari, Naana Obeng-Marnu, Manan Dey, Mohammed Hamdy,et al.Bridging the data provenance gap across text, speech and video.arXiv preprintarXiv:2412.17847, 2024.",
      "text": "[110] Shayne Longpre, Gregory Yauney, Emily Reif, Katherine Lee, Adam Roberts, Barret Zoph,Denny Zhou, Jason Wei, Kevin Robinson, David Mimno, et al. A pretrainer’s guide to trainingdata: Measuring the effects of data age, domain coverage, quality, & toxicity. In Proceedingsof the 2024 Conference of the North American Chapter of the Association for ComputationalLinguistics: Human Language Technologies (Volume 1: Long Papers), pages 3245–3276, 2024.",
      "start_page": 20,
      "end_page": 20
    },
    {
      "section_id": "19a5eb89-eab7-44f0-857b-1cd41a000aa2",
      "heading": "[111] Ilya Loshchilov and Frank Hutter.Decoupled weight decay regularization.InInternationalConferenceonLearningRepresentations,2019.URLhttps://openreview.net/forum?id=Bkg6RiCqY7.",
      "text": "",
      "start_page": 20,
      "end_page": 20
    },
    {
      "section_id": "45fa8bd1-4f24-4521-b9ed-c2b4633e92fc",
      "heading": "[112] Annie Louis, D. Roth, and Filip Radlinski. “i’d rather just go to bed”’: Understanding indirectanswers. In Conference on Empirical Methods in Natural Language Processing, volumeabs/2010.03450, 2020.",
      "text": "",
      "start_page": 20,
      "end_page": 20
    },
    {
      "section_id": "1f5ee5ed-acb7-41f8-995e-61fe1ef922c3",
      "heading": "[113] Anton Lozhkov, Raymond Li, Loubna Ben Allal, Federico Cassano, Joel Lamy-Poirier,Nouamane Tazi, Ao Tang, Dmytro Pykhtar, Jiawei Liu, Yuxiang Wei, Tianyang Liu, Max Tian,Denis Kocetkov, Arthur Zucker, Younes Belkada, Zijian Wang, Qian Liu, Dmitry Abulkhanov,Indraneil Paul, Zhuang Li, Wen-Ding Li, Megan Risdal, Jia Li, Jian Zhu, Terry Yue Zhuo,",
      "text": "20",
      "start_page": 20,
      "end_page": 20
    },
    {
      "section_id": "7ed07633-7516-4e34-8a03-d9bb8ee067b5",
      "heading": "Evgenii Zheltonozhskii, Nii Osae Osae Dade, Wenhao Yu, Lucas Krauß, Naman Jain, YixuanSu, Xuanli He, Manan Dey, Edoardo Abati, Yekun Chai, Niklas Muennighoff, XiangruTang, Muhtasham Oblokulov, Christopher Akiki, Marc Marone, Chenghao Mou, MayankMishra, Alex Gu, Binyuan Hui, Tri Dao, Armel Zebaze, Olivier Dehaene, Nicolas Patry,Canwen Xu, Julian McAuley, Han Hu, Torsten Scholak, Sebastien Paquet, Jennifer Robinson,Carolyn Jane Anderson, Nicolas Chapados, Mostofa Patwary, Nima Tajbakhsh, Yacine Jernite,Carlos Muñoz Ferrandis, Lingming Zhang, Sean Hughes, Thomas Wolf, Arjun Guha, Leandrovon Werra, and Harm de Vries. Starcoder 2 and the stack v2: The next generation, 2024.",
      "text": "",
      "start_page": 21,
      "end_page": 21
    },
    {
      "section_id": "9a159a94-094e-4599-9dda-2e07cbb8550a",
      "heading": "[114] Robert Mahari and Shayne Longpre. Discit ergo est: Training data provenance and fair use.Robert Mahari and Shayne Longpre, Discit ergo est: Training Data Provenance And Fair Use,Dynamics of Generative AI (ed. Thibault Schrepel & Volker Stocker), Network Law Review,Winter, 2023.",
      "text": "[115] Matt Mahoney. Large text compression benchmark, 2011.",
      "start_page": 21,
      "end_page": 21
    },
    {
      "section_id": "64e84483-de96-465d-b638-714751aa5fd9",
      "heading": "[116] Stephen Merity, Caiming Xiong, James Bradbury, and R. Socher. Pointer sentinel mixturemodels. ArXiv, abs/1609.07843, 2016.",
      "text": "",
      "start_page": 21,
      "end_page": 21
    },
    {
      "section_id": "b16714e5-cead-4d76-854c-e12a987d7e00",
      "heading": "[117] Jean-BaptisteMichel,YuanKuiShen,AvivaPresserAiden,AdrianVeres,Matthew K. Gray,The Google Books Team,Joseph P. Pickett,Dale Hoiberg,Dan Clancy,Peter Norvig,Jon Orwant,Steven Pinker,Martin A. Nowak,andErez Lieberman Aiden.Quantitative analysis of culture using millions of digitizedbooks.Science, 331(6014):176–182, 2011.doi:10.1126/science.1199644.URLhttps://www.science.org/doi/abs/10.1126/science.1199644.",
      "text": "[118] Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a suit of armor conductelectricity? a new dataset for open book question answering. arXiv preprint arXiv:1809.02789,2018.",
      "start_page": 21,
      "end_page": 21
    },
    {
      "section_id": "f44a6e0d-a3ff-4dcb-8d05-17b15a105b19",
      "heading": "[119] Sewon Min, Suchin Gururangan, Eric Wallace, Weijia Shi, Hannaneh Hajishirzi, Noah A.Smith, and Luke Zettlemoyer. SILO language models: Isolating legal risk in a nonparametricdatastore. In The Twelfth International Conference on Learning Representations, 2024. URLhttps://openreview.net/forum?id=ruk0nyQPec.",
      "text": "",
      "start_page": 21,
      "end_page": 21
    },
    {
      "section_id": "9cdabc73-ffc1-4342-89c6-e2fed2440f20",
      "heading": "[120] Niklas Muennighoff, Alexander Rush, Boaz Barak, Teven Le Scao, Nouamane Tazi,Aleksandra Piktus, Sampo Pyysalo, Thomas Wolf, and Colin Raffel. Scaling data-constrainedlanguage models. Advances in Neural Information Processing Systems, 36, 2023.",
      "text": "",
      "start_page": 21,
      "end_page": 21
    },
    {
      "section_id": "a26a73a6-876f-4595-a2fe-0357a4cd88c7",
      "heading": "[121] Niklas Muennighoff, Alexander Rush, Boaz Barak, Teven Le Scao, Nouamane Tazi, Alek-sandra Piktus, Sampo Pyysalo, Thomas Wolf, and Colin A Raffel. Scaling data-constrainedlanguage models. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine,editors, Advances in Neural Information Processing Systems, volume 36, pages 50358–50376.Curran Associates, Inc., 2023. URL https://proceedings.neurips.cc/paper_files/paper/2023/file/9d89448b63ce1e2e8dc7af72c984c196-Paper-Conference.pdf.",
      "text": "",
      "start_page": 21,
      "end_page": 21
    },
    {
      "section_id": "e7afc4e4-0085-42a6-8e28-ce35465e8bed",
      "heading": "[122] Nikita Nangia, Clara Vania, Rasika Bhalerao, and Samuel R. Bowman. Crows-pairs: Achallenge dataset for measuring social biases in masked language models. In Conference onEmpirical Methods in Natural Language Processing, pages 1953–1967, 2020.",
      "text": "",
      "start_page": 21,
      "end_page": 21
    },
    {
      "section_id": "3fddd470-4d77-4850-bc04-98d21120bb47",
      "heading": "[123] Jekaterina Novikova, Ondrej Dusek, and Verena Rieser. The e2e dataset: New challengesfor end-to-end generation. ArXiv, abs/1706.09254, 2017.",
      "text": "[124] Tomoko Ohta, Sampo Pyysalo, Junichi Tsujii, and S. Ananiadou. Open-domain anatomicalentity mention detection. In Annual Meeting of the Association for Computational Linguistics,pages 27–36, 2012.",
      "start_page": 21,
      "end_page": 21
    },
    {
      "section_id": "85d945af-de78-4dc9-aedc-42f54ffd5e0c",
      "heading": "[125] Yasumasa Onoe, Michael J.Q. Zhang, Eunsol Choi, and Greg Durrett. Creak: A dataset forcommonsense reasoning over entity knowledge. ArXiv, abs/2109.01653, 2021.",
      "text": "[126] OpenAlex, 2025. URL https://openalex.org.\n\n21",
      "start_page": 21,
      "end_page": 21
    },
    {
      "section_id": "4d3f43d8-0b05-429b-bcff-f87e5684a8d3",
      "heading": "[127] Vassil Panayotov, Guoguo Chen, Daniel Povey, and S. Khudanpur. Librispeech: An asr corpusbased on public domain audio books. 2015 IEEE International Conference on Acoustics,Speech and Signal Processing (ICASSP), pages 5206–5210, 2015.",
      "text": "[128] Ashwinee Panda, Xinyu Tang, Milad Nasr, Christopher A Choquette-Choo, and Prateek Mittal.Privacy auditing of large language models. arXiv preprint arXiv:2503.06808, 2025.",
      "start_page": 22,
      "end_page": 22
    },
    {
      "section_id": "3e459458-59ee-4c5f-9aff-e3d041d4775a",
      "heading": "[129] SungMinPark,KristianGeorgiev,AndrewIlyas,GuillaumeLeclerc,andAleksander Madry.Trak:Attributing model behavior at scale,2023.URLhttps://arxiv.org/abs/2303.14186.",
      "text": "",
      "start_page": 22,
      "end_page": 22
    },
    {
      "section_id": "dc47aa68-f4d4-4e47-b920-4a6974ce88e6",
      "heading": "[130] European Parliament and Council of the European Union.Directive (eu) 2019/790,2019.URL https://eur-lex.europa.eu/legal-content/EN/ALL/?uri=CELEX:32019L0790#art_3.",
      "text": "[131] ParlParse. Parser for uk parliament proceedings. https://parser.theyworkforyou.com/,2025. Accessed: 2025-05-09.",
      "start_page": 22,
      "end_page": 22
    },
    {
      "section_id": "f4e00d6f-90de-4297-8d94-f29c3c63e221",
      "heading": "[132] Guilherme Penedo, Hynek Kydlíˇcek, Anton Lozhkov, Margaret Mitchell, Colin Raffel,Leandro Von Werra, and Thomas Wolf. The FineWeb datasets: Decanting the web for thefinest text data at scale. Advances in Neural Information Processing Systems, 37, 2024.",
      "text": "",
      "start_page": 22,
      "end_page": 22
    },
    {
      "section_id": "063ca12f-0455-4845-9dad-a571d1c77f52",
      "heading": "[133] Fabio Petroni, Tim Rocktäschel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, YuxiangWu, and Alexander Miller. Language models as knowledge bases? In Proceedings of the2019 Conference on Empirical Methods in Natural Language Processing, 2019.",
      "text": "",
      "start_page": 22,
      "end_page": 22
    },
    {
      "section_id": "adaed9a6-ee78-41a4-8a23-8cf473fd1c3a",
      "heading": "[134] E. Ponti, Goran Glavavs, Olga Majewska, Qianchu Liu, Ivan Vulic, and A. Korhonen. Xcopa:A multilingual dataset for causal commonsense reasoning. In Conference on EmpiricalMethods in Natural Language Processing, pages 2362–2376, 2020.",
      "text": "[135] Christopher Potts, Zhengxuan Wu, Atticus Geiger, and Douwe Kiela. Dynasent: A dynamicbenchmark for sentiment analysis. ArXiv, abs/2012.15349, 2020.\n\n[136] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving languageunderstanding by generative pre-training, 2018.",
      "start_page": 22,
      "end_page": 22
    },
    {
      "section_id": "7a8336c5-6679-4870-b536-4e778adf7972",
      "heading": "[137] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever.Language models are unsupervised multitask learners, 2019.",
      "text": "",
      "start_page": 22,
      "end_page": 22
    },
    {
      "section_id": "b2d1cd1d-1eea-4c0d-87bb-9b78dee239ed",
      "heading": "[138] Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and IlyaSutskever.Robust speech recognition via large-scale weak supervision, 2022.URLhttps://arxiv.org/abs/2212.04356.",
      "text": "",
      "start_page": 22,
      "end_page": 22
    },
    {
      "section_id": "d2027d4a-e794-4cb1-97e2-8f8b42045ab1",
      "heading": "[139] Filip Radlinski, K. Balog, B. Byrne, and K. Krishnamoorthi.Coached conversationalpreference elicitation: A case study in understanding movie preferences.In SIGDIALConferences, pages 353–360, 2019.",
      "text": "",
      "start_page": 22,
      "end_page": 22
    },
    {
      "section_id": "dee7f5b5-3025-4740-b6d8-50714bd70198",
      "heading": "[140] Jack W Rae, Anna Potapenko, Siddhant M Jayakumar, Chloe Hillier, and Timothy P Lillicrap.Compressive transformers for long-range sequence modelling. arXiv preprint, 2019. URLhttps://arxiv.org/abs/1911.05507.",
      "text": "",
      "start_page": 22,
      "end_page": 22
    },
    {
      "section_id": "d3fbc144-6e0f-4804-aecd-09e00b42d54d",
      "heading": "[141] Jack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song,John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, Eliza Rutherford, Tom Hen-nigan, Jacob Menick, Albin Cassirer, Richard Powell, George van den Driessche, Lisa AnneHendricks, Maribeth Rauh, Po-Sen Huang, Amelia Glaese, Johannes Welbl, SumanthDathathri, Saffron Huang, Jonathan Uesato, John F. J. Mellor, Irina Higgins, Antonia Creswell,Nat McAleese, Amy Wu, Erich Elsen, Siddhant M. Jayakumar, Elena Buchatskaya, David Bud-den, Esme Sutherland, Karen Simonyan, Michela Paganini, L. Sifre, Lena Martens, Xiang Lor-raine Li, Adhiguna Kuncoro, Aida Nematzadeh, Elena Gribovskaya, Domenic Donato, An-geliki Lazaridou, Arthur Mensch, Jean-Baptiste Lespiau, Maria Tsimpoukelli, N. K. Grigorev,Doug Fritz, Thibault Sottiaux, Mantas Pajarskas, Tobias Pohlen, Zhitao Gong, Daniel Toyama,Cyprien de Masson d’Autume, Yujia Li, Tayfun Terzi, Vladimir Mikulik, Igor Babuschkin,Aidan Clark, Diego de Las Casas, Aurelia Guy, Chris Jones, James Bradbury, Matthew G.",
      "text": "22",
      "start_page": 22,
      "end_page": 22
    },
    {
      "section_id": "32dc9f18-ab05-4eef-a2b0-c84426598996",
      "heading": "Johnson, Blake A. Hechtman, Laura Weidinger, Iason Gabriel, William S. Isaac, EdwardLockhart, Simon Osindero, Laura Rimell, Chris Dyer, Oriol Vinyals, Kareem W. Ayoub, JeffStanway, L. L. Bennett, Demis Hassabis, Koray Kavukcuoglu, and Geoffrey Irving. Scalinglanguage models: Methods, analysis & insights from training gopher. ArXiv, abs/2112.11446,2021. URL https://api.semanticscholar.org/CorpusID:245353475.",
      "text": "[142] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena,Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unifiedtext-to-text transformer. Journal of machine learning research, 21(140), 2020.",
      "start_page": 23,
      "end_page": 23
    },
    {
      "section_id": "6389069d-ea1c-4bf1-af4b-589b65b8d1d7",
      "heading": "[143] Robi Rahman and David Owen.The size of datasets used to train language modelsdoubles approximately every seven months, 2024.URL https://epoch.ai/data-insights/dataset-size-trend. Accessed: 2025-05-08.",
      "text": "[144] Nazneen Rajani, Bryan McCann, Caiming Xiong, and R. Socher. Explain yourself! leveraginglanguage models for commonsense reasoning. ArXiv, abs/1906.02361, 2019.",
      "start_page": 23,
      "end_page": 23
    },
    {
      "section_id": "39f3a5f2-6825-4cdc-ac3c-2194828a9dc2",
      "heading": "[145] Inioluwa Deborah Raji, Peggy Xu, Colleen Honigsberg, and Daniel Ho. Outsider oversight:Designing a third party audit ecosystem for ai governance. In Proceedings of the 2022AAAI/ACM Conference on AI, Ethics, and Society, pages 557–571, 2022.",
      "text": "",
      "start_page": 23,
      "end_page": 23
    },
    {
      "section_id": "1adfa47c-4151-4128-a907-d8989f9a0799",
      "heading": "[146] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. Squad: 100,000+questions for machine comprehension of text. In Conference on Empirical Methods in NaturalLanguage Processing, pages 2383–2392, 2016.",
      "text": "",
      "start_page": 23,
      "end_page": 23
    },
    {
      "section_id": "32597b26-2f6c-454e-b7b4-0d3d1d93a2d6",
      "heading": "[147] Pranav Rajpurkar, Robin Jia, and Percy Liang. Know what you don’t know: Unanswerablequestions for squad. In Annual Meeting of the Association for Computational Linguistics,volume abs/1806.03822, 2018.",
      "text": "",
      "start_page": 23,
      "end_page": 23
    },
    {
      "section_id": "5016847f-d046-45bb-84bf-3e605abe2309",
      "heading": "[148] Abhinav Rastogi, Xiaoxue Zang, Srinivas Sunkara, Raghav Gupta, and Pranav Khaitan.Schema-guided dialogue state tracking task at dstc8. ArXiv, abs/2002.01359, 2020.",
      "text": "[149] Abhilasha Ravichander, Matt Gardner, and Ana Marasovi´c. Condaqa: A contrastive readingcomprehension dataset for reasoning about negation. ArXiv, abs/2211.00295, 2022.",
      "start_page": 23,
      "end_page": 23
    },
    {
      "section_id": "ef1cfe85-dae9-4396-8676-950d05499310",
      "heading": "[150] Varshini Reddy, Craig W. Schmidt, Yuval Pinter, and Chris Tanner.How muchis enough?the diminishing returns of tokenization training data, 2025.URLhttps://arxiv.org/abs/2502.20273.",
      "text": "",
      "start_page": 23,
      "end_page": 23
    },
    {
      "section_id": "58b3c323-2ea3-42ba-bd6a-419e55768590",
      "heading": "[151] Hammam Riza, Michael Purwoadi, Gunarso, Teduh Uliniansyah, Aw Ai Ti, Sharifah MahaniAljunied, Luong Chi Mai, V. Thang, N. Thai, Vichet Chea, Rapid Sun, Sethserey Sam,Sopheap Seng, K. Soe, K. Nwet, M. Utiyama, and Chenchen Ding. Introduction of the asianlanguage treebank. In Oriental COCOSDA International Conference on Speech Databaseand Assessments, pages 1–6, 2016.",
      "text": "",
      "start_page": 23,
      "end_page": 23
    },
    {
      "section_id": "313ee7c4-bc07-44e8-8453-7cd6bf9733c1",
      "heading": "[152] Adam Roberts, Colin Raffel, and Noam Shazeer. How much knowledge can you pack intothe parameters of a language model? In Proceedings of the 2020 Conference on EmpiricalMethods in Natural Language Processing (EMNLP), 2020.",
      "text": "",
      "start_page": 23,
      "end_page": 23
    },
    {
      "section_id": "12d941fa-a986-4eca-869f-f0142f35e4e4",
      "heading": "[153] Anna Rogers, Olga Kovaleva, Matthew Downey, and Anna Rumshisky. Getting closer toai complete question answering: A set of prerequisite real tasks. In AAAI Conference onArtificial Intelligence, pages 8722–8731, 2020.",
      "text": "[154] Anna Rogers, Olga Kovaleva, and Anna Rumshisky. A primer in bertology: What we knowabout how BERT works. Transactions of the association for computational linguistics, 8, 2021.",
      "start_page": 23,
      "end_page": 23
    },
    {
      "section_id": "15ea9fd1-42a5-4335-b7a5-2bc421eb011a",
      "heading": "[155] Rachel Rudinger, Vered Shwartz, Jena D. Hwang, Chandra Bhagavatula, Maxwell Forbes,Ronan Le Bras, Noah A. Smith, and Yejin Choi. Thinking like a skeptic: Defeasible inferencein natural language. In Trevor Cohn, Yulan He, and Yang Liu, editors, Findings of the As-sociation for Computational Linguistics: EMNLP 2020, pages 4661–4675, Online, November2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.findings-emnlp.418.URL https://aclanthology.org/2020.findings-emnlp.418/.",
      "text": "23",
      "start_page": 23,
      "end_page": 23
    },
    {
      "section_id": "e07d4ce3-5ca0-4896-a6cf-983abc17a99e",
      "heading": "[156] Matthew Sag and Peter K. Yu.The globalization of copyright exceptions for ai train-ing. Emory Law Journal, 74, 2025. doi: http://dx.doi.org/10.2139/ssrn.4976393. URLhttps://ssrn.com/abstract=4976393.",
      "text": "",
      "start_page": 24,
      "end_page": 24
    },
    {
      "section_id": "eca8f4a1-d2bf-46f1-b4b7-879cae68ef57",
      "heading": "[157] Swarnadeep Saha, Yixin Nie, and Mohit Bansal. Conjnli: Natural language inference overconjunctive sentences. In Conference on Empirical Methods in Natural Language Processing,pages 8240–8252, 2020.",
      "text": "",
      "start_page": 24,
      "end_page": 24
    },
    {
      "section_id": "a3aa099a-bc04-4819-a77c-0ad6e15f776a",
      "heading": "[158] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande.Communications of the ACM, 64:99 – 106, 2019.",
      "text": "",
      "start_page": 24,
      "end_page": 24
    },
    {
      "section_id": "d3ba98fe-33c5-4cf7-a647-ffb6b2b4c405",
      "heading": "[159] Pamela Samuelson. How to Think About Remedies in the Generative AI Copyright Cases.Lawfare, February 2024.URL https://www.lawfaremedia.org/article/how-to-think-about-remedies-in-the-generative-ai-copyright-cases.",
      "text": "",
      "start_page": 24,
      "end_page": 24
    },
    {
      "section_id": "e20a7966-2c60-4857-8a96-8b5d234ca3ce",
      "heading": "[160] MaartenSap,HannahRashkin,DerekChen,RonanLeBras,andYejinChoi.Socialiqa:Commonsensereasoningaboutsocialinteractions,2019.URLhttps://arxiv.org/abs/1904.09728.",
      "text": "",
      "start_page": 24,
      "end_page": 24
    },
    {
      "section_id": "96f0741f-6733-4369-bc62-9f07401c0fcf",
      "heading": "[161] A. Sboev, A. Naumov, and R. Rybka. Data-driven model for emotion detection in russiantexts. In BICAAI, pages 637–642, 2020.",
      "text": "",
      "start_page": 24,
      "end_page": 24
    },
    {
      "section_id": "babcdb92-15d0-4b5c-90a0-64819be1e458",
      "heading": "[162] Tal Schuster, Adam Fisch, and R. Barzilay. Get your vitamin c! robust fact verificationwith contrastive evidence. In North American Chapter of the Association for ComputationalLinguistics, pages 624–643, 2021.",
      "text": "[163] Emily Sheng and David C. Uthus. Investigating societal biases in a poetry composition system.ArXiv, abs/2011.02686, 2020.",
      "start_page": 24,
      "end_page": 24
    },
    {
      "section_id": "796bae0d-4de5-4ab5-87ee-76d0bc139ebf",
      "heading": "[164] Mohit Shridhar, Xingdi Yuan, Marc-Alexandre Côté, Yonatan Bisk, Adam Trischler, andMatthew J. Hausknecht. Alfworld: Aligning text and embodied environments for interactivelearning. ArXiv, abs/2010.03768, 2020.",
      "text": "",
      "start_page": 24,
      "end_page": 24
    },
    {
      "section_id": "8c074a13-d7aa-459a-9c04-4789d4e9de63",
      "heading": "[165] Shivalika Singh, Freddie Vargus, Daniel Dsouza, B\"orje F. Karlsson, Abinaya Mahendiran,Wei-Yin Ko, Herumb Shandilya, Jay Patel, Deividas Mataciunas, Laura OMahony, MikeZhang, Ramith Hettiarachchi, Joseph Wilson, Marina Machado, Luisa Souza Moura,Dominik Krzemi’nski, Hakimeh Fadaei, Irem Ergun, Ifeoma Okoh, Aisha Alaagib, OshanMudannayake, Zaid Alyafeai, Minh Chien Vu, Sebastian Ruder, Surya Guthikonda,Emad A. Alghamdi, Sebastian Gehrmann, Niklas Muennighoff, Max Bartolo, JuliaKreutzer, A. Ustun, Marzieh Fadaee, and Sara Hooker.Aya dataset: An open-accesscollection for multilingual instruction tuning.ArXiv, abs/2402.06619, 2024.URLhttps://api.semanticscholar.org/CorpusID:267617144.",
      "text": "",
      "start_page": 24,
      "end_page": 24
    },
    {
      "section_id": "63d2afdc-ce81-4eb1-8cf3-42621b594005",
      "heading": "[166] Luca Soldaini and Kyle Lo. peS2o (Pretraining Efficiently on S2ORC) Dataset. Technicalreport, Allen Institute for AI, 2023. ODC-By, https://github.com/allenai/pes2o.",
      "text": "",
      "start_page": 24,
      "end_page": 24
    },
    {
      "section_id": "75a3915f-e843-4a46-b1f1-61203beb2cf4",
      "heading": "[167] Luca Soldaini, Rodney Kinney, Akshita Bhagia, Dustin Schwenk, David Atkinson, RussellAuthur, Ben Bogin, Khyathi Chandu, Jennifer Dumas, Yanai Elazar, Valentin Hofmann,Ananya Harsh Jha, Sachin Kumar, Li Lucy, Xinxi Lyu, Nathan Lambert, Ian Magnusson, JacobMorrison, Niklas Muennighoff, Aakanksha Naik, Crystal Nam, Matthew E. Peters, AbhilashaRavichander, Kyle Richardson, Zejiang Shen, Emma Strubell, Nishant Subramani, OyvindTafjord, Pete Walsh, Luke Zettlemoyer, Noah A. Smith, Hannaneh Hajishirzi, Iz Beltagy,Dirk Groeneveld, Jesse Dodge, and Kyle Lo. Dolma: an open corpus of three trillion tokensfor language model pretraining research. In Proceedings of the 62nd Annual Meeting of theAssociation for Computational Linguistics, 2024.",
      "text": "[168] Gabriel Stanovsky, Noah A. Smith, and Luke Zettlemoyer. Evaluating gender bias in machinetranslation. ArXiv, abs/1906.00591, 2019.",
      "start_page": 24,
      "end_page": 24
    },
    {
      "section_id": "015031f6-b05f-46cd-a94a-c76f967c278e",
      "heading": "[169] Pedro Javier Ortiz Suárez, Benoît Sagot, and Laurent Romary. Asynchronous pipeline forprocessing huge corpora on medium to low resource infrastructures. In 7th Workshop on theChallenges in the Management of Large Corpora (CMLC-7). Leibniz-Institut für DeutscheSprache, 2019.",
      "text": "24\n\n[170] Oyvind Tafjord, Matt Gardner, Kevin Lin, and Peter Clark. Quartz: An open-domain dataset ofqualitative relationship questions. In Conference on Empirical Methods in Natural LanguageProcessing, volume abs/1909.03553, 2019.",
      "start_page": 24,
      "end_page": 25
    },
    {
      "section_id": "5b554eec-3621-48b9-b106-93b978a8bbf9",
      "heading": "[171] Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. Commonsenseqa:A question answering challenge targeting commonsense knowledge.arXiv preprintarXiv:1811.00937, 2018.",
      "text": "",
      "start_page": 25,
      "end_page": 25
    },
    {
      "section_id": "bc5ff0f3-f51a-43fb-a85a-1b40b2f5d497",
      "heading": "[172] Niket Tandon, Keisuke Sakaguchi, Bhavana Dalvi, Dheeraj Rajagopal, Peter Clark, MichalGuerquin, Kyle Richardson, and E. Hovy. A dataset for tracking entities in open domainprocedural text. ArXiv, abs/2011.08092, 2020.",
      "text": "",
      "start_page": 25,
      "end_page": 25
    },
    {
      "section_id": "24bff7e1-8662-4d88-a811-7828cdb2bbf7",
      "heading": "[173] Liping Tang, Nikhil Ranjan, Omkar Pangarkar, Xuezhi Liang, Zhen Wang, Li An, BhaskarRao, Linghao Jin, Huijuan Wang, Zhoujun Cheng, Suqi Sun, Cun Mu, Victor Miller, XuezheMa, Yue Peng, Zhengzhong Liu, and Eric P. Xing. Txt360: A top-quality llm pre-trainingdataset requires the perfect blend, 2024.",
      "text": "[174] Ishan Tarunesh, Somak Aditya, and M. Choudhury. Trusting roberta over bert: Insights fromchecklisting the natural language inference task. ArXiv, abs/2107.07229, 2021.\n\n[175] MosaicML NLP Team. Introducing mpt-7b: A new standard for open-source, commerciallyusable llms, 2023. URL www.mosaicml.com/blog/mpt-7b. Accessed: 2023-05-05.\n\n[176] Qwen Team. Qwen3, April 2025. URL https://qwenlm.github.io/blog/qwen3/.",
      "start_page": 25,
      "end_page": 25
    },
    {
      "section_id": "c50c2c1e-0ea7-4e93-ad65-6787657981f3",
      "heading": "[177] James Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal. Fever: alarge-scale dataset for fact extraction and verification. ArXiv, abs/1803.05355, 2018.",
      "text": "",
      "start_page": 25,
      "end_page": 25
    },
    {
      "section_id": "26f6322a-84aa-495e-a7dc-0fbe69c0dd29",
      "heading": "[178] Anvith Thudi, Evianne Rovers, Yangjun Ruan, Tristan Thrush, and Chris J. Mad-dison.Mixmin:Finding data mixtures via convex minimization, 2025.URLhttps://arxiv.org/abs/2502.10510.",
      "text": "",
      "start_page": 25,
      "end_page": 25
    },
    {
      "section_id": "c10c5a39-c2a9-4d1a-8a89-a352ef2af177",
      "heading": "[179] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux,Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, AurelienRodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficientfoundation language models, 2023. URL https://arxiv.org/abs/2302.13971.",
      "text": "[180] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei,Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Openfoundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.",
      "start_page": 25,
      "end_page": 25
    },
    {
      "section_id": "c40bc395-fd64-4ff6-b673-c2d39702f1e2",
      "heading": "[181] UK Parliament.Open parliament license.https://www.parliament.uk/site-information/copyright-parliament/open-parliament-licence/,Unknown.Accessed: 2025-05-09.",
      "text": "[182] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural informationprocessing systems, 30, 2017.",
      "start_page": 25,
      "end_page": 25
    },
    {
      "section_id": "3aff39a1-9abc-4bbe-9ff6-87d9c06f052b",
      "heading": "[183] Mathurin Videau, Badr Youbi Idrissi, Daniel Haziza, Luca Wehrstedt, Jade Copet, OlivierTeytaud, and David Lopez-Paz. Meta Lingua: A minimal PyTorch LLM training library, 2024.URL https://github.com/facebookresearch/lingua.",
      "text": "",
      "start_page": 25,
      "end_page": 25
    },
    {
      "section_id": "b883f0ac-f49e-4a19-8132-ae1c355d1fb3",
      "heading": "[184] Zhilin Wang, Yi Dong, Jiaqi Zeng, Virginia Adams, Makesh Narsimhan Sreedhar, Daniel Egert,Olivier Delalleau, Jane Polak Scowcroft, Neel Kant, Aidan Swope, and Oleksii Kuchaiev.Helpsteer: Multi-attribute helpfulness dataset for steerlm. ArXiv, abs/2311.09528, 2023.",
      "text": "",
      "start_page": 25,
      "end_page": 25
    },
    {
      "section_id": "c0f6c56b-d5f5-450b-839f-eb7ebe6a75c6",
      "heading": "[185] Maurice Weber, Daniel Fu, Quentin Anthony, Yonatan Oren, Shane Adams, Anton Alexandrov,Xiaozhong Lyu, Huu Nguyen, Xiaozhe Yao, Virginia Adams, Ben Athiwaratkun, RahulChalamala, Kezhen Chen, Max Ryabinin, Tri Dao, Percy Liang, Christopher Ré, Irina Rish,and Ce Zhang. Redpajama: an open dataset for training large language models, 2024. URLhttps://arxiv.org/abs/2411.12372.",
      "text": "25",
      "start_page": 25,
      "end_page": 25
    },
    {
      "section_id": "c4a483ea-ad7f-4d82-99d0-0237c52e2734",
      "heading": "[186] Kellie Webster, Marta Recasens, Vera Axelrod, and Jason Baldridge. Resolving genderedambiguous pronouns with bert. ArXiv, abs/1906.01161, 2019.",
      "text": "",
      "start_page": 26,
      "end_page": 26
    },
    {
      "section_id": "ba194095-37d4-4db5-9e86-c903c3a05f63",
      "heading": "[187] Wei Wei, Quoc V. Le, Andrew M. Dai, and Jia Li.Airdialogue: An environment forgoal-oriented dialogue research. In Conference on Empirical Methods in Natural LanguageProcessing, pages 3844–3854, 2018.",
      "text": "[188] Alexander Wettig, Kyle Lo, Sewon Min, Hannaneh Hajishirzi, Danqi Chen, and Luca Soldaini.Organize the web: Constructing domains enhances pre-training data curation. arXiv preprintarXiv:2502.10341, 2025.",
      "start_page": 26,
      "end_page": 26
    },
    {
      "section_id": "e8e14a4c-195b-467b-a94e-aa1174d42b1b",
      "heading": "[189] Sang Michael Xie, Hieu Pham, Xuanyi Dong, Nan Du, Hanxiao Liu, Yifeng Lu, Percy S Liang,Quoc V Le, Tengyu Ma, and Adams Wei Yu. Doremi: Optimizing data mixtures speeds uplanguage model pretraining. Advances in Neural Information Processing Systems, 36, 2023.",
      "text": "",
      "start_page": 26,
      "end_page": 26
    },
    {
      "section_id": "cda43841-8e21-4178-a673-7fc8210aac39",
      "heading": "[190] Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W. Cohen, R. Salakhutdinov,and Christopher D. Manning. Hotpotqa: A dataset for diverse, explainable multi-hop questionanswering. In Conference on Empirical Methods in Natural Language Processing, pages2369–2380, 2018.",
      "text": "[191] Cat Zakrzewski, Nitasha Tiku, and Elizabeth Dwoskin. OpenAI prepares to fight for its lifeas legal troubles mount. The Washington Post, 2024.",
      "start_page": 26,
      "end_page": 26
    },
    {
      "section_id": "087c6624-f1e9-42e3-832a-918210956302",
      "heading": "[192] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Cana machine really finish your sentence? arXiv preprint arXiv:1905.07830, 2019.",
      "text": "",
      "start_page": 26,
      "end_page": 26
    },
    {
      "section_id": "d01cc9df-8ca9-4c37-a2f0-1bf4a8724590",
      "heading": "[193] Ge Zhang, Scott Qu, Jiaheng Liu, Chenchen Zhang, Chenghua Lin, Chou Leuang Yu, DannyPan, Esther Cheng, Jie Liu, Qunshu Lin, Raven Yuan, Tuney Zheng, Wei Pang, Xinrun Du,Yiming Liang, Yinghao Ma, Yizhi Li, Ziyang Ma, Bill Lin, Emmanouil Benetos, Huan Yang,Junting Zhou, Kaijing Ma, Minghao Liu, Morry Niu, Noah Wang, Quehry Que, Ruibo Liu,Sine Liu, Shawn Guo, Soren Gao, Wangchunshu Zhou, Xinyue Zhang, Yizhi Zhou, YuboWang, Yuelin Bai, Yuhan Zhang, Yuxiang Zhang, Zenith Wang, Zhenzhu Yang, Zijian Zhao,Jiajun Zhang, Wanli Ouyang, Wenhao Huang, and Wenhu Chen. MAP-Neo: Highly capableand transparent bilingual large language model series. arXiv preprint arXiv:2405.19327, 2024.",
      "text": "[194] Hongming Zhang, Xinran Zhao, and Yangqiu Song. Winowhy: A deep diagnosis of essentialcommonsense knowledge for answering winograd schema challenge. In Annual Meeting ofthe Association for Computational Linguistics, pages 5736–5745, 2020.\n\n[195] Wenting Zhao, Xiang Ren, Jack Hessel, Claire Cardie, Yejin Choi, and Yuntian Deng. Wildchat:1m chatGPT interaction logs in the wild. In The Twelfth International Conference on LearningRepresentations, 2024. URL https://openreview.net/forum?id=Bl8u7ZRlbM.",
      "start_page": 26,
      "end_page": 26
    },
    {
      "section_id": "49e53263-9dc4-4a80-8279-8274b5a4806b",
      "heading": "[196] Ben Zhou, Kyle Richardson, Qiang Ning, Tushar Khot, Ashish Sabharwal, and D. Roth.Temporal reasoning on implicit events from distant supervision. ArXiv, abs/2010.12753, 2020.",
      "text": "26",
      "start_page": 26,
      "end_page": 26
    },
    {
      "section_id": "2bc555cc-6021-4a0b-b9a8-e7be68f956a1",
      "heading": "Appendix",
      "text": "",
      "start_page": 27,
      "end_page": 27
    },
    {
      "section_id": "aaf220ab-b0a8-49f5-b89b-184d147c4f93",
      "heading": "Table of Contents",
      "text": "A Contributions28\n\nBDetailed Description of Sources28B.1Scientific and Scholarly Text. . . . . . . . . . . . . . . . . . . . . . . . . . .28B.2Online Discussions and Forums . . . . . . . . . . . . . . . . . . . . . . . . . .29B.3Government and Legal Texts. . . . . . . . . . . . . . . . . . . . . . . . . . .30B.4Curated Task Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .30B.5Books in the Public Domain . . . . . . . . . . . . . . . . . . . . . . . . . . . .31B.6Open Educational Resources. . . . . . . . . . . . . . . . . . . . . . . . . . .31B.7Wikis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .32B.8Source Code . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .32B.9Transcribed Audio Content. . . . . . . . . . . . . . . . . . . . . . . . . . . .33B.10 Web Text . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .33\n\nC Additional insights on licensing33C.1Why we can’t always trust automatic license detection . . . . . . . . . . . . . .34\n\nD List of Data Provenance Initiative sources34\n\nEList of News sources44\n\nFList of WikiMedia wikis44\n\nG CCCC Source Statistics44\n\nH PeS2o Source Statistics46\n\nIGrowth rates of openly licensed data47\n\nJDetails on filtering pipelines47\n\nK Details on Comma’s pre-training data mixture49\n\nLDetails on Comma’s cool-down data mixture51\n\nM Details on small-scale data ablations52\n\nN Additional Comma results52\n\nO Additional training runs53O.1 Ablations at 1T Tokens. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .53\n\n27",
      "start_page": 27,
      "end_page": 27
    },
    {
      "section_id": "e9c05321-97b0-48b9-8791-9d9a7eb21a74",
      "heading": "AContributions",
      "text": "Figure 5: Author contributions to this work. Large squares indicate a major contribution and smallsquares indicate a supporting contribution.",
      "start_page": 28,
      "end_page": 28
    },
    {
      "section_id": "775d46b6-4d9e-4fa4-91c1-c75bc1451fcd",
      "heading": "BDetailed Description of Sources",
      "text": "Below, we give a more in-depth overview of the sources that make up the Common Pile, includingspecific license decisions and tools used during collection.\n\nB.1Scientific and Scholarly Text",
      "start_page": 28,
      "end_page": 28
    },
    {
      "section_id": "35e8c153-7737-4227-8435-32f7e45eb3bf",
      "heading": "Scientific and scholarly texts are a staple of modern LLM pretraining corpora, appearing in nearly alllarge-scale datasets [e.g. 57, 185, 167] since they expose models to technical terminology, formalreasoning, and long-range document structure—skills that are essential for downstream tasks inscience, education, and question answering. Thanks to open access mandates and academic culturalnorms, many scholarly texts are either in the public domain or are distributed under open licenses.",
      "text": "",
      "start_page": 28,
      "end_page": 28
    },
    {
      "section_id": "b5415719-262a-473e-9ac1-8edb3e3ccad7",
      "heading": "peS2oTo ensure broad coverage across many scientific disciplines, we include a version ofpeS2o [166] restricted to openly licensed articles. pes2o is derived from S2ORC [104], a cor-pus of openly licensed abstract and full-text papers that have been converted to a structured formatusing Grobid [63]. Starting from Grobid’s XML output, peS2o filters papers that are too short, haveincorrect metadata, are in languages other than English, and contain OCR errors using a combinationof heuristic- and model-based filtering steps. We refer the reader to the datasheet and code for moredetails on this processing pipeline. The subset of peS2o included in the Common Pile starts fromv3 of the corpus, which contains documents from January 1, 1970 to October 6, 2024. We retainfull-text papers with CC BY, CC BY-SA, or CC0 licenses, or that have been labeled as public domain;metadata is provided by the Semantic Scholar APIs [87]. After filtering, this set contains 6.3 million",
      "text": "28\n\npapers, or 35.7 billion whitespace-separated segments. We provide more details on the compositionof this subset in Appendix H.",
      "start_page": 28,
      "end_page": 29
    },
    {
      "section_id": "fe2a9338-11f4-48fa-9234-21d1ee222db3",
      "heading": "PubMedPubMed Central (PMC) is an open-access archive of biomedical and life sciences researchpapers maintained by the U.S. National Institutes of Health’s National Library of Medicine. Wecollected papers from PMC whose metadata indicated that the publishing journal had designated aCC BY, CC BY-SA, or CC0 license. PMC stores the text content of each article as a single XML file,which we convert to markdown using pandoc.",
      "text": "",
      "start_page": 29,
      "end_page": 29
    },
    {
      "section_id": "902c3cb3-dfcc-4c3b-9379-503c59e6baa4",
      "heading": "ArXiv PapersArXiv is an online open-access repository of over 2.4 million scholarly paperscovering fields such as computer science, mathematics, physics, quantitative biology, economics,and more. When uploading papers, authors can choose from a variety of licenses. We includedtext from all papers uploaded under CC BY, CC BY-SA, and CC0 licenses in the Common Pilethrough a three-step pipeline: first, the latex source files for openly licensed papers were downloadedfrom ArXiv’s bulk-access S3 bucket; next, the LATEXML conversion tool was used to convert thesesource files into a single HTML document; finally, the HTML was converted to plaintext using theTrafilatura [10] HTML-processing library.",
      "text": "",
      "start_page": 29,
      "end_page": 29
    },
    {
      "section_id": "d163982b-3bdd-49d9-9a1e-e8644111817d",
      "heading": "ArXiv AbstractsEach paper uploaded to ArXiv includes structured metadata fields, including anabstract summarizing the paper’s findings and contributions. According to ArXiv’s licensing policy,the metadata for any paper submitted to ArXiv is distributed under the CC0 license, regardless ofthe license of the paper itself. Thus, we include as an additional source the abstracts for every papersubmitted to ArXiv. We source the abstracts from ArXiv’s API via the Open Archives InitiativeProtocol for Metadata Harvesting endpoint and reproduce them as-is.",
      "text": "B.2Online Discussions and Forums",
      "start_page": 29,
      "end_page": 29
    },
    {
      "section_id": "fa31e0d4-83c7-43d5-beac-1064d54b4b81",
      "heading": "Online forums are a rich source of multi-turn, user-generated dialogue covering a wide range oftopics. These platforms often feature question–answer pairs, problem-solving discussions, andinformal explanations of technical and non-technical concepts. The Common Pile incorporates onlinediscussions from sources that distribute content under an open license.",
      "text": "",
      "start_page": 29,
      "end_page": 29
    },
    {
      "section_id": "e8bfb323-d402-4ac8-8812-160e02a0b6b6",
      "heading": "StackExchangeWhile StackExchange formerly provided structured XML dumps of all of theircontent, since July of 2024, StackExchange has stopped publishing dumps to the Internet Archive.Instead, each site can provide a logged-in user with a custom URL to download the dump for thatsite. This means that dumps for defunct sites like windowsphone.stackexchange.com are inaccessible.Additionally, in dumps produced by the new export tool, many questions that are available in pastdumps (and accessible on the site) are not present. We therefore extract all questions and answersfrom community uploaded dumps from December of 2024 from the Internet Archive and additionallyextract missing questions and answers from the last official dumps in July of 2024 to account for thedeficiencies listed above. We use a question, its comments, its answers and the comments on eachanswer as a single document. Following the display order on StackExchange, answers are ordered bythe number of votes they received, with the exception that the “accepted answer” always appears first.PyMarkdown was used to convert each comment into plain text.",
      "text": "",
      "start_page": 29,
      "end_page": 29
    },
    {
      "section_id": "b55b6a0c-d8a2-4c83-895f-32ab809c9794",
      "heading": "GitHub ArchiveAccording to GitHub’s terms of service, issues and pull request descriptions—along with their comments—inherit the license of their associated repository. To collect this data,we used the GitHub Archive’s public BigQuery table of events to extracted all issue, pull request,and comment events since 2011 and aggregated them into threads. The table does not include “edit”events so the text from each comment is the original from when it was first posted. We filteredout comments from bots. This resulted in approximately 177 million threads across 19 millionrepositories. We then removed threads whose repositories did not have a Blue Oak Council-approvedlicense. License information for each repository comes from either 1) the “public-data:github_repos”BigQuery Table, 2) metadata from the StackV2, or 3) the GitHub API. License filtering left 10 millionrepositories. PyMarkdown was used to convert from GitHub-flavored markdown to plain text. Whenparsing failed, the raw markdown was kept.",
      "text": "",
      "start_page": 29,
      "end_page": 29
    },
    {
      "section_id": "3bbf9de4-193d-492a-85ef-73fb7a577cf6",
      "heading": "Ubuntu IRCLogs of all discussions on the Ubuntu-hosted Internet Relay Chat (IRC) since 2004have been archived and released into the Public Domain. We downloaded all chats from all channelsup until March of 2025. We consider all messages for given channel on a given day as a singledocument. We removed system messages as well as those from known bots.",
      "text": "29\n\nB.3Government and Legal Texts\n\nGovernments produce a vast amount of informational text, ranging from legislation and legal opinionsto scientific reports, public communications, and regulatory notices. This content is explicitly intendedto inform the public, and as such, in many jurisdictions it is published directly into the public domainor under open licenses. In the United States, for example, works authored by federal employees aspart of their official duties are not subject to copyright. Government and legal texts offer languagemodels exposure to formal argumentation, legal reasoning, and procedural language.",
      "start_page": 29,
      "end_page": 30
    },
    {
      "section_id": "8cb8c35f-f572-4d60-9bcc-b6f5094f1034",
      "heading": "US Government Publishing OfficeThe United States Government Publishing Office (USGPO) isa federal agency responsible for disseminating official documents authored by the U.S. government.The Common Pile v0.1 includes all plain-text documents made available through the USGPO’sGovInfo.gov developer API. This collection comprises over 2.7 million documents, spanning issuesof the Federal Register, congressional hearing transcripts, budget reports, economic indicators, andother federal publications.",
      "text": "",
      "start_page": 30,
      "end_page": 30
    },
    {
      "section_id": "e57b9dff-8bed-48bb-8f7c-6c5b48574543",
      "heading": "US Patents and Trademark OfficeIn the US, patent documents are released into the publicdomain as government works. Patents follow a highly standardized format with distinct requiredsections for background, detailed description, and claims. We include parents from the US Patentsand Trademark Office (USPTO) as provided by the Google Patents Public Data dataset [77], whichincludes millions of granted patents and published patent applications dating back to 1782. Weprocessed these documents to extract clean text while preserving this structured format. Mathematicalexpressions and equations were converted into LATEX.",
      "text": "",
      "start_page": 30,
      "end_page": 30
    },
    {
      "section_id": "d1072ab0-8109-4c7a-95df-154cfa94c510",
      "heading": "Caselaw Access Project and Court ListenerThe Common Pile contains 6.7 million cases fromthe Caselaw Access Project and Court Listener. The Caselaw Access Project consists of nearly 40million pages of U.S. federal and state court decisions and judges’ opinions from the last 365 years.In addition, Court Listener adds over 900 thousand cases scraped from 479 courts. The CaselawAccess Project and Court Listener source legal data from a wide variety of resources such as theHarvard Law Library, the Law Library of Congress, and the Supreme Court Database. From thesesources, we only included documents that were in the public domain. Erroneous OCR errors werefurther corrected after digitization, and additional post-processing was done to fix formatting andparsing.",
      "text": "",
      "start_page": 30,
      "end_page": 30
    },
    {
      "section_id": "709f45d5-5444-4be2-afbb-5090a0aafaf1",
      "heading": "UK HansardHansard represents the official record of parliamentary proceedings across theUnited Kingdom’s legislative bodies. The Common Pile incorporates records from multiple sources,including debates and written answers from the UK Commons and Lords, devolved legislatures(Scottish Parliament, Senedd in both English and Welsh, Northern Ireland Assembly), LondonMayor’s Questions, and ministerial statements. Data was sourced from ParlParse [131], coveringCommons debates from 1918 forward and Lords proceedings from the 1999 reform. Each documentwas processed to preserve complete parliamentary sessions as cohesive units, maintaining the naturalflow of debate. All content is published under the Open Parliament License [181].",
      "text": "",
      "start_page": 30,
      "end_page": 30
    },
    {
      "section_id": "28342a43-ec00-4829-9022-af0db4adf0b5",
      "heading": "Regulations.govRegulations.gov is an online platform operated by the U.S. General ServicesAdministration that collates newly proposed rules and regulations from federal agencies along withcomments and feedback from the general public. The Common Pile includes all plain-text regulatorydocuments published by U.S. federal agencies on this platform, acquired via the bulk downloadinterface provided by Regulations.gov.",
      "text": "B.4Curated Task Data",
      "start_page": 30,
      "end_page": 30
    },
    {
      "section_id": "7f1e604c-0806-4af5-9738-a2fc001fcf31",
      "heading": "Curated datasets that cover specific tasks such as question answering, summarization, or text classifi-cation are often released via open licenses to the research community. While not traditionally partof pretraining corpora, including a small amount of task-oriented data during pretraining can helpmodels acquire early familiarity with task formats and prompt–completion structures.",
      "text": "",
      "start_page": 30,
      "end_page": 30
    },
    {
      "section_id": "0316c0f2-2e03-49bf-bab8-263ecafb1a7a",
      "heading": "Data Provenance InitiativeThe Data Provenance Initiative is a digital library of superviseddatasets that have been manually annotated with their source and license information [106, 109].We leverage their tooling to filter HuggingFace datasets, based on a range of criteria, includingtheir licenses, which may be particularly relevant for supervised datasets [114]. Specifically, wefilter the data according to these criteria: contains English language or code data, the text is notmodel-generated, the dataset’s audit yielded a open license and the original sources of the data areonly from recognized public domain sources.",
      "text": "30\n\nB.5Books in the Public Domain",
      "start_page": 30,
      "end_page": 31
    },
    {
      "section_id": "5e4d0808-0d61-46a5-bd1c-f3bee3f5198b",
      "heading": "Books represent a time-tested resource for language model pretraining, offering carefully edited,long-form prose that supports learning of narrative coherence and long-range dependency modeling.For these reasons, many large-scale pretraining corpora—including the Pile [57], Dolma [167],and RedPajama [185]—include content from books [41]. In the United States, as of 2024, bookspublished prior to 1929 are in the public domain. Thus, the Common Pile includes public domainbooks drawn from curated collections, covering topics such as literature, science, and history.",
      "text": "",
      "start_page": 31,
      "end_page": 31
    },
    {
      "section_id": "37808a73-239d-46c9-98d2-a5078e7a6b8d",
      "heading": "Biodiversity Heritage LibraryThe Biodiversity Heritage Library (BHL) is an open-access digitallibrary for biodiversity literature and archives. The Common Pile contains over 42 million publicdomain books and documents from the BHL collection. These works were collected using the bulkdata download interface provided by the BHL and were filtered based on their associated licensemetadata. We use the optical character recognition (OCR)-generated text distributed by BHL.",
      "text": "",
      "start_page": 31,
      "end_page": 31
    },
    {
      "section_id": "d0151f11-a6e7-432c-b44c-b91234e76d64",
      "heading": "Pre-1929 BooksBooks published in the US before 1929 passed into the public domain on January1, 2024. We used the bibliographic catalog Hathifiles produced by HathiTrust to identify digitizedbooks which were published in the US before 1929. The collection contains over 130,000 booksdigitized and processed by the Internet Archive on behalf of HathiTrust member libraries. The OCRplain text files were downloaded directly from the Internet Archive website.",
      "text": "Library of CongressThe Library of Congress (LoC) curates a collection of public domain bookscalled “Selected Digitized Books”. We downloaded over 130,000 English-language books from thispublic domain collection as OCR plain text files using the LoC APIs.",
      "start_page": 31,
      "end_page": 31
    },
    {
      "section_id": "fe1e24bc-bf3e-4cfa-af95-5457b426d0e4",
      "heading": "Project GutenbergProject Gutenberg is an online collection of over 75,000 digitized booksavailable as plain text. We use all books that are 1) English and 2) marked as in the Public Domainaccording to the provided metadata. Additionally, we include any books that are part of the pg19 [140]dataset, which only includes books that are over 100 years old. Minimal preprocessing is appliedto remove the Project Gutenberg header and footers, and many scanned books include preambleinformation about who digitized them.",
      "text": "B.6Open Educational Resources",
      "start_page": 31,
      "end_page": 31
    },
    {
      "section_id": "877ae4c0-80b5-4a6f-aace-26d7324ca047",
      "heading": "Open Educational Resources (OERs) are educational materials, typically published under openlicenses, to support free and equitable access to education. These resources include educationalartifacts such as textbooks, lecture notes, lesson plans, syllabi, and problem sets. For languagemodels, OERs offer exposure to instructional formatting and domain-specific information, makingthem valuable for improving performance on knowledge-based downstream tasks. The Common Pileincludes a range of such materials sourced from major OER repositories, including collections ofopen-access books and structured teaching resources.",
      "text": "",
      "start_page": 31,
      "end_page": 31
    },
    {
      "section_id": "5d3d299c-0f26-4646-b418-c07b723d97cd",
      "heading": "Directory of Open Access BooksThe Directory of Open Access Books (DOAB) is an onlineindex of over 94,000 peer-reviewed books curated from trusted open-access publishers. To collectthe openly licensed content from DOAB, we retrieve metadata using their official metadata feed.We then filter the collection to include only English-language books released under CC BY and CCBY-SA licenses. The filtered books are downloaded in PDF format and converted to plaintext usingthe Marker PDF-to-text converter. As an additional validation step, we manually create a whitelist ofopen license statements and retain only texts explicitly containing one of these statements in theirfront- or back-matter.",
      "text": "",
      "start_page": 31,
      "end_page": 31
    },
    {
      "section_id": "5c1cff61-132d-405e-8910-8c4453dec57c",
      "heading": "PressBooksPressBooks is a searchable catalog of over 8,000 open access books. To collect openlylicensed content from PressBooks we construct a search query to retrieve URLs for all books writtenin English and listed as public domain or under CC BY or CC BY-SA licenses. For each matchedbook, we collect its contents directly from the publicly available web version provided by PressBooks.",
      "text": "",
      "start_page": 31,
      "end_page": 31
    },
    {
      "section_id": "410d7010-4da8-4026-8a93-7baf0c554b21",
      "heading": "OERCommonsOERCommons is an online platform where educators share open-access instruc-tional materials—such as textbooks, lesson plans, problem sets, course syllabi, and worksheets—withthe goal of expanding access to affordable education. To collect the openly licensed content availableon OERCommons, we construct a search query to retrieve English-language content released into thepublic domain or under CC BY or CC BY-SA licenses. The resulting documents are converted toplain text directly from the HTML pages hosted on the OERCommons website.",
      "text": "31",
      "start_page": 31,
      "end_page": 31
    },
    {
      "section_id": "eeed5a93-4e77-47df-b062-582fddd4c5e6",
      "heading": "LibreTextsLibreTexts is an online platform that provides a catalog of over 3,000 open-accesstextbooks. To collect openly licensed content from LibreTexts we gather links to all textbooks inthe catalog and check each textbook section for a license statement indicating that it is in the publicdomain or under a CC BY, CC BY-SA, or the GNU Free Documentation License. We extract plaintext from these textbook sections directly from the HTML pages hosted on the LibreTexts website.",
      "text": "B.7Wikis",
      "start_page": 32,
      "end_page": 32
    },
    {
      "section_id": "bb7069b7-d013-4dea-b925-433aaa79c17c",
      "heading": "Wikis are collaboratively maintained websites that organize information around specific topics ordomains. Their crowd-sourced nature, coupled with community moderation and citation requirements,often results in text that is both informative and well-structured. Prominent examples such asWikipedia have become staples in large-scale language model pretraining corpora due to their breadthof coverage and high quality. In addition, most major wikis are distributed under open licenses suchas CC BY and CC BY-SA. The Common Pile includes content from a range of openly licensed wikisto provide models with structured and well-researched informational text.",
      "text": "WikimediaWe downloaded the official database dumps from March 2025 of the English-languagewikis that are directly managed by the Wikimedia foundation (see Appendix F for a complete list).These database dumps include the wikitext—Mediawiki’s custom markup language—for each page aswell as talk pages, where editors discuss changes made to a page. We only use the most recent versionof each page. We converted wikitext to plain text using wtf_wikipedia after light adjustments informatting to avoid errors in section ordering caused by a bug. Before parsing, we converted wikitextmath into LATEX math using our custom code. Finally, any remaining HTML tags were removed viaregexes.",
      "start_page": 32,
      "end_page": 32
    },
    {
      "section_id": "1d7a5569-d2b7-431d-8f2e-72ba200235eb",
      "heading": "WikiteamThere are many wikis on the internet that are not managed by the Wikimedia foundation,but do use their MediaWiki software to power their wiki. Many of these wikis have been archivedby wikiteam, a collection of volunteers that create unofficial database dumps of wikis and uploadthem to the Internet Archive. We download all dumps made by wikiteam when the metadata indicatesthe wiki was licensed under CC BY, CC BY-SA, or released into the public domain on the InternetArchive in September of 2024. This results in downloading approximately 330,000 wikis. Whenmultiple dumps of the same wiki exists, we use the most recent dump. The wikitext was convertedto plain text following the same steps as with Wikimedia wikis. After preprocessing, we removeddocuments from wikis that appeared to contain large amounts of license laundering, e.g. those thatwere collections of song lyrics or transcripts.",
      "text": "B.8Source Code",
      "start_page": 32,
      "end_page": 32
    },
    {
      "section_id": "e491408a-f83a-479c-b7d8-8db2d7c063ee",
      "heading": "Source code has become an increasingly important component of large-scale language model pretrain-ing corpora, as it enables models to learn syntax, program structure, and problem solving strategiesuseful for both code generation and reasoning tasks. Thanks to the Free and Open Source Software(FOSS) movement, code also happens to be one of the most openly licensed forms of text, withmany software repositories distributed under open licenses such as MIT, BSD, Apache 2.0, and theGNU Free Documentation License (GFDL). The Common Pile includes high-quality, openly licensedsource code from large-scale public code datasets and documentation standards, enabling modelstrained on it to perform better on coding and technical writing tasks.",
      "text": "",
      "start_page": 32,
      "end_page": 32
    },
    {
      "section_id": "514e61ad-fa36-42de-ba8c-668f9772b7e8",
      "heading": "The Stack V2The Stack V2 [113] consists of a mixture of openly licensed and unlicensed work.We use the tooling that the Software Heritage Foundation and BigCode created to build our dataset.In particular, we relied on the license detection performed by the creators of Stack V2. When multiplelicenses are detected in a single repository, we make sure that all of them meet our definition of“openly licensed”.",
      "text": "",
      "start_page": 32,
      "end_page": 32
    },
    {
      "section_id": "0a42b6d1-9e52-4733-a20b-8fcc0293eb5b",
      "heading": "Python Enhancement ProposalsPython Enhancement Proposals, or PEPs, are design documentsthat generally provide a technical specification and rationale for new features of the Python program-ming language. There are been 661 PEPs published. The majority of PEPs are published in the PublicDomain, but 5 were published under the “Open Publication License” and omitted. PEPs are long,highly-polished, and technical in nature and often include code examples paired with their prose.PEPs are authored in ReStructured Text; we used pandoc, version 3.5, to convert them to plain text.",
      "text": "32\n\nB.9Transcribed Audio Content\n\nA historically underutilized source of text data is speech transcribed from audio and video content.Spoken language in educational videos, speeches, and interviews provide an opportunity for modelsto learn conversational speech patterns.",
      "start_page": 32,
      "end_page": 33
    },
    {
      "section_id": "1324503d-0257-462d-919b-69bcc702dd20",
      "heading": "Creative Commons YouTubeYouTube is large-scale video-sharing platform where users have theoption of uploading content under a CC BY license. To collect high-quality speech-based textualcontent and combat the rampant license laundering on YouTube, we manually curated a set of over2,000 YouTube channels that consistently release original openly licensed content containing speech.The resulting collection spans a wide range of genres, including lectures, tutorials, reviews, videoessays, speeches, and vlogs. From these channels, we retrieved over 1.1 million openly licensedvideos comprising more than 470,000 hours of content. Finally, each video was transcribed to textusing the Whisper speech recognition model [138].",
      "text": "B.10Web Text",
      "start_page": 33,
      "end_page": 33
    },
    {
      "section_id": "b4a61388-c8b7-4405-9786-473863f355f3",
      "heading": "The success of modern LLM pre-training relies on text scraped indiscriminately from the web, asweb text covers an extremely diverse range of textual domains. In the Common Pile, we restrict thisapproach to only include web content with clear public domain status or open license statements.",
      "text": "",
      "start_page": 33,
      "end_page": 33
    },
    {
      "section_id": "613f641c-d1f7-4b54-b846-fb8d755db2f9",
      "heading": "Creative Commons Common CrawlWe sourced text from 52 Common Crawl snapshots, coveringabout half of Common Crawl snapshots available to date and covering all years of operations ofCommon Crawl up to 2024. We found a higher level of duplication across this collection, suggestingthat including more snapshots would lead to a modest increase in total token yield. From thesesnapshots, we extract HTML content using FastWarc [15]. Then, using a regular expression adaptedfrom the C4Corpus project [68], we retain only those pages where a CC BY, CC BY-SA, or CC0license appears. To ensure license accuracy, we manually verified the top 1000 domains by contentvolume, retaining only the 537 domains with confirmed licenses where the Creative Commonsdesignation is applied to all text content rather than only embedded media or a subset of the texton the domain. We extract the main content of these documents and remove boilerplate usingResiliparse [14]. We perform URL-level exact deduplication and use Bloom filters to remove near-duplicates with 80% ngram overlap. We also employ rule-based filters matching Dolma [167];namely, we use C4-derived heuristics [142] to filter pages containing Javascript, Lorem Ipsum, andcurly braces {}. We also apply all Gopher rules [141] to remove low-quality pages. We provide moredetails on the composition of this subset in Appendix G.",
      "text": "",
      "start_page": 33,
      "end_page": 33
    },
    {
      "section_id": "d5e0250b-2e90-44ac-b504-1a52f85e280e",
      "heading": "FoodistaFoodista is a community-maintained site with recipes, food-related news, and nutritioninformation. All content is licensed under CC BY. Plain text is extracted from the HTML using acustom pipeline that includes extracting title and author information to include at the beginning ofthe text. Additionally, comments on the page are appended to the article after we filter automaticallygenerated comments.",
      "text": "",
      "start_page": 33,
      "end_page": 33
    },
    {
      "section_id": "098a8c2c-90d7-4310-8017-6000aed15843",
      "heading": "NewsWe scrape the news sites that publish content under CC BY or CC BY-SA according toopennewswire. A full list of sites can be found in Appendix E. Plain text was extracted fromthe HTML using our custom pipeline, including extraction of the title and byline to include at thebeginning of each article.",
      "text": "Public Domain ReviewThe Public Domain Review is an online journal dedicated to explorationof works of art and literature that have aged into the public domain. We collect all articles publishedin the Public Domain Review under a CC BY-SA license.",
      "start_page": 33,
      "end_page": 33
    },
    {
      "section_id": "8c68c95d-0d2a-4b5e-a5bd-8f78621cc6a4",
      "heading": "CAdditional insights on licensing",
      "text": "",
      "start_page": 33,
      "end_page": 33
    },
    {
      "section_id": "5c6cc460-c3ef-4377-8e20-71099c0de2e5",
      "heading": "There are many standards we could have chosen for what licenses to include in our dataset. The opensource, knowledge, and culture movements have harmonized on the high level principles described insection 1: “open” means that permission is granted for content to be freely used, studied, modified,and shared for any purpose. This language is found in the Open Knowledge Definition we followas well as the Open Source Institute’s Open Definition, Creative Commons’s statement on OpenCulture, Wikimedia’s Acceptable licenses policy and more. Our work was also developed to beconsistent with the Open movement’s work in the specific context of AI technologies such as the",
      "text": "33\n\nOpen Source Initiative’s Open Source AI Definition and in consultations with leading members ofthe community [9].\n\nC.1Why we can’t always trust automatic license detection\n\nThere are many reasons why identifying the licensing status of internet text with automatic toolingcan be challenging. In this section, we briefly discuss some major themes from our experience.",
      "start_page": 33,
      "end_page": 34
    },
    {
      "section_id": "d8d9314b-cad0-452b-9726-abcc25d63c0f",
      "heading": "There are many ways to say the same thing.While there exist standards for how to express alicense, people don’t always follow those standards and failure to follow the standards doesn’t meanthat the license is invalid. For example, simple string matching on “CC BY” misses a huge amount ofCC BY licensed text because a very common way to denote Creative Commons licenses is using animage badge. Current web-processing tools are substantially stronger at identifying text than images,and the failure rate on sites using image badges is quite high.",
      "text": "",
      "start_page": 34,
      "end_page": 34
    },
    {
      "section_id": "0faf7c6c-ebcf-4802-9130-0856d71c5c4b",
      "heading": "Lack of understanding of licenses.Most people are not lawyers and do not understand the fulllegal scope and meaning of the licenses that they attempt to put on their text. Developers routinelytweak boilerplate to produce ambiguous language like (“Licensed under MIT-ish terms”) or writecontradictory statements (“All rights reserved / CC-BY”). In general, it is common for people to writequasi-legal language along side a more traditional license. Non-standard licenses require substantialamounts of work to interpret and are not always valid or meaningful.",
      "text": "Licensing signals can be noisy.Even when a developer intends to clearly communicate a specificlicense, contradictions and errors can occur in practice. For example, Longpre et al. [107] found thatthere were substantial disagreements between the terms of service of a website and the restrictionsfound in a robots.txt file. We have not yet found a reliable way to have an automatic system identifylicensed text and therefore frequently resort to manual review by humans.",
      "start_page": 34,
      "end_page": 34
    },
    {
      "section_id": "54f2eca2-acc9-49a4-9ca1-e8dbdc2b8f8d",
      "heading": "DList of Data Provenance Initiative sources",
      "text": "",
      "start_page": 34,
      "end_page": 34
    },
    {
      "section_id": "0e423305-d090-4ef8-bf92-ee8657b9d286",
      "heading": "The openly licensed supervised datasets included in the Common Pile are listed in Table 1. Thesedatasets were identified and collected using metadata from the Data Provenance Initiative. For moreinformation on these datasets, consult the Data Provenance Initiative Dataset Explorer.",
      "text": "",
      "start_page": 34,
      "end_page": 34
    },
    {
      "section_id": "f6993f35-6084-4842-a6a2-5693221e5d4d",
      "heading": "Table 1: Supervised datasets included in the Common Pile from the Data Provenance Initiativecollection.",
      "text": "CollectionDataset IdentifierLicenses\n\nAgentInstructAgentInstruct-alfworld[164]MIT License\n\nHelpSteerHelpSteer[184]CC BY 4.0\n\nAya Datasetaya-english[165]Apache License 2.0\n\nCommitPackFTcommitpackft-abap[165]MIT License\n\nCommitPackFTcommitpackft-agda[165]MIT License, BSD 3-Clause License\n\nCommitPackFTcommitpackft-apl[165]MIT License, ISC License\n\nCommitPackFTcommitpackft-arc[165]MIT License\n\nCommitPackFTcommitpackft-aspectj[165]Apache License 2.0, BSD 3-ClauseLicense, MIT License\n\nCommitPackFTcommitpackft-ats[165]Apache License 2.0, MIT License\n\nCommitPackFTcommitpackft-blitzmax[165]MIT License\n\nCommitPackFTcommitpackft-bluespec[165]MIT License\n\nCommitPackFTcommitpackft-boo[165]MIT License\n\nContinued on next page\n\n34\n\nCollectionDataset IdentifierLicenses\n\nCommitPackFTcommitpackft-brainfuck[165]Apache License 2.0, BSD 2-ClauseLicense, MIT License\n\nCommitPackFTcommitpackft-bro[165]MIT License, BSD 3-Clause License\n\nCommitPackFTcommitpackft-cartocss[165]MIT License\n\nCommitPackFTcommitpackft-chapel[165]Apache License 2.0, BSD 3-ClauseLicense, MIT License\n\nCommitPackFTcommitpackft-clean[165]Apache License 2.0, MIT License\n\nCommitPackFTcommitpackft-coldfusion[165]Apache License 2.0, MIT License\n\nCommitPackFTcommitpackft-creole[165]Apache License 2.0, MIT License\n\nCommitPackFTcommitpackft-crystal[165]Apache License 2.0, MIT License\n\nCommitPackFTcommitpackft-dns-zone[165]MIT License, BSD 3-Clause License\n\nCommitPackFTcommitpackft-dylan[165]MIT License\n\nCommitPackFTcommitpackft-eiffel[165]MIT License\n\nCommitPackFTcommitpackft-emberscript[165]Apache License 2.0, BSD 3-ClauseLicense, MIT License\n\nCommitPackFTcommitpackft-fancy[165]MIT License, BSD 3-Clause License\n\nCommitPackFTcommitpackft-flux[165]Apache License 2.0, MIT License\n\nCommitPackFTcommitpackft-forth[165]MIT License\n\nCommitPackFTcommitpackft-g-code[165]Apache License 2.0, BSD 3-ClauseLicense, MIT License\n\nCommitPackFTcommitpackft-gdscript[165]Apache License 2.0, CC0 1.0, MITLicense\n\nCommitPackFTcommitpackft-genshi[165]Apache License 2.0, MIT License\n\nCommitPackFTcommitpackft-graphql[165]Apache License 2.0, BSD 3-ClauseLicense, CC0 1.0, MIT License\n\nCommitPackFTcommitpackft-harbour[165]MIT License\n\nCommitPackFTcommitpackft-hlsl[165]Apache License 2.0, MIT License\n\nCommitPackFTcommitpackft-http[165]Apache License 2.0, MIT License\n\nCommitPackFTcommitpackft-idris[165]MIT License, BSD 3-ClauseLicense, BSD 2-Clause License\n\nCommitPackFTcommitpackft-igor-pro[165]MIT License, BSD 3-Clause License\n\nCommitPackFTcommitpackft-inform-7[165]MIT License, BSD 3-Clause License\n\nCommitPackFTcommitpackft-ioke[165]MIT License\n\nCommitPackFTcommitpackft-isabelle[165]MIT License, BSD 2-Clause License\n\nCommitPackFTcommitpackft-jflex[165]MIT License\n\nCommitPackFTcommitpackft-json5[165]MIT License, BSD 3-ClauseLicense, BSD 2-Clause License\n\nCommitPackFTcommitpackft-jsonld[165]Apache License 2.0, BSD 3-ClauseLicense, CC0 1.0, MIT License\n\nCommitPackFTcommitpackft-krl[165]MIT License\n\nCommitPackFTcommitpackft-latte[165]MIT License\n\nCommitPackFTcommitpackft-lean[165]Apache License 2.0, MIT License\n\nCommitPackFTcommitpackft-lfe[165]Apache License 2.0, MIT License\n\nContinued on next page\n\n35\n\nCollectionDataset IdentifierLicenses\n\nCommitPackFTcommitpackft-lilypond[165]MIT License\n\nCommitPackFTcommitpackft-liquid[165]Apache License 2.0, CC0 1.0, MITLicense\n\nCommitPackFTcommitpackft-literate-agda[165]MIT License\n\nCommitPackFTcommitpackft-literate-coffeescript[165]MIT License\n\nCommitPackFTcommitpackft-literate-haskell[165]MIT License, BSD 3-Clause License\n\nCommitPackFTcommitpackft-llvm[165]Apache License 2.0, BSD 3-ClauseLicense, BSD 2-Clause License,MIT License\n\nCommitPackFTcommitpackft-logos[165]Apache License 2.0, BSD 3-ClauseLicense, BSD 2-Clause License,MIT License, ISC License\n\nCommitPackFTcommitpackft-lsl[165]MIT License, BSD 3-Clause License\n\nCommitPackFTcommitpackft-maple[165]MIT License, BSD 3-Clause License\n\nCommitPackFTcommitpackft-mathematica[165]MIT License, CC0 1.0\n\nCommitPackFTcommitpackft-metal[165]Apache License 2.0, MIT License\n\nCommitPackFTcommitpackft-mirah[165]Apache License 2.0, MIT License\n\nCommitPackFTcommitpackft-monkey[165]Apache License 2.0, MIT License\n\nCommitPackFTcommitpackft-moonscript[165]MIT License\n\nCommitPackFTcommitpackft-mtml[165]MIT License\n\nCommitPackFTcommitpackft-mupad[165]Apache License 2.0, BSD 3-ClauseLicense, MIT License\n\nCommitPackFTcommitpackft-nesc[165]MIT License\n\nCommitPackFTcommitpackft-netlinx[165]MIT License\n\nCommitPackFTcommitpackft-ninja[165]Apache License 2.0, BSD 3-ClauseLicense, MIT License\n\nCommitPackFTcommitpackft-nit[165]Apache License 2.0, MIT License\n\nCommitPackFTcommitpackft-nu[165]Apache License 2.0, MIT License\n\nCommitPackFTcommitpackft-ooc[165]MIT License\n\nCommitPackFTcommitpackft-openscad[165]MIT License, CC0 1.0, BSD2-Clause License\n\nCommitPackFTcommitpackft-oz[165]MIT License, BSD 2-Clause License\n\nCommitPackFTcommitpackft-pan[165]Apache License 2.0, MIT License\n\nCommitPackFTcommitpackft-piglatin[165]Apache License 2.0, MIT License\n\nCommitPackFTcommitpackft-pony[165]MIT License, BSD 2-Clause License\n\nCommitPackFTcommitpackft-propeller-spin[165]MIT License\n\nCommitPackFTcommitpackft-pure-data[165]MIT License\n\nCommitPackFTcommitpackft-purebasic[165]MIT License, BSD 3-Clause License\n\nCommitPackFTcommitpackft-purescript[165]Apache License 2.0, BSD 3-ClauseLicense, MIT License\n\nCommitPackFTcommitpackft-ragel-in-ruby-host[165]MIT License\n\nCommitPackFTcommitpackft-rebol[165]Apache License 2.0, MIT License\n\nContinued on next page\n\n36\n\nCollectionDataset IdentifierLicenses\n\nCommitPackFTcommitpackft-red[165]MIT License, BSD 2-Clause License\n\nCommitPackFTcommitpackft-rouge[165]Apache License 2.0, MIT License\n\nCommitPackFTcommitpackft-sage[165]MIT License\n\nCommitPackFTcommitpackft-sas[165]MIT License\n\nCommitPackFTcommitpackft-scaml[165]MIT License, BSD 2-Clause License\n\nCommitPackFTcommitpackft-scilab[165]MIT License, BSD 3-Clause License\n\nCommitPackFTcommitpackft-slash[165]Apache License 2.0, MIT License\n\nCommitPackFTcommitpackft-smt[165]MIT License, BSD 3-Clause License\n\nCommitPackFTcommitpackft-solidity[165]Apache License 2.0, MIT License\n\nCommitPackFTcommitpackft-sourcepawn[165]Apache License 2.0, MIT License\n\nCommitPackFTcommitpackft-squirrel[165]MIT License\n\nCommitPackFTcommitpackft-ston[165]MIT License\n\nCommitPackFTcommitpackft-systemverilog[165]Apache License 2.0, BSD 3-ClauseLicense, MIT License\n\nCommitPackFTcommitpackft-unity3d-asset[165]Apache License 2.0, BSD 3-ClauseLicense, BSD 2-Clause License,MIT License, ISC License, CC0 1.0\n\nCommitPackFTcommitpackft-uno[165]MIT License\n\nCommitPackFTcommitpackft-unrealscript[165]MIT License\n\nCommitPackFTcommitpackft-urweb[165]MIT License, BSD 3-Clause License\n\nCommitPackFTcommitpackft-vcl[165]Apache License 2.0, BSD 3-ClauseLicense, MIT License\n\nCommitPackFTcommitpackft-xbase[165]Apache License 2.0, MIT License\n\nCommitPackFTcommitpackft-xpages[165]Apache License 2.0, MIT License\n\nCommitPackFTcommitpackft-xproc[165]Apache License 2.0, MIT License\n\nCommitPackFTcommitpackft-yacc[165]MIT License, ISC License, BSD2-Clause License\n\nCommitPackFTcommitpackft-zephir[165]MIT License\n\nCommitPackFTcommitpackft-zig[165]MIT License\n\nDolly 15kdolly-brainstorming[165]CC BY-SA 3.0\n\nDolly 15kdolly-classification[165]CC BY-SA 3.0\n\nDolly 15kdolly-closedqa[165]CC BY-SA 3.0\n\nDolly 15kdolly-creative_writing[165]CC BY-SA 3.0\n\nDolly 15kdolly-infoextract[165]CC BY-SA 3.0\n\nDolly 15kdolly-openqa[165]CC BY-SA 3.0\n\nDolly 15kdolly-summarization[165]CC BY-SA 3.0\n\nDialogStudiods-ABCD[165]Apache License 2.0, MIT License\n\nDialogStudiods-ATIS[165]Apache License 2.0, CC BY 4.0\n\nDialogStudiods-ATIS-NER[165]Apache License 2.0, CC BY 4.0\n\nDialogStudiods-AirDialogue[165]Apache License 2.0\n\nDialogStudiods-AntiScam[165]Apache License 2.0, CC0 1.0\n\nDialogStudiods-BANKING77[165]Apache License 2.0, CC BY 4.0\n\nContinued on next page\n\n37\n\nCollectionDataset IdentifierLicenses\n\nDialogStudiods-BANKING77-OOS[165]Apache License 2.0, CC BY 4.0\n\nDialogStudiods-BiTOD[165]Apache License 2.0\n\nDialogStudiods-CLINC-Single-Domain-OOS-banking[165]Apache License 2.0, CC BY 3.0\n\nDialogStudiods-CLINC-Single-Domain-OOS-credit_cards[165]Apache License 2.0, CC BY 3.0\n\nDialogStudiods-CLINC150[165]Apache License 2.0, CC BY-SA 3.0\n\nDialogStudiods-CaSiNo[165]Apache License 2.0, CC BY 4.0\n\nDialogStudiods-CoQA[165]Apache License 2.0, MIT License\n\nDialogStudiods-CoSQL[165]Apache License 2.0, CC BY-SA 4.0\n\nDialogStudiods-ConvAI2[165]Apache License 2.0\n\nDialogStudiods-CraigslistBargains[165]Apache License 2.0, MIT License\n\nDialogStudiods-DART[165]Apache License 2.0, MIT License\n\nDialogStudiods-DSTC8-SGD[165]Apache License 2.0, CC BY-SA 4.0\n\nDialogStudiods-DialogSum[165]Apache License 2.0, MIT License\n\nDialogStudiods-Disambiguation[165]Apache License 2.0, MIT License\n\nDialogStudiods-FeTaQA[165]Apache License 2.0, CC BY-SA 4.0\n\nDialogStudiods-GECOR[165]Apache License 2.0, CC BY 4.0\n\nDialogStudiods-GrailQA[165]Apache License 2.0\n\nDialogStudiods-HDSA-Dialog[165]Apache License 2.0, MIT License\n\nDialogStudiods-HH-RLHF[165]Apache License 2.0, MIT License\n\nDialogStudiods-HWU64[165]Apache License 2.0, CC BY-SA 3.0\n\nDialogStudiods-HybridQA[165]Apache License 2.0, MIT License\n\nDialogStudiods-KETOD[165]Apache License 2.0, MIT License\n\nDialogStudiods-MTOP[165]Apache License 2.0, CC BY-SA 4.0\n\nDialogStudiods-MULTIWOZ2_2[165]Apache License 2.0, MIT License\n\nDialogStudiods-MulDoGO[165]Apache License 2.0, CDLAPermissive 1.0\n\nDialogStudiods-MultiWOZ_2.1[165]Apache License 2.0, MIT License\n\nDialogStudiods-Prosocial[165]Apache License 2.0, MIT License\n\nDialogStudiods-RESTAURANTS8K[165]Apache License 2.0, CC BY 4.0\n\nDialogStudiods-SGD[165]Apache License 2.0, CC BY-SA 4.0\n\nDialogStudiods-SNIPS[165]Apache License 2.0\n\nDialogStudiods-SNIPS-NER[165]Apache License 2.0\n\nDialogStudiods-SParC[165]Apache License 2.0, CC BY-SA 4.0\n\nDialogStudiods-SQA[165]Apache License 2.0, CC BY-SA 4.0\n\nDialogStudiods-STAR[165]Apache License 2.0, MIT License\n\nDialogStudiods-Spider[165]Apache License 2.0, CC BY-SA 4.0\n\nDialogStudiods-TOP[165]Apache License 2.0, CC BY-SA\n\nDialogStudiods-TOP-NER[165]Apache License 2.0, CC BY-SA\n\nDialogStudiods-Taskmaster1[165]Apache License 2.0, CC BY 4.0\n\nContinued on next page\n\n38\n\nCollectionDataset IdentifierLicenses\n\nDialogStudiods-Taskmaster2[165]Apache License 2.0, CC BY 4.0\n\nDialogStudiods-Taskmaster3[165]Apache License 2.0, CC BY 4.0\n\nDialogStudiods-ToTTo[165]Apache License 2.0, CC BY-SA 3.0\n\nDialogStudiods-TweetSumm[165]Apache License 2.0, CC0 1.0\n\nDialogStudiods-WOZ2_0[165]Apache License 2.0\n\nDialogStudiods-WebQSP[165]Apache License 2.0, CC BY 4.0\n\nDialogStudiods-WikiSQL[165]Apache License 2.0, BSD 3-ClauseLicense\n\nDialogStudiods-WikiTQ[165]Apache License 2.0, CC BY-SA 4.0\n\nDialogStudiods-chitchat-dataset[165]Apache License 2.0, MIT License\n\nDialogStudiods-wizard_of_internet[165]Apache License 2.0, CC BY 4.0\n\nDialogStudiods-wizard_of_wikipedia[165]Apache License 2.0, CC BY 4.0\n\nFlan Collection (Chain-of-Thought)fc-cot-cot_gsm8k[34]MIT License\n\nFlan Collection (Chain-of-Thought)fc-cot-cot_strategyqa[59]CC BY-SA 3.0\n\nFlan Collection (Chain-of-Thought)fc-cot-stream_creak[125]MIT License, CC BY-SA 4.0\n\nFlan Collection (Chain-of-Thought)fc-cot-stream_esnli[21]MIT License, CC BY-SA 4.0\n\nFlan Collection (Flan 2021)fc-flan-drop[46]CC BY 4.0\n\nFlan Collection (Flan 2021)fc-flan-e2e_nlg[123]CC BY-SA 4.0\n\nFlan Collection (Flan 2021)fc-flan-natural_questions[89]Apache License 2.0, CC BY-SA 3.0\n\nFlan Collection (Flan 2021)fc-flan-quac[29]CC BY-SA 4.0\n\nFlan Collection (Flan 2021)fc-flan-squad_v1[147]CC BY-SA 4.0\n\nFlan Collection (Flan 2021)fc-flan-squad_v2[147]CC BY-SA 4.0\n\nFlan Collection (Flan 2021)fc-flan-trec[100]CC0 1.0\n\nFlan Collection (Flan 2021)fc-flan-true_case[100]CC0 1.0\n\nFlan Collection (Flan 2021)fc-flan-wiki_lingua_english_en[90]CC BY 3.0\n\nFlan Collection (Flan 2021)fc-flan-winogrande[158]Apache License 2.0, CC BY 4.0\n\nFlan Collection (Flan 2021)fc-flan-wnli[98]CC BY 4.0\n\nFlan Collection (Flan 2021)fc-flan-word_segment[98]CC0 1.0\n\nFlan Collection (Flan 2021)fc-flan-wsc[98]CC BY 4.0\n\nFlan Collection (P3)fc-p3-adversarial_qa[11]CC BY-SA 3.0\n\nFlan Collection (P3)fc-p3-cos_e[144]BSD 3-Clause License\n\nFlan Collection (P3)fc-p3-dbpedia_14[97]CC BY-SA 3.0\n\nFlan Collection (P3)fc-p3-hotpotqa[190]Apache License 2.0, CC BY-SA 4.0\n\nFlan Collection (P3)fc-p3-quarel[153]CC BY 4.0\n\nFlan Collection (P3)fc-p3-quartz[170]CC BY 4.0\n\nFlan Collection (P3)fc-p3-quoref[44]CC BY 4.0\n\nFlan Collection (P3)fc-p3-web_questions[13]CC BY 4.0\n\nFlan Collection (P3)fc-p3-wiki_bio[93]CC BY-SA 3.0\n\nFlan Collection (P3)fc-p3-wiki_hop[93]CC BY-SA 3.0\n\nfc-sni-adversarial_qa[11]CC BY-SA 3.0\n\nContinued on next page\n\n39\n\nCollectionDataset IdentifierLicenses\n\nfc-sni-adverserial_qa[11]MIT License\n\nfc-sni-air_dialogue[187]Apache License 2.0\n\nfc-sni-ancora_ca_ner[187]CC BY 4.0\n\nfc-sni-anem[124]MIT License, CC BY-SA 3.0\n\nfc-sni-argkpApache License 2.0, CC BY-SA 3.0\n\nfc-sni-asian_language_-treebank[151]CC BY 4.0\n\nfc-sni-atomic[75]CC BY 4.0\n\nfc-sni-bard[54]Apache License 2.0\n\nfc-sni-cedr[161]Apache License 2.0\n\nfc-sni-circa[112]CC BY-SA 4.0\n\nfc-sni-clue_cmrc2018[42]CC BY-SA 4.0\n\nfc-sni-coached_conv_pref[139]CC BY 4.0\n\nfc-sni-copa_hrBSD 2-Clause License\n\nfc-sni-crows_pairs[122]CC BY-SA 4.0\n\nfc-sni-cuad[71]CC BY 4.0\n\nfc-sni-defeasible_nli_atomic[155]MIT License\n\nfc-sni-disfl_qa[67]CC BY 4.0\n\nfc-sni-e_snli[21]MIT License\n\nfc-sni-gap[186]Apache License 2.0\n\nfc-sni-hotpotqa[190]Apache License 2.0, CC BY-SA 4.0\n\nfc-sni-human_ratings_of_natural_-language_generation_outputs[190]CC BY 4.0\n\nfc-sni-hybridqa[26]CC BY 4.0, MIT License\n\nfc-sni-iirc[53]CC BY 4.0\n\nfc-sni-jigsaw[53]CC0 1.0\n\nContinued on next page\n\n40\n\nCollectionDataset IdentifierLicenses\n\nfc-sni-librispeech_asr[127]CC BY 4.0\n\nfc-sni-logic2text[27]MIT License\n\nfc-sni-numeric_fused_head[49]MIT License\n\nfc-sni-offenseval_dravidian[49]CC BY 4.0\n\nfc-sni-open_pi[172]CC BY 4.0\n\nfc-sni-paper_reviews_data_set[172]CC BY 4.0\n\nfc-sni-poem_sentiment[163]CC BY 4.0\n\nfc-sni-propara[43]Apache License 2.0\n\nfc-sni-quarel[153]CC BY 4.0\n\nfc-sni-quartz[170]CC BY 4.0\n\nfc-sni-quoref[44]CC BY 4.0\n\nfc-sni-ro_sts_parallel[48]CC BY-SA 4.0\n\nfc-sni-schema_guided_dstc8[148]CC BY-SA 4.0\n\nfc-sni-scitail[86]Apache License 2.0\n\nfc-sni-scitailv1.1[86]Apache License 2.0\n\nfc-sni-semeval_2020_task4[86]CC BY-SA 4.0\n\nfc-sni-sms_spam_collection_v.1[86]CC BY 4.0\n\nfc-sni-splash[86]CC BY-SA 4.0\n\nfc-sni-squad2.0[147]CC BY-SA 4.0\n\nfc-sni-squad_1.1[146]CC BY-SA 4.0\n\nfc-sni-strategyqa[59]MIT License\n\nfc-sni-universal_dependencies___-english_dependency_treebank[59]CC BY-SA 4.0\n\nfc-sni-web_questions[13]CC BY 4.0\n\nfc-sni-wiki_hop[93]CC BY-SA 3.0\n\nContinued on next page\n\n41\n\nCollectionDataset IdentifierLicenses\n\nfc-sni-wikitext[116]CC BY-SA 3.0\n\nfc-sni-winograd_wsc[98]CC BY 4.0\n\nfc-sni-winomt[168]MIT License\n\nfc-sni-winowhy[194]MIT License\n\nfc-sni-wsc; enhanced_wsc[194]CC BY 4.0\n\nfc-sni-wsc_fiexed[194]CC BY-SA 3.0\n\nfc-sni-xcopa[134]CC BY 4.0\n\nfc-sni-xquad[6]CC BY-SA 4.0\n\nOpen Assistantoasst-en[88]Apache License 2.0, CC BY 4.0\n\nOpen Assistant OctoPackoasst-en-octopack[88]Apache License 2.0, CC BY 4.0\n\nOpen Assistant v2oasst2-en[88]Apache License 2.0\n\nOIGoig-unified_canadian_-parliament[88]Apache License 2.0\n\nOIGoig-unified_cuad[88]Apache License 2.0, CC BY 4.0\n\nOIGoig-unified_grade_school_math_-instructions[88]Apache License 2.0, MIT License\n\nOIGoig-unified_nq[88]Apache License 2.0, CC BY-SA 3.0\n\nOIGoig-unified_sqlv1[88]Apache License 2.0, CC BY-SA 4.0\n\nOIGoig-unified_sqlv2[88]Apache License 2.0, CC BY-SA 4.0\n\nOIGoig-unified_squad_v2_more_-neg[88]Apache License 2.0, CC BY-SA 4.0\n\nTasksource Instructtsi-balanced_copa[85]BSD 2-Clause License\n\nTasksource Instructtsi-breaking_nli[60]CC BY-SA 4.0\n\nTasksource Instructtsi-cladder[60]MIT License\n\nTasksource Instructtsi-condaqa[149]Apache License 2.0\n\nTasksource Instructtsi-conj_nli[157]MIT License\n\nTasksource Instructtsi-defeasible_nli-atomic[155]MIT License\n\nTasksource Instructtsi-defeasible_nli-snli[155]MIT License\n\nTasksource Instructtsi-dynasent-dynabench.dynasent.r1.all-r1[135]CC BY 4.0\n\nTasksource Instructtsi-dynasent-dynabench.dynasent.r2.all-r2[135]CC BY 4.0\n\nTasksource Instructtsi-fever_evidence_related-mwong_-_fever_related[177]CC BY-SA 4.0\n\nTasksource Instructtsi-few_nerd-supervised[45]CC BY-SA 4.0\n\nTasksource Instructtsi-fig_qa[102]MIT License\n\nTasksource Instructtsi-fracas[56]MIT License\n\nContinued on next page\n\n42\n\nCollectionDataset IdentifierLicenses\n\nTasksource Instructtsi-hyperpartisan_news[56]CC BY 4.0\n\nTasksource Instructtsi-lex_glue-case_hold[23]Apache License 2.0\n\nTasksource Instructtsi-lonli[174]MIT License\n\nTasksource Instructtsi-moral_stories-full[51]MIT License\n\nTasksource Instructtsi-neqa[51]CC BY 4.0\n\nTasksource Instructtsi-prostApache License 2.0\n\nTasksource Instructtsi-quote_repetitionCC BY 4.0\n\nTasksource Instructtsi-recast-recast_factualityCC BY-SA 4.0\n\nTasksource Instructtsi-recast-recast_megaveridicalityCC BY-SA 4.0\n\nTasksource Instructtsi-recast-recast_nerCC BY-SA 4.0\n\nTasksource Instructtsi-recast-recast_punsCC BY-SA 4.0\n\nTasksource Instructtsi-recast-recast_sentimentCC BY-SA 4.0\n\nTasksource Instructtsi-recast-recast_verbcornerCC BY-SA 4.0\n\nTasksource Instructtsi-recast-recast_verbnetCC BY-SA 4.0\n\nTasksource Instructtsi-redefine_mathCC BY 4.0\n\nTasksource Instructtsi-tracie[196]Apache License 2.0\n\nTasksource Instructtsi-truthful_qa-multiple_-choice[101]Apache License 2.0\n\nTasksource Instructtsi-vitaminc-tals__vitaminc[162]MIT License\n\nTasksource Instructtsi-winowhy[194]MIT License\n\nTasksource Symbol-Tuningtsy-breaking_nli[60]CC BY-SA 4.0\n\nTasksource Symbol-Tuningtsy-cladder[60]MIT License\n\nTasksource Symbol-Tuningtsy-condaqa[149]Apache License 2.0\n\nTasksource Symbol-Tuningtsy-conj_nli[157]MIT License\n\nTasksource Symbol-Tuningtsy-defeasible_nli-atomic[155]MIT License\n\nTasksource Symbol-Tuningtsy-defeasible_nli-snli[155]MIT License\n\nTasksource Symbol-Tuningtsy-dynasent-dynabench.dynasent.r1.all-r1[135]CC BY 4.0\n\nTasksource Symbol-Tuningtsy-dynasent-dynabench.dynasent.r2.all-r2[135]CC BY 4.0\n\nTasksource Symbol-Tuningtsy-fever_evidence_related-mwong__fever_related[177]CC BY-SA 4.0\n\nTasksource Symbol-Tuningtsy-fracas[56]MIT License\n\nTasksource Symbol-Tuningtsy-hyperpartisan_news[56]CC BY 4.0\n\nTasksource Symbol-Tuningtsy-lonli[174]MIT License\n\nTasksource Symbol-Tuningtsy-recast-recast_factuality[174]CC BY-SA 4.0\n\nTasksource Symbol-Tuningtsy-recast-recast_-megaveridicality[174]CC BY-SA 4.0\n\nTasksource Symbol-Tuningtsy-recast-recast_ner[174]CC BY-SA 4.0\n\nTasksource Symbol-Tuningtsy-recast-recast_puns[174]CC BY-SA 4.0\n\nTasksource Symbol-Tuningtsy-recast-recast_sentiment[174]CC BY-SA 4.0\n\nTasksource Symbol-Tuningtsy-recast-recast_verbcorner[174]CC BY-SA 4.0\n\nContinued on next page\n\n43\n\nCollectionDataset IdentifierLicenses\n\nTasksource Symbol-Tuningtsy-recast-recast_verbnet[174]CC BY-SA 4.0\n\nTasksource Symbol-Tuningtsy-tracie[196]Apache License 2.0\n\nTasksource Symbol-Tuningtsy-vitaminc-tals__vitaminc[162]MIT License\n\nTasksource Symbol-Tuningtsy-winowhy[194]MIT License",
      "start_page": 34,
      "end_page": 44
    },
    {
      "section_id": "d63cc466-8748-4313-bf43-f1346837ef2e",
      "heading": "EList of News sources",
      "text": "",
      "start_page": 44,
      "end_page": 44
    },
    {
      "section_id": "915bbdf0-3720-47d2-90a4-ace517f79b43",
      "heading": "The Common Pile contains a variety of openly licensed news sources released under CC BY andCC BY-SA licenses. The sources licensed under CC BY include: 360info, Africa is a Country, AltNews, Balkan Diskurs, Factly, Freedom of the Press Foundation, Agenzia Fides, Global Voices,Meduza, Mekong Eye, Milwaukee Neighborhood News Service, Minority Africa, New CanadianMedia, SciDev.Net, The Solutions Journalism Exchange, Tasnim News Agency, and ZimFact. Thesources licensed under CC BY-SA include: Oxpeckers, Propastop, and The Public Record.",
      "text": "",
      "start_page": 44,
      "end_page": 44
    },
    {
      "section_id": "7798f4a0-5f63-40bc-bcf4-b777dcd6c9e0",
      "heading": "FList of WikiMedia wikis",
      "text": "",
      "start_page": 44,
      "end_page": 44
    },
    {
      "section_id": "97132dfa-9e49-4968-9d78-50854b16167b",
      "heading": "Official Wikimedia wikis are released under a CC BY-SA license. The Common Pile includes thefollowing Wikimedia wikis: Wikipedia, Wikinews, Wikibooks, Wikiquote, Wikisource, Wikiversity,Wikivoyage, and Wiktionary.",
      "text": "",
      "start_page": 44,
      "end_page": 44
    },
    {
      "section_id": "82d0cdd0-df72-4c20-8305-5a524c0f34f3",
      "heading": "GCCCC Source Statistics",
      "text": "We provide additional statistics on the CCCC subset of the Common Pile, including the number ofunicode words and documents sourced from each Common Crawl snapshot, in Table 2.\n\nTable 2: Counts of words and documents extracted from 52 snapshots after filtering with our pipeline.\n\nSnapshotUnicode WordsDocuments\n\nCC-MAIN-2013-203,851,018,1975,529,294\n\nCC-MAIN-2013-484,544,197,2526,997,831\n\nCC-MAIN-2014-104,429,217,9416,682,672\n\nCC-MAIN-2014-154,059,132,8735,912,779\n\nCC-MAIN-2014-235,193,195,7658,253,690\n\nCC-MAIN-2014-354,254,690,9456,551,673\n\nCC-MAIN-2014-414,289,814,4496,558,170\n\nCC-MAIN-2014-423,986,284,7416,144,797\n\nCC-MAIN-2014-493,316,075,4524,699,472\n\nCC-MAIN-2014-524,307,765,2896,338,983\n\nCC-MAIN-2015-063,675,982,6795,181,955\n\nCC-MAIN-2015-113,932,442,9005,438,533\n\nCC-MAIN-2015-143,658,107,7654,954,273\n\nContinued on next page\n\n44\n\nSnapshotUnicode WordsDocuments\n\nCC-MAIN-2015-184,451,734,9466,319,757\n\nCC-MAIN-2015-224,285,945,3195,949,267\n\nCC-MAIN-2015-273,639,904,1284,975,152\n\nCC-MAIN-2016-071,588,496,7033,798,207\n\nCC-MAIN-2016-183,228,754,2004,446,815\n\nCC-MAIN-2016-223,217,827,6764,242,762\n\nCC-MAIN-2017-043,852,699,2135,239,605\n\nCC-MAIN-2017-094,186,915,4985,119,171\n\nCC-MAIN-2017-134,950,110,9315,923,670\n\nCC-MAIN-2017-174,684,050,8305,645,725\n\nCC-MAIN-2017-224,683,569,2785,514,717\n\nCC-MAIN-2017-264,744,689,1375,514,047\n\nCC-MAIN-2017-511,981,004,3062,529,289\n\nCC-MAIN-2018-134,816,417,9305,520,099\n\nCC-MAIN-2018-223,921,533,2514,401,956\n\nCC-MAIN-2018-264,506,583,9314,916,546\n\nCC-MAIN-2018-304,936,722,4035,282,886\n\nCC-MAIN-2018-343,865,953,9783,808,725\n\nCC-MAIN-2018-473,933,439,8413,637,947\n\nCC-MAIN-2018-514,745,124,4224,616,832\n\nCC-MAIN-2019-044,475,679,1904,140,277\n\nCC-MAIN-2019-094,287,868,8004,142,190\n\nCC-MAIN-2019-133,966,330,3483,849,631\n\nCC-MAIN-2019-304,179,526,1884,430,572\n\nCC-MAIN-2019-355,144,426,2705,048,106\n\nCC-MAIN-2019-394,572,972,4574,527,430\n\nCC-MAIN-2020-295,200,565,5014,984,248\n\nCC-MAIN-2020-344,458,827,9474,297,009\n\nCC-MAIN-2021-171,768,757,3861,824,942\n\nCC-MAIN-2021-394,599,961,6754,287,356\n\nCC-MAIN-2021-435,337,349,3315,304,846\n\nCC-MAIN-2021-493,980,018,7734,050,641\n\nCC-MAIN-2022-054,517,850,0194,503,863\n\nCC-MAIN-2023-065,135,614,2274,959,915\n\nCC-MAIN-2023-145,117,143,7654,675,097\n\nCC-MAIN-2023-235,461,486,8074,869,627\n\nCC-MAIN-2023-505,881,860,0144,901,306\n\nCC-MAIN-2024-105,164,171,5624,335,071\n\nCC-MAIN-2024-184,745,457,0543,949,186\n\nTotal221,715,271,483259,728,610\n\n45",
      "start_page": 44,
      "end_page": 45
    },
    {
      "section_id": "eb415513-3a3f-4313-9faa-b77d2b61fd7a",
      "heading": "HPeS2o Source Statistics",
      "text": "",
      "start_page": 46,
      "end_page": 46
    },
    {
      "section_id": "399df9fd-64f7-4098-8251-64d0f20eca5e",
      "heading": "Additional statistics on the composition of the peS2o subset of the Common Pile can be found inTable 3 and Table 4.",
      "text": "Table 3: Distribution of licenses in the peS2o subset.\n\nLicenseTrain SplitValidation Split\n\nCC BY6,088,32537,754CC BY-SA120,1501,231CC036,373121Public domain10,0606",
      "start_page": 46,
      "end_page": 46
    },
    {
      "section_id": "df6af844-e68f-444c-9b32-692d365845ea",
      "heading": "Table 4: Distribution of papers across 23 fields of study, as identified by the Semantic ScholarAPI [87]. A paper may belong to one or more fields of study.",
      "text": "Field of StudyTrain SplitValidation Split\n\nMedicine2,435,24423,734\n\nBiology1,518,4788,879\n\nEnvironmentalScience993,4997,601\n\nEngineering656,0215,005\n\nComputer Science462,3203,003\n\nMaterials Science416,0453,166\n\nPhysics413,4611,285\n\nChemistry406,4292,781\n\nPsychology364,4412,126\n\nEducation220,0141,532\n\nBusiness193,536946\n\nEconomics185,716921\n\nAgricultural and FoodSciences333,7762,013\n\nSociology137,2571,535\n\nMathematics135,676199\n\nPolitical Science106,748378\n\nGeology67,258217\n\nGeography44,269257\n\nLinguistics41,737228\n\nHistory36,848192\n\nLaw30,888251\n\nPhilosophy27,518148\n\nArt26,65875\n\n46",
      "start_page": 46,
      "end_page": 46
    },
    {
      "section_id": "434678ad-87db-4c99-bcd1-274afc4f31df",
      "heading": "IGrowth rates of openly licensed data",
      "text": "",
      "start_page": 47,
      "end_page": 47
    },
    {
      "section_id": "f4e125ef-cae0-432d-b582-bd94fff8d795",
      "heading": "Over time, the volume of openly licensed data continues to grow as more creators release contentunder open licenses. In Figure 6, we quantify this growth between 2010 and 2024 by analyzingsubsets of the Common Pile for which reliable creation date metadata is available. We plot thecumulative proportion of data created up to various cutoff dates and find that approximately halfof the Common Pile (around 3.8TB) was created since 2020. This trend provides insight into thegrowing availability of openly licensed data and suggests a promising trajectory for future LLMstrained entirely on openly licensed sources.",
      "text": "20102012201420162018202020222024Cutoff Date\n\n0\n\n20\n\n40\n\n60\n\n80\n\n100\n\nPercentage of Total Size\n\nQuantity of Openly Licensed Data Over Time\n\nAllCodeEducational Resources\n\nOtherOnline ForumsWeb\n\nWikisGovernmentAcademic Papers",
      "start_page": 47,
      "end_page": 47
    },
    {
      "section_id": "a590d0ee-6528-4d94-8dd3-0b70495785cf",
      "heading": "Figure 6: The amount of openly licensed text grows steadily over time. We visualize the cumulativeproportion of data created up to various cutoff dates for sources in the Common Pile with reliablecreation date metadata. This includes all sources except for the Caselaw Access Project, DataProvenance Initiative, and the sources covering early 20th century Public Domain books.",
      "text": "",
      "start_page": 47,
      "end_page": 47
    },
    {
      "section_id": "bd45d50e-eb0d-4fbf-bd5b-0b5e942a9be3",
      "heading": "JDetails on filtering pipelines",
      "text": "",
      "start_page": 47,
      "end_page": 47
    },
    {
      "section_id": "cfaac78a-0109-4b0d-831e-60cc4b43f46e",
      "heading": "In subsection 4.1, we detail the steps used to produce the Comma v0.1 training dataset from theraw text in the Common Pile. These include applying filters based on language, text quality, length,likelihood, and toxicity; removing various forms of PII; and removal of source-specific boilerplatetext using regular expressions. The Common Pile contains a diverse range of sources and we thereforedesign separate filtering thresholds for each source. The exact source-specific thresholds used topost-process the Common Pile can be found in Table 5. Additionally, statistics on the pre- andpost-filtered sizes of each source can be found in Table 6.",
      "text": "Table 5: Pre-processing pipelines applied to each source in the Common Pile to construct the Commadataset.\n\nSourceLanguage Text QualityDoc LengthLog-LikelihoodToxicityPIIRegex Filter\n\nArXiv Abstracts–––––YN\n\nArXiv Papers> 0.5––––YN\n\nBiodiversityHeritage Library> 0.5–> 100> -20–NY\n\nCaselaw AccessProject––> 100–> 0.1YN\n\nCC CommonCrawl> 0.5> 0.0001> 100–> 0.1YN\n\nContinued on next page\n\n47\n\nSourceLanguage Text QualityDoc LengthLog-LikelihoodToxicityPIIRegex Filter\n\nData ProvenanceInitiative–––––NN\n\nDatabase ofOpen AccessBooks\n\n> 0.5–> 200–> 0.1YN\n\nFoodista> 0.5–> 100––NN\n\nGitHub Archive> 0.5–> 100–> 0.1YN\n\nLibrary ofCongress–––> -20> 0.1NY\n\nLibreTexts> 0.5–> 700–> 0.1YN\n\nNews> 0.5–> 100––YN\n\nOERCommons> 0.5–> 300–> 0.1YN\n\npeS2o–––––YN\n\nPre-1929 Books–––> -20> 0.1NY\n\nPressBooks> 0.5–> 600–> 0.1YN\n\nProjectGutenberg> 0.5––> -20–NN\n\nPublic DomainReview––> 100––YN\n\nPubMed> 0.5–> 100––YN\n\nPEPs–––––YN\n\nRegulations.gov––> 100––YY\n\nStackExchange> 0.5––––YN\n\nUbuntu IRC> 0.5–> 100–> 0.1YN\n\nUK Hansard> 0.5––––YN\n\nUSGPO–––––NY\n\nUSPTO––> 100> -20–YN\n\nWikimedia> 0.5–> 100––YN\n\nWikiteam> 0.5–> 700–> 0.1YN\n\nCC YouTube> 0.5–> 100–> 0.1YN\n\nTable 6: Raw and filtered sizes of the Common Pile’s constituent datasets.\n\nDocument CountSize (GB)\n\nSourceRawFilteredRawFiltered\n\nArXiv Abstracts2,538,9352,504,6792.42.4\n\nArXiv Papers321,336304,0482119\n\nBiodiversity HeritageLibrary42,418,49815,111,3139635\n\nCaselaw Access Project6,919,2406,735,5257877\n\nContinued on next page\n\n48\n\nDocument CountSize (GB)\n\nSourceRawFilteredRawFiltered\n\nCC Common Crawl51,054,4126,852,13726058\n\nData Provenance Initiative9,688,2113,508,51873\n\nDirectory of Open AccessBooks474,445403,99212.512\n\nFoodista72,09065,6400.090.08\n\nGitHub Archive30,318,77423,358,58054.740.4\n\nLibrary of Congress135,500129,05247.835.6\n\nLibreTexts62,26940,0495.33.6\n\nNews172,308126,6730.40.3\n\nOERCommons9,3395,2490.10.05\n\npeS2o6,294,0206,117,280188.2182.6\n\nPre-1929 Books137,127124,89873.846.3\n\nPressBooks106,88154,4551.50.6\n\nProject Gutenberg71,81055,45426.220.1\n\nPublic Domain Review1,4121,4060.0070.007\n\nPubMed4,068,8673,829,689158.9147.1\n\nPEPs6566550.010.01\n\nRegulations.gov225,196208,3016.15.1\n\nStackExchange33,415,40030,987,814103.789.7\n\nStack V2218,364,13369,588,6074774.7259.9\n\nUbuntu IRC329,115234,9826.35.3\n\nUK Hansard51,55247,909109.6\n\nUSGPO2,732,6772,148,54874.536.1\n\nUSPTO20,294,15217,030,2311003.4661.1\n\nWikimedia63,969,93816,311,57490.557.4\n\nWikiteam219,139,36826,931,807437.513.7\n\nCC YouTube1,129,692998,10421.518.6\n\nTotal692,854,953233,817,1697557.91838.3",
      "start_page": 47,
      "end_page": 49
    },
    {
      "section_id": "56b87158-6377-43de-94a5-68720f84cd82",
      "heading": "KDetails on Comma’s pre-training data mixture",
      "text": "We estimated the quality of each source in the Common Pile by training a 1.7B-parameter model for28B tokens on each source individually and evaluating the resulting models on the set of “early signal”tasks from [132]. In doing so, we found that the amount of text in each source was poorly correlatedwith text quality, motivating the use of heuristic mixing weights to up-/down-weight different sourcesin our pre-training mix. In Table 7 we list the pre-training mixture weights for each of the sources inthe Common Pile.\n\n49",
      "start_page": 49,
      "end_page": 49
    },
    {
      "section_id": "9d736c2c-bede-40e7-acd3-17790774a503",
      "heading": "Table 7: Overview of the data mixing used to up/down-weight individual sources in the CommonPile to construct the Comma v0.1-1T pre-training dataset. Comma v0.1-2T simply repeats this fullmixture twice.",
      "text": "SourceSize (GB)RepeatsEffective Size(GB)Tokens(Billions)Percentage\n\nArXivAbstracts2.4614.43.60.360%\n\nArXiv Papers19.5611729.32.932%\n\nBiodiversityHeritageLibrary\n\n35.50.258.92.20.220%\n\nCaselawAccess Project77.5177.519.41.941%\n\nCC CommonCrawl58.16348.687.18.716%\n\nDataProvenanceInitiative\n\n3.4620.45.10.510%\n\nDatabase ofOpen AccessBooks\n\n12672181.801%\n\nFoodista0.0860.480.120.012%\n\nGitHubArchive40.46242.460.66.064%\n\nLibrary ofCongress35.60.258.92.20.220%\n\nLibreTexts3.6621.65.40.540%\n\nNews0.2561.50.380.038%\n\nOERCommons0.0560.30.080.008%\n\npeS2o182.661,095.6273.927.409%\n\nPre-1929Books46.3146.311.61.161%\n\nPressBooks0.663.60.90.090%\n\nProjectGutenberg20.1120.150.500%\n\nPublic DomainReview0.00760.040.010.001%\n\nPubMed147.11147.136.83.683%\n\nPEPs0.0160.060.020.002%\n\nRegulations.gov5.1630.67.60.761%\n\nStackExchange89.76538.2134.613.469%\n\nStack V2259.92519.813013.009%\n\nContinued on next page\n\n50\n\nSourceSize (GB)RepeatsEffective Size(GB)Tokens(Billions)Percentage\n\nUbuntu IRC5.3631.87.90.791%\n\nUK Hansard9.6657.614.41.441%\n\nUSGPO36.10.2592.30.230%\n\nUSPTO661.10.25165.341.34.133%\n\nWikimedia57.46344.486.18.616%\n\nWikiteam13.7454.813.71.371%\n\nCC YouTube18.6118.64.70.470%\n\nTotal1838.3–3997.4999.3100%",
      "start_page": 50,
      "end_page": 51
    },
    {
      "section_id": "17009b97-325c-4595-a1f4-e85ae61c7f16",
      "heading": "LDetails on Comma’s cool-down data mixture",
      "text": "",
      "start_page": 51,
      "end_page": 51
    },
    {
      "section_id": "982582ee-6aa8-4116-9cc7-b63cd290d52b",
      "heading": "Following Hu et al. [73], we end training with a “cool-down” where we train on 37.7B tokens ofhigh-quality data while linearly decaying the learning rate to 0. We provide the source mixtureweights for this cool-down phase in Table 8.",
      "text": "",
      "start_page": 51,
      "end_page": 51
    },
    {
      "section_id": "fbcffa71-f142-42be-a8e5-20e92097a411",
      "heading": "Table 8: Overview of the data mixing used to up/down-weight individual sources in the CommonPile to construct the training distribution for Comma v0.1-1T’s cool-down phase. Comma v0.1-2Tsimply repeats this full mixture twice.",
      "text": "SourceSize (GB)RepeatsEffective Size(GB)Tokens(Billions)Percentage\n\nArXiv Papers19.50.59.82.46.50%\n\nCC CommonCrawl58.10.317.44.411.63%\n\nDataProvenanceInitiative\n\n3.426.81.74.55%\n\nDatabase ofOpen AccessBooks\n\n12224616.04%\n\nFoodista0.0820.160.040.11%\n\nLibreTexts3.627.21.80.48%\n\nNews0.2520.50.130.33%\n\nOERCommons0.0520.10.030.07%\n\npeS2o182.60.118.34.612.18%\n\nPressBooks0.621.20.30.77%\n\nPublic DomainReview0.00720.0140.0040.01%\n\nContinued on next page\n\n51\n\nSourceSize (GB)RepeatsEffective Size(GB)Tokens(Billions)Percentage\n\nPEPs0.0120.020.0050.02%\n\nStackExchange89.70.2522.45.614.96%\n\nStack V2259.90.126.06.517.04%\n\nWikimedia57.40.4235.715.32%\n\nTotal679.4–149.937.5100%",
      "start_page": 51,
      "end_page": 52
    },
    {
      "section_id": "bea34a1d-290b-4a6c-8585-6636cbbbc63e",
      "heading": "MDetails on small-scale data ablations",
      "text": "",
      "start_page": 52,
      "end_page": 52
    },
    {
      "section_id": "6a8cec60-7b1b-4a01-99db-a49aa9da320d",
      "heading": "In subsection 4.3 we report results from a series of small-scale data ablations where we identicallytrained 1.7B parameter models on various openly licensed and unlicensed datasets and evaluate theirperformance on the “early signal” tasks from Penedo et al. [132] to compare their data quality againstthe Common Pile. In Figure 7 we show how the performance of these models evolve over the courseof their training run, highlighting that differences in data quality become apparent very early intraining. Additionally, we provide exact numerical results for each model in Table 9, showing thatthe Common Pile has higher data quality than any previously released openly licensed datasets andthe Pile, and nearly matches the data quality of the OSCAR dataset. To validate that this is not purelydue to the presence of high-quality supervised fine-tuning data from the Data Provenance Initiative(DPI) data source, we also perform an ablation on the Common Pile excluding the DPI data and findthat the final performance of this model is largely unchanged.",
      "text": "",
      "start_page": 52,
      "end_page": 52
    },
    {
      "section_id": "ac630072-16c6-4172-99bd-74eaee5eb5f1",
      "heading": "Figure 7: A model trained on the Comma dataset consistently outperforms models trained onother corpora of openly licensed text and outperforms the Pile on all but two tasks. We trainidentical 1.7B parameter models on 28B tokens from each dataset following Penedo et al. [132].",
      "text": "",
      "start_page": 52,
      "end_page": 52
    },
    {
      "section_id": "010fc548-8d74-4223-bbcf-17e332e71cbd",
      "heading": "NAdditional Comma results",
      "text": "",
      "start_page": 52,
      "end_page": 52
    },
    {
      "section_id": "6c545904-2019-41b7-a9d4-f64f4ebab845",
      "heading": "We provide exact numerical results for Comma v0.1-1T and -2T alongside baseline models resultsacross a variety of knowledge, reasoning, and coding tasks in Table 10 and Table 11 respectively. Wefind that particularly on knowledge-based benchmarks (such as MMLU) and coding benchmarks,",
      "text": "52",
      "start_page": 52,
      "end_page": 52
    },
    {
      "section_id": "7120a145-a5af-45d9-ba81-6f7ab315cd96",
      "heading": "Table 9: Comma’s training dataset has higher quality than previous openly-licensed datasetsand unlicensed datasets like the Pile. In the small-scale (1.7B parameter) data ablation setting,we find that Comma’s training dataset yields better models than previous openly licensed datasetsand the Pile, and nearly matches the performance of models trained on OSCAR. Additionally, wefind that removing the high-quality supervised data from the Data Provenance Initiative has marginalaffect on the Comma dataset’s overall quality.",
      "text": "DatasetARCMMLUHSOBQACSQAPIQASIQAAvg.\n\nKL3M31.826.329.928.426.858.238.036.2OLC33.127.533.827.427.759.438.537.3Common Corpus34.227.033.630.226.461.037.737.6Comma (no DPI)37.728.737.631.030.863.839.840.0Comma38.029.539.932.429.665.839.440.8\n\nThe Pile37.027.835.828.631.566.838.239.6OSCAR35.427.640.830.432.169.739.740.9FineWeb38.029.148.234.233.673.440.343.7",
      "start_page": 53,
      "end_page": 53
    },
    {
      "section_id": "704a175d-1eda-4d1f-93ee-8af08d3fac28",
      "heading": "Comma v0.1-1T and -2T outperform baseline models trained on an equivalent amount (1T or 2Ttokens, respectively) of unlicensed text.",
      "text": "Table 10: Comparison between Comma v0.1-1T and baseline models trained with similar resources (7billion parameters, 1 trillion tokens) across a variety of knowledge, reasoning, and coding benchmarks.\n\nModelARC-CARC-EMMLUBoolQHSOBQACSQAPIQASIQAHEvalMBPPAvg.\n\nRPJ-INCITE42.868.427.868.670.349.457.776.046.911.115.948.6LLaMA44.567.934.875.476.251.261.877.250.319.927.953.4StableLM50.865.445.271.775.648.257.277.048.223.132.054.0MPT46.570.530.274.277.648.663.377.349.127.333.254.3OpenLLaMA44.567.240.372.672.650.862.878.049.727.633.954.5Comma v0.1-1T52.868.442.475.762.647.059.470.850.836.535.554.7\n\nQwen357.274.577.086.177.050.866.478.255.094.567.571.3\n\nTable 11: Performance of Comma v0.1-2T and a variety of budget-matched baseline models.\n\nModelARC-CARC-EMMLUBoolQHSOBQACSQAPIQASIQAHEvalMBPPAvg.\n\nOLMo Twin45.267.528.271.773.448.061.877.948.518.227.551.6Llama 248.569.545.880.276.248.462.876.750.826.128.555.8Comma v0.1 2T45.871.849.878.664.446.264.072.552.344.241.557.4DeepSeekLLM49.567.748.571.774.152.066.677.851.643.143.858.8",
      "start_page": 53,
      "end_page": 53
    },
    {
      "section_id": "7e014fb4-c5df-4d27-ba91-2ba306c778cc",
      "heading": "OAdditional training runs",
      "text": "",
      "start_page": 53,
      "end_page": 53
    },
    {
      "section_id": "24462f09-7b37-4ef5-a6f6-dbd27c27d499",
      "heading": "To explore the sensitivity of our Comma v0.1 results to hyperparameter choices, we perform a seriesof additional 7B parameter/1T token training runs on AMD MI300A GPUs with slight alterationsto the training recipe. Due to both a desire to reach the same 1T token target rapidly, and the lowersingle-GPU throughput on the system available for these ablations, for all additional runs the thetraining batch size is 8.3M (223) versus the 2.1M (221) tokens per step of Comma v0.1. Unlessotherwise specified, we did not use the two phase training process described in subsection 4.4 (i.e. noseparate high-quality cooldown phase is run and we do not perform checkpoint averaging at the endof training and before evaluation).",
      "text": "O.1Ablations at 1T Tokens\n\nWe first performed a set of training runs for 125,000 steps, resulting in 1.048T total tokens (referredto as “1T” for brevity).\n\n53",
      "start_page": 53,
      "end_page": 53
    },
    {
      "section_id": "e4f01b54-f451-41b3-9dfb-c546880194ea",
      "heading": "“8M Batch”We perform a run with nearly the same training hyperparameters as Comma v0.1-1T,except with a larger 8M token batch size. We also use a single phase training setup; the base datamixture (Table 7) is run for the entire duration to 1T tokens. The learning rate schedule is 2,000 stepsof warm-up from 0 to a peak of 1e −3 with 123,000 steps of decay to a minimum of 1.8e −9.",
      "text": "",
      "start_page": 54,
      "end_page": 54
    },
    {
      "section_id": "6c359db4-87ce-4f52-84ab-0d64064c5224",
      "heading": "“Curriculum”In this experimental run, a different data mixture is used in each of three trainingstages of equal duration (we also use the modified hyperparameters from “8M Batch” ablationabove). The first stage of the curriculum comprises data from only the Common Pile’s largest sources(mostly USPTO, Table 13). The second stage uses the same data mixture as Comma v0.1’s mainpre-training phase (“phase I”), but run for only 1/3 of the duration. Finally, the third and last stage ofthe curriculum up-weights Common Pile’s highest quality, benchmark-relevant sources (Table 14).",
      "text": "",
      "start_page": 54,
      "end_page": 54
    },
    {
      "section_id": "9bdb1001-0b8c-4a9b-9970-d14485d8da44",
      "heading": "We provide exact numerical results for Comma v0.1 and alternate Comma runs performed withdifferent hyperparameters and data mixture curricula across a variety of knowledge, reasoning, andcoding benchmarks in Table 12. We find that the 8M Batch and Curriculum ablations are roughlycomparable on average to the main Comma v0.1-1T run, with the notable exception that both ablationsslightly outperform Comma v0.1-1T on the coding benchmarks. We conclude that the benchmarkresults reported for Comma v0.1-1T in subsection 4.4 seem relatively robust to minor changes intraining hyperparameters, dataset mixture curriculum (assuming similar amounts of most data splitsappear at some time during training), and the software environment and GPU hardware used to trainthe model.",
      "text": "Table 12: Comparison between our main Comma v0.1-1T training run and alternate runs performedwith different hyperparameters and data mixture curricula across a variety of knowledge, reasoning,and coding benchmarks. For “Main”, we report the performance of Comma v0.1-1T without averagingthe cooldown checkpoints so that it is a fair comparison.\n\nModelARC-CARC-EMMLUBoolQHSOBQACSQAPIQASIQAHEvalMBPPAvg.\n\nCurriculum45.269.141.474.760.846.859.170.548.638.134.653.58M Batch47.269.642.969.962.947.056.970.450.536.837.253.8Main50.868.440.272.962.346.259.571.051.232.134.653.6\n\nTable 13: Overview of the data mixing used to up/down-weight individual sources for the Stage 1 ofthe Curriculum ablation run. In this table we omit the size columns for brevity.\n\nSourceRepeatsTokens(Billions)Percentage\n\nUSPTO1.4125233.566.81%\n\nPre-1929Books5.6565.418.71%\n\nStack V2(HTML)11.312.83.65%\n\nUSGPO1.4112.83.65%\n\nLibrary ofCongress1.4112.63.59%\n\nBiodiversityHeritageLibrary\n\n1.4112.523.58%\n\nTotal–349.4100%\n\n54\n\nTable 14: Overview of the data mixing used to up/down-weight individual sources for the Stage 3 ofthe Curriculum ablation run. In this table we omit the size columns for brevity.\n\nSourceRepeatsTokens(Billions)Percentage\n\nStack V2163.818.519%\n\nDatabase ofOpen AccessBooks\n\n6185.230%\n\nWikimedia686.124.981%\n\nStackExchange2.556.116.259%\n\npeS2o145.613.241%\n\nCC CommonCrawl343.612.638%\n\nArXiv Papers524.47.063%\n\nDataProvenanceInitiative\n\n65.11.485%\n\nPressBooks60.870.251%\n\nLibreTexts60.540.157%\n\nNews60.370.108%\n\nFoodista60.120.036%\n\nOERCommons60.080.023%\n\nPEPs60.020.005%\n\nPublic DomainReview60.010.003%\n\nTotal–344.7100%\n\n55",
      "start_page": 54,
      "end_page": 55
    }
  ],
  "tables": [
    {
      "table_id": "9495b910-0c22-455a-bedc-10942f766532",
      "page": 2,
      "csv_path": "data\\processed\\tables\\The_Common_Pile_v0.1_p2_table1.csv",
      "rows": 9,
      "cols": 40,
      "raw": [
        [
          "Code\n(4775 GB) Gover ( n 1 m 17 e 2 n G t B & ) Legal (5 W 28 ik G is B) (26 W 0 e G b B) A (3 c P 7 a a 0 d p e e G m r B s i ) c Onl ( i 1 n 6 e 5 F G o B ru ) ms Pub (2 l B i 4 c o 4 D o G k o s m B) ain (2 O 9 t h G e B r ) E R d e u s c o a u t r io c n es al",
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null
        ],
        [
          null,
          "",
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null
        ],
        [
          "",
          "",
          "",
          null,
          "",
          "",
          "",
          "",
          null,
          null,
          "",
          "",
          "",
          "",
          "",
          null,
          null,
          null,
          "",
          "",
          "",
          null,
          null,
          "",
          "",
          "",
          null,
          "",
          "",
          "",
          "(15 GB)",
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null
        ],
        [
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          "",
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          "",
          null,
          null,
          null
        ],
        [
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          "",
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          "",
          null,
          null
        ],
        [
          "",
          "",
          "",
          null,
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          null,
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          ""
        ],
        [
          null,
          null,
          "",
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          "",
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null
        ],
        [
          null,
          "2V\nkcatS",
          "sPEP",
          "",
          "OTPSU",
          "PAC",
          "OPGSU",
          "drasnaH\nKU",
          "vog.snoitalugeR",
          "",
          "maetikiW",
          "aidemikiW",
          "",
          "CCCC",
          "sweN",
          "atsidooF",
          "RDP",
          "",
          "o2Sep",
          "deMbuP",
          "srepaP\nviXrA",
          "stcartsbA\nviXrA",
          "",
          "egnahcxE\nkcatS",
          "evihcrA\nbuHtiG",
          "CRI\nutnubU",
          "",
          "LHB",
          "skooB\n9291-erP",
          "ssergnoC\nfo\nyrarbiL",
          "grebnetuG\ntcejorP",
          "",
          "ebuTuoY\nCC",
          "IPD",
          "",
          "BAOD",
          "skooBsserP",
          "stxeTerbiL",
          "snommoC\nREO",
          null
        ],
        [
          null,
          "e\nso\nw\ngt\nS\n6\nsu\ne\nda\nse\no\nki\nlly\nrc\n1\nst\nle\nn\nna\nde\ntt\nnd\nr\nib\nth\nop\nve\nis\nh\ns\n-2\nel\nnc\npr\no\ng\ndi\ncti\nee\nc\net\nhe",
          "Co\nur\nit\nhe\ndo\n9,\nlti\nbi\nta\no\ntc\nng\n,\nh\n28\no\n[\nds\ntu\nls\nha\nA\nth\nuti\ne\ne\nri\nto\nat\nth\nT,\nst\ny,\ne-\nw\nra\nst\non\n,e\nop\no\np",
          null,
          "m\ns\nut\nut\nar\n,\nin\non\nso\nth\nns\nI\nil\nto\n4\nar\n].\npe\nfi\nin\nfo\ne\nco\ni\nm\nl\ndi\nca\nter\nfo\np\nin\ne\nin\ns\nut\nor\n.,\nig\nec\nli",
          "o\nco\nc\nho\ns [\n92\nn\ns\nra\neir\nen\ncr\neo\nt\n5],\ne\nr\nrs\ng\nllo\nnd\nnt\nnt\nm\nice\nve\nl b\na\nun\nair\ned\nrel\nin\nme\nco\ne,\nig\n1])\nht\nifi\ncd",
          "nP\nm\nom\nrs\n82\n,1\num\n[4\nis\nw\ntt\naw\np\nhe\na\nth\nefl\nts\non\nw\nix\nen\nhi\non\nns\nrs\noo\npp\nda\no\no\nea\ng\na\nnte\nan\nin\nty\nse\ncc\no",
          "il\npri\np\no\n].\n30\ne\n0,\nes\nor\noi\nle\nen\nst\nnd\neir\nec\ntep\nly\nst\nC)\ntt\nsp\nP\ned\ned\nk\nro\nti\nf\nnu\nse\nda\nn\nnt\nd\nal,\npi\nxp\nar\nma",
          "ei\nsi\nen\nfp\n,\nro\n96\net\nk\nts\nrs\nm\nud\nm\nd\nts\nt\np\nhe\n,\no\nap\nile\nt\no\ns,\npr\non\n7-\nn\nth\ntas\nb\nc\ncr\ncr\nca\nir\nve\nin",
          null,
          "an\nth\ntio\n-t\nil\n6]\nla\n15\nca\nrL\ne\n0\ne\no\nre,\ns\ngr\nar\nli\npe\nic\nfr\nis\n0.\ntt\nain\nu\nte\nor\nlli\nen\nC\n,a\n“o\nto\nte\nti\nyf\naft\nut\ny",
          "8\ne\nn\nra\ne\n,\nw\n9]\nl\nL\nas\n7]\nls\nf l\nt\nets\now\nd\ncd\nn\nh\nee\nt\n1,\no\ns\nca\nfil\nco\non\ns\nom\nn\np\nrs\nd\nve\nal\ner\ns\nth",
          null,
          "B\nom\nt\nin\npy\nny\nits\nBe\nnc\ntr\nL\nol\nin\nrn\nu\nn\ng\nso\nm\nno\nfer\na\ne\n8\nte\ncl\nn\nri\np\nar\nd\no\nll\nnl\nert\nva\nor\nit\nle\ng.\nc",
          "da\nm\nhe\ngd\nri\nr\na\nyo\ner\nai\nM\nlo\ned\nin\nse\nd\ndi\nlvi\nain\nw\ns\ncc\nmo\nT\n. T\nud\nal\nng\neti\na\nata\nnP\nd\ny\nai\ntiv\nks\nhi\nga\n,g\nop",
          "ta\no\ncr\nat\ngh\nig\ngai\nnd\nns\nni\ntr\nwi\no\ng\nof\nha\nvi\nng\na\nle\nto\nes\nns\nB\nh\nin\nre\n,d\ntiv\nme\nse\nil\nata\nlic\nnr\nes\n(t\nnt\nlly\nov\nyr",
          "se\nn\ne\na,\nt\nhts\nns\nq\n[9\nng\nain\nng\nn\nd\nu\nsp\nde\nt\nn\ndg\nco\nse\ntr\nda\ne\ngr\nso\ned\ne\nte\nts\ne\nc\ne\nig\no\nha\nhe\nd\ner\nig",
          null,
          "of\ne\nrs\nve\nem\nol\nLL\nest\nas\nn\ng\nro\nbl\nam\nic\nvi\net\nt\np\nFo\nen\nu\net\net\nm\nea\nce\npli\nL\nm\nc\n.1,\nlec\nse\ns,\nhe\nre\nco\nat\nme\no\n2",
          "o\nare\no\nn\np\nde\nM\nio\nc\nfa\nda\nwi\nic\nic\nen\nou\nwe\nen\nen\nun\nt\nse\nha\nth\nm\nrc\ns,\nca\nMs\nod\nha\nb\nti\nd\nsu\nir\n“\npe\ned\nnt\nwn",
          "pe\ns\nft\nat\ntio\nrs\nd\nns\non\nct,\nta\nng\nly\ns\nse\nsl\nen\nsio\nly\nda\nwh\nd,\ntt\nat\non\nh\na\ntio\n.\nels\ns\not\non\n”?\nch\nor\nfix\nof\nti\nd\ner",
          "nl\nho\nhi\nco\nn\nh\nev\no\nte\nre\n,a\na\nre\n[5\nd\ny\nL\nn\nli\nti\ner\nm\nhis\n—\nP\npu\nud\nn\nSp\nLl\nh\na\na\nig\ned\nc\nm\noc\ns(",
          null,
          "ic\nn\nco\ne\nor\ne\nop\nnt\ncr\nnt\nsh\nar\nas\n7\nin\nu\nM\nto\nns\n’s\nhe\nifi\nsi\no\nc\nic\nt\nnd\nifi\nh\na\nm\np\nxc\nal\nn\nyr\ner\ne\ng.",
          "en\nab\nnt\nrva\nt\nob\ner\nell\nea\ne\now\nen\ned\n6,\nin\nlte\nd\nas\ned\nO\nc\ned\nnd\nur\no\nati\nran\nr\nca\nco\n1\nm\nro\nlu\nw\nat\nig\nio\nnts\n,w",
          "se\nov\nen\nti\nex\nje\ns[\nec\nto\nvi\nn\nes\np\n8\ng\nd\nev\nk:\nte\npe\nop\n,\nee\nk\nmp\non\ns\new\nll\nm\nan\na\nce\nsi\nor\nan\nht.\nd)\ni\nit",
          null,
          "te\nca\nR\nly\nn\nd\n,1\nal\nra\nnc\ny\nof\n-tr\n,\nta\nD\nop\nsi\n?\nD\nig\nd\npo\nwl\nse\no\npt\ng\nw\nar\n2\n.1\nin\nri\n.\nbl\nWo\ne\nhe\na",
          "xt\nte\nec\nlo\nd\nto\n9\npr\nre\ne\nas\nw\nai\nme\nh\nM\ner\ntp\nW\nefi\nht\nsh\nss\ned\ns3\npe\ns,\nhti\net\nab\n7\nm\ngc\ngh\nAlt\ne\nrk\nre\nU\nCC",
          "c\ngo\nen\nw\ndat\nth\n1]\nop\nly\nsu\nha\neb\nni\nm\nea\nC\nsa\nos\ne\nni\nho\nar\nibl\nge\n0\nn-s\nan\nng\nra\nle\nB.\nod\nod\nts\nho\nme\nsi\nne\n.S\n0",
          "ur\nri\nt\nw\na\ne\nth\ne\nex\ngg\nrp\nd\nng\nor\nvi\nA\nn\nsi\nde\ntio\nld\ned\ne\n—\nte\no\nd\n,t\nin\npe\nIn\nel\ne.\n(w\nu\ndi\nn\nve\n.[\nlic",
          null,
          "ed\nd\ni\nge\nin\nco\nco\n(\nic\nts\nid\nab\nata\nati\nli\nke\no\nt\ne\n2.\nh\nr\nc\non\nso\nce\no\nC\no\nor\ne\nn\nh\nc\nm,\nep\nli\n),\nns",
          "f\nby\nma\nra\nin\nm\nul\nIP\nitl\nth\n-2\nei\ns\no\nmi\ndo\nnt\not\n“o\n1\nas\nan\nol\nst\nu\nco\nre\no\nm\nm\nsp\ndt\nce\nop\nsu\nu\ngi\nor\ne[",
          null,
          "m\nxt\nss\ns,\nex\nen\nca\naw\nco\nm\n23\nu\n[\n[1\nth\nns\ntc\nin\nen\nrt\nra\np\nti\nte\nes\ne,\nCr\nm\nv\nc\nit\nir\nin\nig\na\nc\nf\ne\n])",
          "3\nua\nu\nw\nis\nsa\nrry\n,\nns\nan\nin\nse\n18\n7,\ne\no\nre\np\nly\nhe\nnte\nurp\nng\nst\n(d\ngo\nuc\non\n0.\ne t\nof\nfil\ne\nht\ns\ndo\nor\nre\n.",
          "0\nld\ngg\nou\nt i\nted\nfi\nthe\nen\ny\ncr\nd\n,6\n22\nab\nfd\nat\nerf\nli\nrd\nd\no\n,c\nhe\net\nve\nia\nP\n1-\no\nop\nte\nxc\nla\nph\nma\nco\not\nCo",
          "di\no\nes\nld\nn\nu\nna\nu\ntt\nco\nea\nto\n4,\n],\nil\nat\nors\nor\nce\net\nex\nse\nur\nla\nail\nrn\nll\nile\n1T\nbu\ne\nre\nep\nws\nys\nin\npy\nhe\npy",
          null
        ]
      ]
    },
    {
      "table_id": "648334b4-16d6-493d-bfbf-789351278d72",
      "page": 9,
      "csv_path": "data\\processed\\tables\\The_Common_Pile_v0.1_p9_table1.csv",
      "rows": 6,
      "cols": 65,
      "raw": [
        [
          "",
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null
        ],
        [
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          "",
          null,
          null,
          null,
          null,
          null,
          null,
          null
        ],
        [
          "",
          null,
          null,
          null,
          null,
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          null,
          null,
          null,
          null,
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          null,
          null,
          null,
          null,
          null,
          null,
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          null,
          null,
          null,
          null,
          "",
          "",
          null,
          null,
          null,
          null,
          "",
          "",
          null,
          null,
          null,
          null,
          "",
          ""
        ],
        [
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          ""
        ],
        [
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          "",
          null,
          null
        ],
        [
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          "",
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null
        ]
      ]
    },
    {
      "table_id": "d884abf9-a5a0-47ef-b127-fc5f583732a5",
      "page": 10,
      "csv_path": "data\\processed\\tables\\The_Common_Pile_v0.1_p10_table1.csv",
      "rows": 6,
      "cols": 64,
      "raw": [
        [
          "",
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null
        ],
        [
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          "",
          null,
          null,
          null,
          null,
          null,
          null,
          null
        ],
        [
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          "",
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null
        ],
        [
          "",
          null,
          null,
          null,
          null,
          "",
          "",
          "",
          "",
          "",
          "",
          null,
          null,
          null,
          null,
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          null,
          null,
          null,
          null,
          null,
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          "",
          "",
          null,
          null,
          null,
          null,
          "",
          ""
        ],
        [
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          "",
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          "",
          null,
          null,
          null,
          null,
          "",
          "",
          null,
          null,
          null
        ],
        [
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          ""
        ]
      ]
    },
    {
      "table_id": "71965aff-2232-4e83-813e-d7d83c624860",
      "page": 47,
      "csv_path": "data\\processed\\tables\\The_Common_Pile_v0.1_p47_table1.csv",
      "rows": 7,
      "cols": 9,
      "raw": [
        [
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          ""
        ],
        [
          "",
          "All\nCode\nEducational Reso",
          "Oth\nOnl\nurces We",
          "er\nine Forums\nb",
          "Wikis\nGovernment\nAcademic Paper",
          "s",
          "",
          "",
          ""
        ],
        [
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          ""
        ],
        [
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          ""
        ],
        [
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          ""
        ],
        [
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          ""
        ],
        [
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          ""
        ]
      ]
    },
    {
      "table_id": "9bc68b81-8eac-44ac-9f02-e0a77106ccc3",
      "page": 53,
      "csv_path": "data\\processed\\tables\\The_Common_Pile_v0.1_p53_table1.csv",
      "rows": 5,
      "cols": 8,
      "raw": [
        [
          "31.8",
          "26.3",
          "29.9",
          "28.4",
          "26.8",
          "58.2",
          "38.0",
          "36.2"
        ],
        [
          "33.1",
          "27.5",
          "33.8",
          "27.4",
          "27.7",
          "59.4",
          "38.5",
          "37.3"
        ],
        [
          "34.2",
          "27.0",
          "33.6",
          "30.2",
          "26.4",
          "61.0",
          "37.7",
          "37.6"
        ],
        [
          "37.7",
          "28.7",
          "37.6",
          "31.0",
          "30.8",
          "63.8",
          "39.8",
          "40.0"
        ],
        [
          "38.0",
          "29.5",
          "39.9",
          "32.4",
          "29.6",
          "65.8",
          "39.4",
          "40.8"
        ]
      ]
    },
    {
      "table_id": "4ef9df5e-f074-4647-af7b-83ff9a926a09",
      "page": 53,
      "csv_path": "data\\processed\\tables\\The_Common_Pile_v0.1_p53_table2.csv",
      "rows": 3,
      "cols": 8,
      "raw": [
        [
          "37.0",
          "27.8",
          "35.8",
          "28.6",
          "31.5",
          "66.8",
          "38.2",
          "39.6"
        ],
        [
          "35.4",
          "27.6",
          "40.8",
          "30.4",
          "32.1",
          "69.7",
          "39.7",
          "40.9"
        ],
        [
          "38.0",
          "29.1",
          "48.2",
          "34.2",
          "33.6",
          "73.4",
          "40.3",
          "43.7"
        ]
      ]
    },
    {
      "table_id": "53ef66bf-46a7-4076-b3b7-6d75f102be74",
      "page": 53,
      "csv_path": "data\\processed\\tables\\The_Common_Pile_v0.1_p53_table3.csv",
      "rows": 6,
      "cols": 12,
      "raw": [
        [
          "42.8",
          "68.4",
          "27.8",
          "68.6",
          "70.3",
          "49.4",
          "57.7",
          "76.0",
          "46.9",
          "11.1",
          "15.9",
          "48.6"
        ],
        [
          "44.5",
          "67.9",
          "34.8",
          "75.4",
          "76.2",
          "51.2",
          "61.8",
          "77.2",
          "50.3",
          "19.9",
          "27.9",
          "53.4"
        ],
        [
          "50.8",
          "65.4",
          "45.2",
          "71.7",
          "75.6",
          "48.2",
          "57.2",
          "77.0",
          "48.2",
          "23.1",
          "32.0",
          "54.0"
        ],
        [
          "46.5",
          "70.5",
          "30.2",
          "74.2",
          "77.6",
          "48.6",
          "63.3",
          "77.3",
          "49.1",
          "27.3",
          "33.2",
          "54.3"
        ],
        [
          "44.5",
          "67.2",
          "40.3",
          "72.6",
          "72.6",
          "50.8",
          "62.8",
          "78.0",
          "49.7",
          "27.6",
          "33.9",
          "54.5"
        ],
        [
          "52.8",
          "68.4",
          "42.4",
          "75.7",
          "62.6",
          "47.0",
          "59.4",
          "70.8",
          "50.8",
          "36.5",
          "35.5",
          "54.7"
        ]
      ]
    },
    {
      "table_id": "e2483ae2-a3ed-4bc2-b266-f47256a6016d",
      "page": 53,
      "csv_path": "data\\processed\\tables\\The_Common_Pile_v0.1_p53_table4.csv",
      "rows": 4,
      "cols": 12,
      "raw": [
        [
          "45.2",
          "67.5",
          "28.2",
          "71.7",
          "73.4",
          "48.0",
          "61.8",
          "77.9",
          "48.5",
          "18.2",
          "27.5",
          "51.6"
        ],
        [
          "48.5",
          "69.5",
          "45.8",
          "80.2",
          "76.2",
          "48.4",
          "62.8",
          "76.7",
          "50.8",
          "26.1",
          "28.5",
          "55.8"
        ],
        [
          "45.8",
          "71.8",
          "49.8",
          "78.6",
          "64.4",
          "46.2",
          "64.0",
          "72.5",
          "52.3",
          "44.2",
          "41.5",
          "57.4"
        ],
        [
          "49.5",
          "67.7",
          "48.5",
          "71.7",
          "74.1",
          "52.0",
          "66.6",
          "77.8",
          "51.6",
          "43.1",
          "43.8",
          "58.8"
        ]
      ]
    }
  ],
  "figures": [
    {
      "image_id": "71cf400f-309f-4dd7-9cea-555bb511d03c",
      "page": 7,
      "path": "data\\processed\\images\\The_Common_Pile_v0.1_p7_img1.png",
      "ext": "png",
      "width": 3452,
      "height": 1094
    },
    {
      "image_id": "8b27ca19-5941-4c62-a9ad-d49022b8e0b3",
      "page": 28,
      "path": "data\\processed\\images\\The_Common_Pile_v0.1_p28_img1.png",
      "ext": "png",
      "width": 2980,
      "height": 2152
    },
    {
      "image_id": "cd1e868e-ca5b-4991-8e7a-9b0e3831c6ce",
      "page": 52,
      "path": "data\\processed\\images\\The_Common_Pile_v0.1_p52_img1.png",
      "ext": "png",
      "width": 3406,
      "height": 1780
    }
  ],
  "references": "[1] 17 U.S. Code § 102. Subject matter of copyright: In general, December 1990. URL https://www.law.cornell.edu/uscode/text/17/102.\n\n[2] 17 U.S. Code § 105. Subject matter of copyright: United States Government works, December2024. URL https://www.law.cornell.edu/uscode/text/17/105.\n\n[3] Alon Albalak, Liangming Pan, Colin Raffel, and William Yang Wang. Efficient online datamixing for language model pre-training. arXiv preprint arXiv:2312.02406, 2023.\n\n[4] Alon Albalak, Yanai Elazar, Sang Michael Xie, Shayne Longpre, Nathan Lambert, XinyiWang, Niklas Muennighoff, Bairu Hou, Liangming Pan, Haewon Jeong, et al. A survey ondata selection for language models. Transactions on Machine Learning Research, 2024.\n\n[5] Loubna Ben Allal, Anton Lozhkov, Elie Bakouch, Gabriel Martín Blázquez, GuilhermePenedo, Lewis Tunstall, Andrés Marafioti, Hynek Kydlíˇcek, Agustín Piqueres Lajarín, VaibhavSrivastav, Joshua Lochner, Caleb Fahlgren, Xuan-Son Nguyen, Clémentine Fourrier, BenBurtenshaw, Hugo Larcher, Haojun Zhao, Cyril Zakka, Mathieu Morlon, Colin Raffel, Leandrovon Werra, and Thomas Wolf. Smollm2: When smol goes big – data-centric training of asmall language model, 2025. URL https://arxiv.org/abs/2502.02737.\n\n[6] Mikel Artetxe, Sebastian Ruder, and Dani Yogatama. On the cross-lingual transferabilityof monolingual representations. In Annual Meeting of the Association for ComputationalLinguistics, pages 4623–4637, 2019.\n\n[7] Viraat Aryabumi, Yixuan Su, Raymond Ma, Adrien Morisot, Ivan Zhang, Acyr Locatelli,Marzieh Fadaee, Ahmet Üstün, and Sara Hooker. To code, or not to code? exploring impact ofcode in pre-training. arXiv preprint arXiv:2408.10914, 2024.\n\n[8] Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, DavidDohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with largelanguage models. arXiv preprint arXiv:2108.07732, 2021.\n\n[9] Stefan Baack, Stella Biderman, Kasia Odrozek, Aviya Skowron, Ayah Bdeir, Jillian Bom-marito, Jennifer Ding, Maximilian Gahntz, Paul Keller, Pierre-Carl Langlais, Greg Lindahl,Sebastian Majstorovic, Nik Marda, Guilherme Penedo, Maarten Van Segbroeck, Jennifer Wang,Leandro von Werra, Mitchell Baker, Julie Belião, Kasia Chmielinski, Marzieh Fadaee, LisaGutermuth, Hynek Kydlíˇcek, Greg Leppert, EM Lewis-Jong, Solana Larsen, Shayne Long-pre, Angela Oduor Lungati, Cullen Miller, Victor Miller, Max Ryabinin, Kathleen Siminyu,Andrew Strait, Mark Surman, Anna Tumadóttir, Maurice Weber, Rebecca Weiss, Lee White,and Thomas Wolf. Towards best practices for open datasets for llm training, 2025. URLhttps://arxiv.org/abs/2501.08365.\n\n[10] Adrien Barbaresi. Trafilatura: A Web Scraping Library and Command-Line Tool for TextDiscovery and Extraction. In Proceedings of the Joint Conference of the 59th Annual Meetingof the Association for Computational Linguistics and the 11th International Joint Conferenceon Natural Language Processing: System Demonstrations, pages 122–131. Association for\n\n11\n\nComputational Linguistics, 2021. URL https://aclanthology.org/2021.acl-demo.15.\n\n[11] Max Bartolo, A. Roberts, Johannes Welbl, Sebastian Riedel, and Pontus Stenetorp. Beat theai: Investigating adversarial human annotation for reading comprehension. Transactions of theAssociation for Computational Linguistics, 8:662–678, 2020.\n\n[12] Marco Bellagente, Jonathan Tow, Dakota Mahan, Duy Phung, Maksym Zhuravinskyi, ReshinthAdithyan, James Baicoianu, Ben Brooks, Nathan Cooper, Ashish Datta, et al. Stable lm 2 1.6b technical report. arXiv preprint arXiv:2402.17834, 2024.\n\n[13] Jonathan Berant, A. Chou, Roy Frostig, and Percy Liang. Semantic parsing on freebase fromquestion-answer pairs. In Conference on Empirical Methods in Natural Language Processing,pages 1533–1544, 2013.\n\n[14] Janek Bevendorff, Benno Stein, Matthias Hagen, and Martin Potthast. Elastic ChatNoir:Search Engine for the ClueWeb and the Common Crawl. In Leif Azzopardi, Allan Hanbury,Gabriella Pasi, and Benjamin Piwowarski, editors, Advances in Information Retrieval. 40thEuropean Conference on IR Research (ECIR 2018), Lecture Notes in Computer Science, BerlinHeidelberg New York, March 2018. Springer.\n\n[15] Janek Bevendorff, Martin Potthast, and Benno Stein. FastWARC: Optimizing Large-ScaleWeb Archive Analytics. In Andreas Wagner, Christian Guetl, Michael Granitzer, and StefanVoigt, editors, 3rd International Symposium on Open Search Technology (OSSYM 2021).International Open Search Symposium, October 2021.\n\n[16] Xiao Bi, Deli Chen, Guanting Chen, Shanhuang Chen, Damai Dai, Chengqi Deng, HonghuiDing, Kai Dong, Qiushi Du, Zhe Fu, et al. Deepseek llm: Scaling open-source languagemodels with longtermism. arXiv preprint arXiv:2401.02954, 2024.\n\n[17] Stella Biderman, Usvsn Prashanth, Lintang Sutawika, Hailey Schoelkopf, Quentin Anthony,Shivanshu Purohit, and Edward Raff. Emergent and predictable memorization in large languagemodels. Advances in Neural Information Processing Systems, 36:28072–28090, 2023.\n\n[18] Stella Biderman, Hailey Schoelkopf, Quentin Anthony, Herbie Bradley, Kyle O’Brien, EricHallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff,Aviya Skowron, Lintang Sutawika, and Oskar van der Wal. Pythia: A suite for analyzing largelanguage models across training and scaling, 2023. URL https://arxiv.org/abs/2304.01373.\n\n[19] Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, and Yejin Choi. Piqa: Reasoningabout physical commonsense in natural language, 2019. URL https://arxiv.org/abs/1911.11641.\n\n[20] Blue Oak Council. License List (version 15), 2025. URL https://blueoakcouncil.org/list.\n\n[21] Oana-Maria Camburu, Tim Rocktäschel, Thomas Lukasiewicz, and Phil Blunsom. e-snli: Nat-ural language inference with natural language explanations. In Neural Information ProcessingSystems, pages 9560–9572, 2018.\n\n[22] Nicholas Carlini, Daphne Ippolito, Matthew Jagielski, Katherine Lee, Florian Tramer, andChiyuan Zhang. Quantifying memorization across neural language models. In The EleventhInternational Conference on Learning Representations, 2022.\n\n[23] Ilias Chalkidis, Abhik Jana, D. Hartung, M. Bommarito, Ion Androutsopoulos, D. Katz, andNikolaos Aletras. Lexglue: A benchmark dataset for legal language understanding in english.In Annual Meeting of the Association for Computational Linguistics, pages 4310–4330, 2021.\n\n[24] Chat GPT Is Eating the World, 2024. URL https://chatgptiseatingtheworld.com.\n\n12\n\n[25] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto,Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, RaulPuri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, BrookeChan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, MohammadBavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, MatthiasPlappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, AlexNichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain,William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra,Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer,Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and WojciechZaremba. Evaluating large language models trained on code, 2021.\n\n[26] Wenhu Chen, Hanwen Zha, Zhiyu Chen, Wenhan Xiong, Hong Wang, and William YangWang. HybridQA: A dataset of multi-hop question answering over tabular and textual data. InTrevor Cohn, Yulan He, and Yang Liu, editors, Findings of the Association for ComputationalLinguistics: EMNLP 2020, pages 1026–1036, Online, November 2020. Association forComputational Linguistics.doi: 10.18653/v1/2020.findings-emnlp.91.URL https://aclanthology.org/2020.findings-emnlp.91/.\n\n[27] Zhiyu Chen, Wenhu Chen, Hanwen Zha, Xiyou Zhou, Yunkai Zhang, Sairam Sundaresan, andWilliam Yang Wang. Logic2text: High-fidelity natural language generation from logical forms.ArXiv, abs/2004.14579, 2020.\n\n[28] Sang Keun Choe, Hwijeen Ahn, Juhan Bae, Kewen Zhao, Minsoo Kang, Youngseog Chung,Adithya Pratapa, Willie Neiswanger, Emma Strubell, Teruko Mitamura, Jeff Schneider, EduardHovy, Roger Grosse, and Eric Xing. What is your data worth to gpt? llm-scale data valuationwith influence functions, 2024. URL https://arxiv.org/abs/2405.13954.\n\n[29] Eunsol Choi, He He, Mohit Iyyer, Mark Yatskar, Wen tau Yih, Yejin Choi, Percy Liang, andLuke Zettlemoyer. Quac: Question answering in context. In Conference on Empirical Methodsin Natural Language Processing, pages 2174–2184, 2018.\n\n[30] cjadams, Jeffrey Sorensen, Julia Elliott, Lucas Dixon, Mark McDonald, nithum, and WillCukierski. Toxic comment classification challenge. https://kaggle.com/competitions/jigsaw-toxic-comment-classification-challenge, 2017. Kaggle.\n\n[31] Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, andKristina Toutanova. Boolq: Exploring the surprising difficulty of natural yes/no questions.arXiv preprint arXiv:1905.10044, 2019.\n\n[32] Kevin Clark, Urvashi Khandelwal, Omer Levy, and Christopher D Manning. What doesBERT look at? an analysis of BERT’s attention. In Proceedings of the 2019 ACL WorkshopBlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, 2019.\n\n[33] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick,and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoningchallenge. arXiv preprint arXiv:1803.05457, 2018.\n\n[34] Karl Cobbe, V. Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser,Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and JohnSchulman. Training verifiers to solve math word problems. ArXiv, abs/2110.14168, 2021.\n\n[35] Creative Commons. CC0 1.0 Universal (CC0 1.0) Public Domain Dedication, 2025. URL\n\nhttps://creativecommons.org/publicdomain/zero/1.0/.\n\n[36] Creative Commons. Creative Commons Attribution 4.0 International License § 2(a)(1)(A),2025. URL https://creativecommons.org/licenses/by/4.0/legalcode.\n\n[37] Creative Commons. Creative Commons Attribution 4.0 International License § 3(a)(1)(A)(i),2025. URL https://creativecommons.org/licenses/by/4.0/legalcode.\n\n[38] Creative Commons. Public Domain Mark 1.0, 2025. URL https://creativecommons.org/publicdomain/mark/1.\n\n13\n\n[39] Creative Commons. Creative Commons Attribution-ShareAlike 4.0 International License,2025. URL https://creativecommons.org/licenses/by-sa/4.0/.\n\n[40] A. Feder Cooper and James Grimmelmann. The Files are in the Computer: Copyright,Memorization, and Generative AI. arXiv preprint arXiv:2404.12590, 2024.\n\n[41] A. Feder Cooper, Aaron Gokaslan, Amy B. Cyphert, Christopher De Sa, Mark A. Lemley,Daniel E. Ho, and Percy Liang. Extracting memorized pieces of (copyrighted) books fromopen-weight language models. arXiv preprint arXiv:2505.12546, 2025.\n\n[42] Yiming Cui, Ting Liu, Li Xiao, Zhipeng Chen, Wentao Ma, Wanxiang Che, Shijin Wang,and Guoping Hu. A span-extraction dataset for chinese machine reading comprehension. InEMNLP-IJCNLP, pages 5882–5888, 2019.\n\n[43] Bhavana Dalvi, Lifu Huang, Niket Tandon, Wen tau Yih, and Peter Clark. Tracking statechanges in procedural text: a challenge dataset and models for process paragraph comprehen-sion. In North American Chapter of the Association for Computational Linguistics, pages1595–1604, 2018.\n\n[44] Pradeep Dasigi, Nelson F. Liu, Ana Marasovi´c, Noah A. Smith, and Matt Gardner. Quoref: Areading comprehension dataset with questions requiring coreferential reasoning. In Conferenceon Empirical Methods in Natural Language Processing, volume abs/1908.05803, 2019.\n\n[45] Ning Ding, Guangwei Xu, Yulin Chen, Xiaobin Wang, Xu Han, Pengjun Xie, HaitaoZheng, and Zhiyuan Liu. Few-nerd: A few-shot named entity recognition dataset. ArXiv,abs/2105.07464, 2021.\n\n[46] Dheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, and MattGardner. Drop: A reading comprehension benchmark requiring discrete reasoning overparagraphs. In North American Chapter of the Association for Computational Linguistics,pages 2368–2378, 2019.\n\n[47] Michael Duan, Anshuman Suri, Niloofar Mireshghallah, Sewon Min, Weijia Shi, Luke Zettle-moyer, Yulia Tsvetkov, Yejin Choi, David Evans, and Hannaneh Hajishirzi. Do membershipinference attacks work on large language models? arXiv preprint arXiv:2402.07841, 2024.\n\n[48] S. Dumitrescu, Petru Rebeja, Beáta L˝orincz, Mihaela G˘aman, M. Ilie, Andrei Pruteanu,Adriana Stan, Luciana Morogan, Traian Rebedea, and Sebastian Ruder. Liro: Benchmark andleaderboard for romanian language tasks. In NeurIPS Datasets and Benchmarks, 2021.\n\n[49] Yanai Elazar and Yoav Goldberg. Where’s my head? definition, data set, and models fornumeric fused-head identification and resolution. Transactions of the Association for Compu-tational Linguistics, 7:519–535, 2019.\n\n[50] Yanai Elazar, Nora Kassner, Shauli Ravfogel, Amir Feder, Abhilasha Ravichander, MariusMosbach, Yonatan Belinkov, Hinrich Schütze, and Yoav Goldberg. Measuring causal effectsof data statistics on language model’sfactual’predictions. arXiv preprint arXiv:2207.14251,2022.\n\n[51] Denis Emelin, Ronan Le Bras, Jena D. Hwang, Maxwell Forbes, and Yejin Choi. Moralstories: Situated reasoning about norms, intents, actions, and their consequences. ArXiv,abs/2012.15738, 2020.\n\n[52] Alex Fang, Hadi Pouransari, Matt Jordan, Alexander Toshev, Vaishaal Shankar, LudwigSchmidt, and Tom Gunter. Datasets, documents, and repetitions: The practicalities of unequaldata quality. arXiv preprint arXiv:2503.07879, 2025.\n\n[53] James Ferguson, Matt Gardner, Tushar Khot, and Pradeep Dasigi. Iirc: A dataset of incompleteinformation reading comprehension questions. In Conference on Empirical Methods in NaturalLanguage Processing, pages 1137–1147, 2020.\n\n[54] Nancy Fulda, Nathan Tibbetts, Zachary Brown, and D. Wingate. Harvesting common-sensenavigational knowledge for robotics from uncurated text corpora. In Conference on RobotLearning, pages 525–534, 2017.\n\n14\n\n[55] Philip Gage. A new algorithm for data compression. The C Users Journal archive, 12:23–38,1994. URL https://api.semanticscholar.org/CorpusID:59804030.\n\n[56] N. Gale, G. Heath, E. Cameron, S. Rashid, and S. Redwood. Using the framework method forthe analysis of qualitative data in multi-disciplinary health research. BMC Medical ResearchMethodology, 13:117 – 117, 2013.\n\n[57] Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster,Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy.The pile: An 800gb dataset of diverse text for language modeling, 2020. URL https://arxiv.org/abs/2101.00027.\n\n[58] Xinyang Geng and Hao Liu. Openllama: An open reproduction of llama, May 2023. URL\n\nhttps://github.com/openlm-research/open_llama.\n\n[59] Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, D. Roth, and Jonathan Berant. Didaristotle use a laptop? a question answering benchmark with implicit reasoning strategies.Transactions of the Association for Computational Linguistics, 9:346–361, 2021.\n\n[60] Max Glockner, Vered Shwartz, and Yoav Goldberg. Breaking nli systems with sentences thatrequire simple lexical inferences. ArXiv, abs/1805.02266, 2018.\n\n[61] Aaron Gokaslan, A. Feder Cooper, Jasmine Collins, Landan Seguin, Austin Jacobson, MihirPatel, Jonathan Frankle, Cory Stephenson, and Volodymyr Kuleshov. CommonCanvas: OpenDiffusion Models Trained on Creative-Commons Images. In Proceedings of the IEEE/CVFConference on Computer Vision and Pattern Recognition (CVPR), pages 8250–8260, June2024.\n\n[62] Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian,Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, Amy Yang,Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravanku-mar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, Aurelien Rodriguez, AustenGregerson, Ava Spataru, Baptiste Roziere, Bethany Biron, Binh Tang, Bobbie Chern, Char-lotte Caucheteux, Chaya Nayak, Chloe Bi, Chris Marra, Chris McConnell, Christian Keller,Christophe Touret, Chunyang Wu, Corinne Wong, Cristian Canton Ferrer, Cyrus Nikolaidis,Damien Allonsius, Daniel Song, Danielle Pintz, Danny Livshits, Danny Wyatt, David Esiobu,Dhruv Choudhary, Dhruv Mahajan, Diego Garcia-Olano, Diego Perino, Dieuwke Hupkes,Egor Lakomkin, Ehab AlBadawy, Elina Lobanova, Emily Dinan, Eric Michael Smith, FilipRadenovic, Francisco Guzmán, Frank Zhang, Gabriel Synnaeve, Gabrielle Lee, Georgia LewisAnderson, Govind Thattai, Graeme Nail, Gregoire Mialon, Guan Pang, Guillem Cucurell,Hailey Nguyen, Hannah Korevaar, Hu Xu, Hugo Touvron, Iliyan Zarov, Imanol Arrieta Ibarra,Isabel Kloumann, Ishan Misra, Ivan Evtimov, Jack Zhang, Jade Copet, Jaewon Lee, JanGeffert, Jana Vranes, Jason Park, Jay Mahadeokar, Jeet Shah, Jelmer van der Linde, JenniferBillock, Jenny Hong, Jenya Lee, Jeremy Fu, Jianfeng Chi, Jianyu Huang, Jiawen Liu, JieWang, Jiecao Yu, Joanna Bitton, Joe Spisak, Jongsoo Park, Joseph Rocca, Joshua Johnstun,Joshua Saxe, Junteng Jia, Kalyan Vasuden Alwala, Karthik Prasad, Kartikeya Upasani, KatePlawiak, Ke Li, Kenneth Heafield, Kevin Stone, Khalid El-Arini, Krithika Iyer, Kshitiz Malik,Kuenley Chiu, Kunal Bhalla, Kushal Lakhotia, Lauren Rantala-Yeary, Laurens van der Maaten,Lawrence Chen, Liang Tan, Liz Jenkins, Louis Martin, Lovish Madaan, Lubo Malo, LukasBlecher, Lukas Landzaat, Luke de Oliveira, Madeline Muzzi, Mahesh Pasupuleti, MannatSingh, Manohar Paluri, Marcin Kardas, Maria Tsimpoukelli, Mathew Oldham, Mathieu Rita,Maya Pavlova, Melanie Kambadur, Mike Lewis, Min Si, Mitesh Kumar Singh, Mona Hassan,Naman Goyal, Narjes Torabi, Nikolay Bashlykov, Nikolay Bogoychev, Niladri Chatterji, NingZhang, Olivier Duchenne, Onur Çelebi, Patrick Alrassy, Pengchuan Zhang, Pengwei Li, PetarVasic, Peter Weng, Prajjwal Bhargava, Pratik Dubal, Praveen Krishnan, Punit Singh Koura,Puxin Xu, Qing He, Qingxiao Dong, Ragavan Srinivasan, Raj Ganapathy, Ramon Calderer,Ricardo Silveira Cabral, Robert Stojnic, Roberta Raileanu, Rohan Maheswari, Rohit Gird-har, Rohit Patel, Romain Sauvestre, Ronnie Polidoro, Roshan Sumbaly, Ross Taylor, RuanSilva, Rui Hou, Rui Wang, Saghar Hosseini, Sahana Chennabasappa, Sanjay Singh, SeanBell, Seohyun Sonia Kim, Sergey Edunov, Shaoliang Nie, Sharan Narang, Sharath Raparthy,Sheng Shen, Shengye Wan, Shruti Bhosale, Shun Zhang, Simon Vandenhende, Soumya Batra,\n\n15\n\nSpencer Whitman, Sten Sootla, Stephane Collot, Suchin Gururangan, Sydney Borodinsky,Tamar Herman, Tara Fowler, Tarek Sheasha, Thomas Georgiou, Thomas Scialom, TobiasSpeckbacher, Todor Mihaylov, Tong Xiao, Ujjwal Karn, Vedanuj Goswami, Vibhor Gupta, Vi-gnesh Ramanathan, Viktor Kerkez, Vincent Gonguet, Virginie Do, Vish Vogeti, Vítor Albiero,Vladan Petrovic, Weiwei Chu, Wenhan Xiong, Wenyin Fu, Whitney Meers, Xavier Martinet,Xiaodong Wang, Xiaofang Wang, Xiaoqing Ellen Tan, Xide Xia, Xinfeng Xie, Xuchao Jia,Xuewei Wang, Yaelle Goldschlag, Yashesh Gaur, Yasmine Babaei, Yi Wen, Yiwen Song,Yuchen Zhang, Yue Li, Yuning Mao, Zacharie Delpierre Coudert, Zheng Yan, ZhengxingChen, Zoe Papakipos, Aaditya Singh, Aayushi Srivastava, Abha Jain, Adam Kelsey, AdamShajnfeld, Adithya Gangidi, Adolfo Victoria, Ahuva Goldstand, Ajay Menon, Ajay Sharma,Alex Boesenberg, Alexei Baevski, Allie Feinstein, Amanda Kallet, Amit Sangani, Amos Teo,Anam Yunus, Andrei Lupu, Andres Alvarado, Andrew Caples, Andrew Gu, Andrew Ho,Andrew Poulton, Andrew Ryan, Ankit Ramchandani, Annie Dong, Annie Franco, Anuj Goyal,Aparajita Saraf, Arkabandhu Chowdhury, Ashley Gabriel, Ashwin Bharambe, Assaf Eisenman,Azadeh Yazdan, Beau James, Ben Maurer, Benjamin Leonhardi, Bernie Huang, Beth Loyd,Beto De Paola, Bhargavi Paranjape, Bing Liu, Bo Wu, Boyu Ni, Braden Hancock, Bram Wasti,Brandon Spence, Brani Stojkovic, Brian Gamido, Britt Montalvo, Carl Parker, Carly Burton,Catalina Mejia, Ce Liu, Changhan Wang, Changkyu Kim, Chao Zhou, Chester Hu, Ching-Hsiang Chu, Chris Cai, Chris Tindal, Christoph Feichtenhofer, Cynthia Gao, Damon Civin,Dana Beaty, Daniel Kreymer, Daniel Li, David Adkins, David Xu, Davide Testuggine, DeliaDavid, Devi Parikh, Diana Liskovich, Didem Foss, Dingkang Wang, Duc Le, Dustin Holland,Edward Dowling, Eissa Jamil, Elaine Montgomery, Eleonora Presani, Emily Hahn, EmilyWood, Eric-Tuan Le, Erik Brinkman, Esteban Arcaute, Evan Dunbar, Evan Smothers, Fei Sun,Felix Kreuk, Feng Tian, Filippos Kokkinos, Firat Ozgenel, Francesco Caggioni, Frank Kanayet,Frank Seide, Gabriela Medina Florez, Gabriella Schwarz, Gada Badeer, Georgia Swee, GilHalpern, Grant Herman, Grigory Sizov, Guangyi, Zhang, Guna Lakshminarayanan, HakanInan, Hamid Shojanazeri, Han Zou, Hannah Wang, Hanwen Zha, Haroun Habeeb, HarrisonRudolph, Helen Suk, Henry Aspegren, Hunter Goldman, Hongyuan Zhan, Ibrahim Damlaj,Igor Molybog, Igor Tufanov, Ilias Leontiadis, Irina-Elena Veliche, Itai Gat, Jake Weissman,James Geboski, James Kohli, Janice Lam, Japhet Asher, Jean-Baptiste Gaya, Jeff Marcus, JeffTang, Jennifer Chan, Jenny Zhen, Jeremy Reizenstein, Jeremy Teboul, Jessica Zhong, JianJin, Jingyi Yang, Joe Cummings, Jon Carvill, Jon Shepard, Jonathan McPhie, Jonathan Torres,Josh Ginsburg, Junjie Wang, Kai Wu, Kam Hou U, Karan Saxena, Kartikay Khandelwal,Katayoun Zand, Kathy Matosich, Kaushik Veeraraghavan, Kelly Michelena, Keqian Li, KiranJagadeesh, Kun Huang, Kunal Chawla, Kyle Huang, Lailin Chen, Lakshya Garg, Lavender A,Leandro Silva, Lee Bell, Lei Zhang, Liangpeng Guo, Licheng Yu, Liron Moshkovich, LucaWehrstedt, Madian Khabsa, Manav Avalani, Manish Bhatt, Martynas Mankus, Matan Hasson,Matthew Lennie, Matthias Reso, Maxim Groshev, Maxim Naumov, Maya Lathi, MeghanKeneally, Miao Liu, Michael L. Seltzer, Michal Valko, Michelle Restrepo, Mihir Patel, MikVyatskov, Mikayel Samvelyan, Mike Clark, Mike Macey, Mike Wang, Miquel Jubert Hermoso,Mo Metanat, Mohammad Rastegari, Munish Bansal, Nandhini Santhanam, Natascha Parks,Natasha White, Navyata Bawa, Nayan Singhal, Nick Egebo, Nicolas Usunier, Nikhil Mehta,Nikolay Pavlovich Laptev, Ning Dong, Norman Cheng, Oleg Chernoguz, Olivia Hart, OmkarSalpekar, Ozlem Kalinli, Parkin Kent, Parth Parekh, Paul Saab, Pavan Balaji, Pedro Rittner,Philip Bontrager, Pierre Roux, Piotr Dollar, Polina Zvyagina, Prashant Ratanchandani, PritishYuvraj, Qian Liang, Rachad Alao, Rachel Rodriguez, Rafi Ayub, Raghotham Murthy, RaghuNayani, Rahul Mitra, Rangaprabhu Parthasarathy, Raymond Li, Rebekkah Hogan, RobinBattey, Rocky Wang, Russ Howes, Ruty Rinott, Sachin Mehta, Sachin Siby, Sai Jayesh Bondu,Samyak Datta, Sara Chugh, Sara Hunt, Sargun Dhillon, Sasha Sidorov, Satadru Pan, SaurabhMahajan, Saurabh Verma, Seiji Yamamoto, Sharadh Ramaswamy, Shaun Lindsay, Shaun Lind-say, Sheng Feng, Shenghao Lin, Shengxin Cindy Zha, Shishir Patil, Shiva Shankar, ShuqiangZhang, Shuqiang Zhang, Sinong Wang, Sneha Agarwal, Soji Sajuyigbe, Soumith Chintala,Stephanie Max, Stephen Chen, Steve Kehoe, Steve Satterfield, Sudarshan Govindaprasad,Sumit Gupta, Summer Deng, Sungmin Cho, Sunny Virk, Suraj Subramanian, Sy Choudhury,Sydney Goldman, Tal Remez, Tamar Glaser, Tamara Best, Thilo Koehler, Thomas Robinson,Tianhe Li, Tianjun Zhang, Tim Matthews, Timothy Chou, Tzook Shaked, Varun Vontimitta,Victoria Ajayi, Victoria Montanez, Vijai Mohan, Vinay Satish Kumar, Vishal Mangla, VladIonescu, Vlad Poenaru, Vlad Tiberiu Mihailescu, Vladimir Ivanov, Wei Li, Wenchen Wang,Wenwen Jiang, Wes Bouaziz, Will Constable, Xiaocheng Tang, Xiaojian Wu, Xiaolan Wang,\n\n16\n\nXilun Wu, Xinbo Gao, Yaniv Kleinman, Yanjun Chen, Ye Hu, Ye Jia, Ye Qi, Yenda Li, YilinZhang, Ying Zhang, Yossi Adi, Youngjin Nam, Yu, Wang, Yu Zhao, Yuchen Hao, YundiQian, Yunlu Li, Yuzi He, Zach Rait, Zachary DeVito, Zef Rosnbrick, Zhaoduo Wen, ZhenyuYang, Zhiwei Zhao, and Zhiyu Ma. The Llama 3 Herd of Models, November 2024. URL\n\nhttp://arxiv.org/abs/2407.21783. arXiv:2407.21783 [cs].\n\n[63] Grobid. Grobid. https://github.com/kermitt2/grobid, 2008–2025.\n\n[64] Dirk Groeneveld, Iz Beltagy, Pete Walsh, Akshita Bhagia, Rodney Kinney, Oyvind Tafjord,Ananya Harsh Jha, Hamish Ivison, Ian Magnusson, Yizhong Wang, Shane Arora, David Atkin-son, Russell Authur, Khyathi Raghavi Chandu, Arman Cohan, Jennifer Dumas, Yanai Elazar,Yuling Gu, Jack Hessel, Tushar Khot, William Merrill, Jacob Morrison, Niklas Muennighoff,Aakanksha Naik, Crystal Nam, Matthew E. Peters, Valentina Pyatkin, Abhilasha Ravichander,Dustin Schwenk, Saurabh Shah, Will Smith, Emma Strubell, Nishant Subramani, MitchellWortsman, Pradeep Dasigi, Nathan Lambert, Kyle Richardson, Luke Zettlemoyer, Jesse Dodge,Kyle Lo, Luca Soldaini, Noah A. Smith, and Hannaneh Hajishirzi. Olmo: Accelerating thescience of language models, 2024. URL https://arxiv.org/abs/2402.00838.\n\n[65] Yuling Gu, Oyvind Tafjord, Bailey Kuehl, Dany Haddad, Jesse Dodge, and HannanehHajishirzi.Olmes: A standard for language model evaluations, 2025.URL https://arxiv.org/abs/2406.08446.\n\n[66] Mandy Guo, Zihang Dai, Denny Vrandeˇci´c, and Rami Al-Rfou. Wiki-40b: Multilinguallanguage model dataset. In Proceedings of the Twelfth Language Resources and EvaluationConference, pages 2440–2452, 2020.\n\n[67] Aditya Gupta, Jiacheng Xu, Shyam Upadhyay, Diyi Yang, and Manaal Faruqui.Disfl-qa: A benchmark dataset for understanding disfluencies in question answering.ArXiv,abs/2106.04016, 2021.\n\n[68] Ivan Habernal, Omnia Zayed, and Iryna Gurevych. C4Corpus: Multilingual web-size corpuswith free license. In Nicoletta Calzolari, Khalid Choukri, Thierry Declerck, Sara Goggi,Marko Grobelnik, Bente Maegaard, Joseph Mariani, Helene Mazo, Asuncion Moreno, JanOdijk, and Stelios Piperidis, editors, Proceedings of the Tenth International Conference onLanguage Resources and Evaluation (LREC‘16), pages 914–922, Portorož, Slovenia, May2016. European Language Resources Association (ELRA). URL https://aclanthology.org/L16-1146/.\n\n[69] Seth Hays.AI Training and Copyright Infringement:Solutions from Asia, Octo-ber 2024.URL https://www.techpolicy.press/ai-training-and-copyright-infringement-solutions-from-asia/.\n\n[70] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, andJacob Steinhardt. Measuring massive multitask language understanding. arXiv preprintarXiv:2009.03300, 2020.\n\n[71] Dan Hendrycks, Collin Burns, Anya Chen, and Spencer Ball. Cuad: An expert-annotated nlpdataset for legal contract review. ArXiv, abs/2103.06268, 2021.\n\n[72] Jeremy Howard and Sebastian Ruder. Universal language model fine-tuning for text classi-fication. In Proceedings of the 56th Annual Meeting of the Association for ComputationalLinguistics, 2018.\n\n[73] Shengding Hu, Yuge Tu, Xu Han, Chaoqun He, Ganqu Cui, Xiang Long, Zhi Zheng, YeweiFang, Yuxiang Huang, Weilin Zhao, et al. Minicpm: Unveiling the potential of small languagemodels with scalable training strategies. arXiv preprint arXiv:2404.06395, 2024.\n\n[74] HuggingFace: Common Corpus, 2025.URL https://huggingface.co/datasets/PleIAs/common_corpus.\n\n[75] Jena D. Hwang, Chandra Bhagavatula, Ronan Le Bras, Jeff Da, Keisuke Sakaguchi, AntoineBosselut, and Yejin Choi. Comet-atomic 2020: On symbolic and neural commonsenseknowledge graphs. In AAAI Conference on Artificial Intelligence, pages 6384–6392, 2020.\n\n17\n\n[76] Adam Ibrahim, Benjamin Thérien, Kshitij Gupta, Mats L Richter, Quentin Anthony, TimothéeLesort, Eugene Belilovsky, and Irina Rish. Simple and scalable strategies to continuallypre-train large language models. arXiv preprint arXiv:2403.08763, 2024.\n\n[77] IFI CLAIMS Patent Services and Google. Google patents public data. https://patents.google.com/, 2023. Licensed under a Creative Commons Attribution 4.0 InternationalLicense.\n\n[78] Michael J Bommarito II, Jillian Bommarito, and Daniel Martin Katz. The kl3m data project:Copyright-clean training resources for large language models, 2025. URL https://arxiv.org/abs/2504.07854.\n\n[79] Infocomm Media Development Authority of Singapore (IMDA), Aicadium, and AI VerifyFoundation. Model AI Governance Framework for Generative AI: Fostering a Trusted Ecosys-tem, May 2024. URL https://aiverifyfoundation.sg/wp-content/uploads/2024/05/Model-AI-Governance-Framework-for-Generative-AI-May-2024-1-1.pdf.\n\n[80] Najko Jahn, Nick Haupka, and Anne Hobert. Analysing and reclassifying open access informa-tion in OpenAlex, 2023. URL https://subugoe.github.io/scholcomm_analytics/posts/oalex_oa_status/?utm_source=chatgpt.com.\n\n[81] Armand Joulin, Edouard Grave, Piotr Bojanowski, and Tomas Mikolov. Bag of tricks forefficient text classification. In Proceedings of the 15th Conference of the European Chapterof the Association for Computational Linguistics: Volume 2, Short Papers, pages 427–431.Association for Computational Linguistics, April 2017.\n\n[82] Nikhil Kandpal and Colin Raffel. Position: The most expensive part of an llm should be itstraining data. arXiv preprint arXiv:2504.12427, 2025.\n\n[83] Nikhil Kandpal, Eric Wallace, and Colin Raffel. Deduplicating training data mitigates privacyrisks in language models, 2022. URL https://arxiv.org/abs/2202.06539.\n\n[84] Nikhil Kandpal, Haikang Deng, Adam Roberts, Eric Wallace, and Colin Raffel. Largelanguage models struggle to learn long-tail knowledge. In International Conference onMachine Learning, pages 15696–15707. PMLR, 2023.\n\n[85] Pride Kavumba, Naoya Inoue, Benjamin Heinzerling, Keshav Singh, Paul Reisert, and KentaroInui. When choosing plausible alternatives, clever hans can be clever. ArXiv, abs/1911.00225,2019.\n\n[86] Tushar Khot, Ashish Sabharwal, and Peter Clark. Scitail: A textual entailment dataset fromscience question answering. In AAAI Conference on Artificial Intelligence, pages 5189–5197,2018.\n\n[87] Rodney Michael Kinney, Chloe Anastasiades, Russell Authur, Iz Beltagy, Jonathan Bragg,Alexandra Buraczynski, Isabel Cachola, Stefan Candra, Yoganand Chandrasekhar, ArmanCohan, Miles Crawford, Doug Downey, Jason Dunkelberger, Oren Etzioni, Rob Evans, SergeyFeldman, Joseph Gorney, David W. Graham, F.Q. Hu, Regan Huff, Daniel King, SebastianKohlmeier, Bailey Kuehl, Michael Langan, Daniel Lin, Haokun Liu, Kyle Lo, Jaron Lochner,Kelsey MacMillan, Tyler C. Murray, Christopher Newell, Smita R Rao, Shaurya Rohatgi,Paul Sayre, Zejiang Shen, Amanpreet Singh, Luca Soldaini, Shivashankar Subramanian,A. Tanaka, Alex D Wade, Linda M. Wagner, Lucy Lu Wang, Christopher Wilhelm, CarolineWu, Jiangjiang Yang, Angele Zamarron, Madeleine van Zuylen, and Daniel S. Weld. TheSemantic Scholar Open Data Platform. ArXiv, abs/2301.10140, 2023. URL https://api.semanticscholar.org/CorpusID:256194545.\n\n[88] Andreas Kopf, Yannic Kilcher, Dimitri von Rutte, Sotiris Anagnostidis, Zhi Rui Tam,K. Stevens, Abdullah Barhoum, Nguyen Minh Duc, Oliver Stanley, Rich’ard Nagyfi,ES Shahul, Sameer Suri, David Glushkov, Arnav Dantuluri, Andrew Maguire, ChristophSchuhmann, Huu Nguyen, and A. Mattick. Openassistant conversations - democratizing largelanguage model alignment. ArXiv, abs/2304.07327, 2023.\n\n18\n\n[89] T. Kwiatkowski, J. Palomaki, Olivia Redfield, Michael Collins, Ankur P. Parikh, Chris Alberti,D. Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones,Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc V. Le, and SlavPetrov. Natural questions: A benchmark for question answering research. Transactions of theAssociation for Computational Linguistics, 7:453–466, 2019.\n\n[90] Faisal Ladhak, Esin Durmus, Claire Cardie, and K. McKeown. Wikilingua: A new benchmarkdataset for multilingual abstractive summarization. ArXiv, abs/2010.03093, 2020.\n\n[91] Pierre-Carl Langlais. Releasing Common Corpus: the largest public domain dataset for trainingLLMs, 2024. URL https://huggingface.co/blog/Pclanglais/common-corpus.\n\n[92] LDP Headquarters for the Promotion of Digital Society and Project Team onthe Evolution and Implementation of AIs.AI White Paper 2024:New Strate-giesinStageII,Towardtheworld’smostAI-friendlycountry,April2024.URL https://aiverifyfoundation.sg/wp-content/uploads/2024/05/Model-AI-Governance-Framework-for-Generative-AI-May-2024-1-1.pdf.\n\n[93] R. Lebret, David Grangier, and Michael Auli. Neural text generation from structured datawith application to the biography domain. In Conference on Empirical Methods in NaturalLanguage Processing, pages 1203–1213, 2016.\n\n[94] Katherine Lee, Daphne Ippolito, Andrew Nystrom, Chiyuan Zhang, Douglas Eck, ChrisCallison-Burch, and Nicholas Carlini. Deduplicating training data makes language modelsbetter, 2022. URL https://arxiv.org/abs/2107.06499.\n\n[95] Katherine Lee, A. Feder Cooper, James Grimmelmann, and Daphne Ippolito. AI and Law:The Next Generation. SSRN, 2023. http://dx.doi.org/10.2139/ssrn.4580739.\n\n[96] Katherine Lee, A. Feder Cooper, and James Grimmelmann. Talkin’ ’Bout AI Generation:Copyright and the Generative-AI Supply Chain. arXiv preprint arXiv:2309.08133, 2023.\n\n[97] Jens Lehmann, Robert Isele, Max Jakob, Anja Jentzsch, D. Kontokostas, Pablo N. Mendes,Sebastian Hellmann, M. Morsey, Patrick van Kleef, S. Auer, and Christian Bizer. Dbpedia - alarge-scale, multilingual knowledge base extracted from wikipedia. Semantic Web, 6:167–195,2015.\n\n[98] H. Levesque, E. Davis, and L. Morgenstern. The winograd schema challenge. In AAAI SpringSymposium: Logical Formalizations of Commonsense Reasoning, 2011.\n\n[99] Jeffrey Li, Alex Fang, Georgios Smyrnis, Maor Ivgi, Matt Jordan, Samir Gadre, HritikBansal, Etash Guha, Sedrick Keh, Kushal Arora, Saurabh Garg, Rui Xin, Niklas Muennighoff,Reinhard Heckel, Jean Mercat, Mayee Chen, Suchin Gururangan, Mitchell Wortsman, AlonAlbalak, Yonatan Bitton, Marianna Nezhurina, Amro Abbas, Cheng-Yu Hsieh, Dhruba Ghosh,Josh Gardner, Maciej Kilian, Hanlin Zhang, Rulin Shao, Sarah Pratt, Sunny Sanyal, GabrielIlharco, Giannis Daras, Kalyani Marathe, Aaron Gokaslan, Jieyu Zhang, Khyathi Chandu,Thao Nguyen, Igor Vasiljevic, Sham Kakade, Shuran Song, Sujay Sanghavi, Fartash Faghri,Sewoong Oh, Luke Zettlemoyer, Kyle Lo, Alaaeldin El-Nouby, Hadi Pouransari, AlexanderToshev, Stephanie Wang, Dirk Groeneveld, Luca Soldaini, Pang Wei Koh, Jenia Jitsev, ThomasKollar, Alexandros G. Dimakis, Yair Carmon, Achal Dave, Ludwig Schmidt, and VaishaalShankar. Datacomp-lm: In search of the next generation of training sets for language models,2025. URL https://arxiv.org/abs/2406.11794.\n\n[100] Xin Li and D. Roth.Learning question classifiers.In International Conference onComputational Linguistics, pages 1–7, 2002.\n\n[101] Stephanie C. Lin, Jacob Hilton, and Owain Evans. Truthfulqa: Measuring how models mimichuman falsehoods. In Annual Meeting of the Association for Computational Linguistics, pages3214–3252, 2021.\n\n[102] Emmy Liu, Chenxuan Cui, Kenneth Zheng, and Graham Neubig. Testing the ability oflanguage models to interpret figurative language. ArXiv, abs/2204.12632, 2022.\n\n19\n\n[103] Zhengzhong Liu, Aurick Qiao, Willie Neiswanger, Hongyi Wang, Bowen Tan, TianhuaTao, Junbo Li, Yuqi Wang, Suqi Sun, Omkar Pangarkar, Richard Fan, Yi Gu, Victor Miller,Yonghao Zhuang, Guowei He, Haonan Li, Fajri Koto, Liping Tang, Nikhil Ranjan, ZhiqiangShen, Xuguang Ren, Roberto Iriondo, Cun Mu, Zhiting Hu, Mark Schulze, Preslav Nakov,Tim Baldwin, and Eric P. Xing. Llm360: Towards fully transparent open-source llms, 2023.URL https://arxiv.org/abs/2312.06550.\n\n[104] Kyle Lo, Lucy Lu Wang, Mark Neumann, Rodney Kinney, and Daniel Weld.S2ORC:The semantic scholar open research corpus. In Proceedings of the 58th Annual Meetingof the Association for Computational Linguistics, pages 4969–4983, Online, July 2020.Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.447. URLhttps://www.aclweb.org/anthology/2020.acl-main.447.\n\n[105] Shayne Longpre, Stella Biderman, Alon Albalak, Hailey Schoelkopf, Daniel McDuff, SayashKapoor, Kevin Klyman, Kyle Lo, Gabriel Ilharco, Nay San, et al. The responsible foundationmodel development cheatsheet: A review of tools & resources. Transactions on MachineLearning Research, 2024.\n\n[106] Shayne Longpre, Robert Mahari, Anthony Chen, Naana Obeng-Marnu, Damien Sileo, WilliamBrannon, Niklas Muennighoff, Nathan Khazam, Jad Kabbara, Kartik Perisetla, Xinyi (Alexis)Wu, Enrico Shippole, Kurt Bollacker, Tongshuang Wu, Luis Villa, Sandy Pentland, andSara Hooker. A large-scale audit of dataset licensing and attribution in AI. Nature MachineIntelligence, 6(8):975–987, August 2024. doi: 10/gt8f5p.\n\n[107] Shayne Longpre, Robert Mahari, Ariel Lee, Campbell Lund, Hamidah Oderinwale, WilliamBrannon, Nayan Saxena, Naana Obeng-Marnu, Tobin South, Cole Hunter, Kevin Klyman,Christopher Klamm, Hailey Schoelkopf, Nikhil Singh, Manuel Cherep, Ahmad Anis, An Dinh,Caroline Chitongo, Da Yin, Damien Sileo, Deividas Mataciunas, Diganta Misra, EmadAlghamdi, Enrico Shippole, Jianguo Zhang, Joanna Materzynska, Kun Qian, Kush Tiwary,Lester Miranda, Manan Dey, Minnie Liang, Mohammed Hamdy, Niklas Muennighoff,Seonghyeon Ye, Seungone Kim, Shrestha Mohanty, Vipul Gupta, Vivek Sharma, Vu MinhChien, Xuhui Zhou, Yizhi Li, Caiming Xiong, Luis Villa, Stella Biderman, Hanlin Li, DaphneIppolito, Sara Hooker, Jad Kabbara, and Sandy Pentland. Consent in crisis: The rapid declineof the AI data commons. Advances in Neural Information Processing Systems, 37, 2024.\n\n[108] Shayne Longpre, Robert Mahari, Naana Obeng-Marnu, William Brannon, Tobin South, KatyGero, Sandy Pentland, and Jad Kabbara. Data authenticity, consent, & provenance for ai areall broken: what will it take to fix them? arXiv preprint arXiv:2404.12691, 2024.\n\n[109] Shayne Longpre, Nikhil Singh, Manuel Cherep, Kushagra Tiwary, Joanna Materzynska,William Brannon, Robert Mahari, Naana Obeng-Marnu, Manan Dey, Mohammed Hamdy,et al.Bridging the data provenance gap across text, speech and video.arXiv preprintarXiv:2412.17847, 2024.\n\n[110] Shayne Longpre, Gregory Yauney, Emily Reif, Katherine Lee, Adam Roberts, Barret Zoph,Denny Zhou, Jason Wei, Kevin Robinson, David Mimno, et al. A pretrainer’s guide to trainingdata: Measuring the effects of data age, domain coverage, quality, & toxicity. In Proceedingsof the 2024 Conference of the North American Chapter of the Association for ComputationalLinguistics: Human Language Technologies (Volume 1: Long Papers), pages 3245–3276, 2024.\n\n[111] Ilya Loshchilov and Frank Hutter.Decoupled weight decay regularization.InInternationalConferenceonLearningRepresentations,2019.URLhttps://openreview.net/forum?id=Bkg6RiCqY7.\n\n[112] Annie Louis, D. Roth, and Filip Radlinski. “i’d rather just go to bed”’: Understanding indirectanswers. In Conference on Empirical Methods in Natural Language Processing, volumeabs/2010.03450, 2020.\n\n[113] Anton Lozhkov, Raymond Li, Loubna Ben Allal, Federico Cassano, Joel Lamy-Poirier,Nouamane Tazi, Ao Tang, Dmytro Pykhtar, Jiawei Liu, Yuxiang Wei, Tianyang Liu, Max Tian,Denis Kocetkov, Arthur Zucker, Younes Belkada, Zijian Wang, Qian Liu, Dmitry Abulkhanov,Indraneil Paul, Zhuang Li, Wen-Ding Li, Megan Risdal, Jia Li, Jian Zhu, Terry Yue Zhuo,\n\n20\n\nEvgenii Zheltonozhskii, Nii Osae Osae Dade, Wenhao Yu, Lucas Krauß, Naman Jain, YixuanSu, Xuanli He, Manan Dey, Edoardo Abati, Yekun Chai, Niklas Muennighoff, XiangruTang, Muhtasham Oblokulov, Christopher Akiki, Marc Marone, Chenghao Mou, MayankMishra, Alex Gu, Binyuan Hui, Tri Dao, Armel Zebaze, Olivier Dehaene, Nicolas Patry,Canwen Xu, Julian McAuley, Han Hu, Torsten Scholak, Sebastien Paquet, Jennifer Robinson,Carolyn Jane Anderson, Nicolas Chapados, Mostofa Patwary, Nima Tajbakhsh, Yacine Jernite,Carlos Muñoz Ferrandis, Lingming Zhang, Sean Hughes, Thomas Wolf, Arjun Guha, Leandrovon Werra, and Harm de Vries. Starcoder 2 and the stack v2: The next generation, 2024.\n\n[114] Robert Mahari and Shayne Longpre. Discit ergo est: Training data provenance and fair use.Robert Mahari and Shayne Longpre, Discit ergo est: Training Data Provenance And Fair Use,Dynamics of Generative AI (ed. Thibault Schrepel & Volker Stocker), Network Law Review,Winter, 2023.\n\n[115] Matt Mahoney. Large text compression benchmark, 2011.\n\n[116] Stephen Merity, Caiming Xiong, James Bradbury, and R. Socher. Pointer sentinel mixturemodels. ArXiv, abs/1609.07843, 2016.\n\n[117] Jean-BaptisteMichel,YuanKuiShen,AvivaPresserAiden,AdrianVeres,Matthew K. Gray,The Google Books Team,Joseph P. Pickett,Dale Hoiberg,Dan Clancy,Peter Norvig,Jon Orwant,Steven Pinker,Martin A. Nowak,andErez Lieberman Aiden.Quantitative analysis of culture using millions of digitizedbooks.Science, 331(6014):176–182, 2011.doi:10.1126/science.1199644.URLhttps://www.science.org/doi/abs/10.1126/science.1199644.\n\n[118] Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a suit of armor conductelectricity? a new dataset for open book question answering. arXiv preprint arXiv:1809.02789,2018.\n\n[119] Sewon Min, Suchin Gururangan, Eric Wallace, Weijia Shi, Hannaneh Hajishirzi, Noah A.Smith, and Luke Zettlemoyer. SILO language models: Isolating legal risk in a nonparametricdatastore. In The Twelfth International Conference on Learning Representations, 2024. URLhttps://openreview.net/forum?id=ruk0nyQPec.\n\n[120] Niklas Muennighoff, Alexander Rush, Boaz Barak, Teven Le Scao, Nouamane Tazi,Aleksandra Piktus, Sampo Pyysalo, Thomas Wolf, and Colin Raffel. Scaling data-constrainedlanguage models. Advances in Neural Information Processing Systems, 36, 2023.\n\n[121] Niklas Muennighoff, Alexander Rush, Boaz Barak, Teven Le Scao, Nouamane Tazi, Alek-sandra Piktus, Sampo Pyysalo, Thomas Wolf, and Colin A Raffel. Scaling data-constrainedlanguage models. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine,editors, Advances in Neural Information Processing Systems, volume 36, pages 50358–50376.Curran Associates, Inc., 2023. URL https://proceedings.neurips.cc/paper_files/paper/2023/file/9d89448b63ce1e2e8dc7af72c984c196-Paper-Conference.pdf.\n\n[122] Nikita Nangia, Clara Vania, Rasika Bhalerao, and Samuel R. Bowman. Crows-pairs: Achallenge dataset for measuring social biases in masked language models. In Conference onEmpirical Methods in Natural Language Processing, pages 1953–1967, 2020.\n\n[123] Jekaterina Novikova, Ondrej Dusek, and Verena Rieser. The e2e dataset: New challengesfor end-to-end generation. ArXiv, abs/1706.09254, 2017.\n\n[124] Tomoko Ohta, Sampo Pyysalo, Junichi Tsujii, and S. Ananiadou. Open-domain anatomicalentity mention detection. In Annual Meeting of the Association for Computational Linguistics,pages 27–36, 2012.\n\n[125] Yasumasa Onoe, Michael J.Q. Zhang, Eunsol Choi, and Greg Durrett. Creak: A dataset forcommonsense reasoning over entity knowledge. ArXiv, abs/2109.01653, 2021.\n\n[126] OpenAlex, 2025. URL https://openalex.org.\n\n21\n\n[127] Vassil Panayotov, Guoguo Chen, Daniel Povey, and S. Khudanpur. Librispeech: An asr corpusbased on public domain audio books. 2015 IEEE International Conference on Acoustics,Speech and Signal Processing (ICASSP), pages 5206–5210, 2015.\n\n[128] Ashwinee Panda, Xinyu Tang, Milad Nasr, Christopher A Choquette-Choo, and Prateek Mittal.Privacy auditing of large language models. arXiv preprint arXiv:2503.06808, 2025.\n\n[129] SungMinPark,KristianGeorgiev,AndrewIlyas,GuillaumeLeclerc,andAleksander Madry.Trak:Attributing model behavior at scale,2023.URLhttps://arxiv.org/abs/2303.14186.\n\n[130] European Parliament and Council of the European Union.Directive (eu) 2019/790,2019.URL https://eur-lex.europa.eu/legal-content/EN/ALL/?uri=CELEX:32019L0790#art_3.\n\n[131] ParlParse. Parser for uk parliament proceedings. https://parser.theyworkforyou.com/,2025. Accessed: 2025-05-09.\n\n[132] Guilherme Penedo, Hynek Kydlíˇcek, Anton Lozhkov, Margaret Mitchell, Colin Raffel,Leandro Von Werra, and Thomas Wolf. The FineWeb datasets: Decanting the web for thefinest text data at scale. Advances in Neural Information Processing Systems, 37, 2024.\n\n[133] Fabio Petroni, Tim Rocktäschel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, YuxiangWu, and Alexander Miller. Language models as knowledge bases? In Proceedings of the2019 Conference on Empirical Methods in Natural Language Processing, 2019.\n\n[134] E. Ponti, Goran Glavavs, Olga Majewska, Qianchu Liu, Ivan Vulic, and A. Korhonen. Xcopa:A multilingual dataset for causal commonsense reasoning. In Conference on EmpiricalMethods in Natural Language Processing, pages 2362–2376, 2020.\n\n[135] Christopher Potts, Zhengxuan Wu, Atticus Geiger, and Douwe Kiela. Dynasent: A dynamicbenchmark for sentiment analysis. ArXiv, abs/2012.15349, 2020.\n\n[136] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving languageunderstanding by generative pre-training, 2018.\n\n[137] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever.Language models are unsupervised multitask learners, 2019.\n\n[138] Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and IlyaSutskever.Robust speech recognition via large-scale weak supervision, 2022.URLhttps://arxiv.org/abs/2212.04356.\n\n[139] Filip Radlinski, K. Balog, B. Byrne, and K. Krishnamoorthi.Coached conversationalpreference elicitation: A case study in understanding movie preferences.In SIGDIALConferences, pages 353–360, 2019.\n\n[140] Jack W Rae, Anna Potapenko, Siddhant M Jayakumar, Chloe Hillier, and Timothy P Lillicrap.Compressive transformers for long-range sequence modelling. arXiv preprint, 2019. URLhttps://arxiv.org/abs/1911.05507.\n\n[141] Jack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song,John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, Eliza Rutherford, Tom Hen-nigan, Jacob Menick, Albin Cassirer, Richard Powell, George van den Driessche, Lisa AnneHendricks, Maribeth Rauh, Po-Sen Huang, Amelia Glaese, Johannes Welbl, SumanthDathathri, Saffron Huang, Jonathan Uesato, John F. J. Mellor, Irina Higgins, Antonia Creswell,Nat McAleese, Amy Wu, Erich Elsen, Siddhant M. Jayakumar, Elena Buchatskaya, David Bud-den, Esme Sutherland, Karen Simonyan, Michela Paganini, L. Sifre, Lena Martens, Xiang Lor-raine Li, Adhiguna Kuncoro, Aida Nematzadeh, Elena Gribovskaya, Domenic Donato, An-geliki Lazaridou, Arthur Mensch, Jean-Baptiste Lespiau, Maria Tsimpoukelli, N. K. Grigorev,Doug Fritz, Thibault Sottiaux, Mantas Pajarskas, Tobias Pohlen, Zhitao Gong, Daniel Toyama,Cyprien de Masson d’Autume, Yujia Li, Tayfun Terzi, Vladimir Mikulik, Igor Babuschkin,Aidan Clark, Diego de Las Casas, Aurelia Guy, Chris Jones, James Bradbury, Matthew G.\n\n22\n\nJohnson, Blake A. Hechtman, Laura Weidinger, Iason Gabriel, William S. Isaac, EdwardLockhart, Simon Osindero, Laura Rimell, Chris Dyer, Oriol Vinyals, Kareem W. Ayoub, JeffStanway, L. L. Bennett, Demis Hassabis, Koray Kavukcuoglu, and Geoffrey Irving. Scalinglanguage models: Methods, analysis & insights from training gopher. ArXiv, abs/2112.11446,2021. URL https://api.semanticscholar.org/CorpusID:245353475.\n\n[142] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena,Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unifiedtext-to-text transformer. Journal of machine learning research, 21(140), 2020.\n\n[143] Robi Rahman and David Owen.The size of datasets used to train language modelsdoubles approximately every seven months, 2024.URL https://epoch.ai/data-insights/dataset-size-trend. Accessed: 2025-05-08.\n\n[144] Nazneen Rajani, Bryan McCann, Caiming Xiong, and R. Socher. Explain yourself! leveraginglanguage models for commonsense reasoning. ArXiv, abs/1906.02361, 2019.\n\n[145] Inioluwa Deborah Raji, Peggy Xu, Colleen Honigsberg, and Daniel Ho. Outsider oversight:Designing a third party audit ecosystem for ai governance. In Proceedings of the 2022AAAI/ACM Conference on AI, Ethics, and Society, pages 557–571, 2022.\n\n[146] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. Squad: 100,000+questions for machine comprehension of text. In Conference on Empirical Methods in NaturalLanguage Processing, pages 2383–2392, 2016.\n\n[147] Pranav Rajpurkar, Robin Jia, and Percy Liang. Know what you don’t know: Unanswerablequestions for squad. In Annual Meeting of the Association for Computational Linguistics,volume abs/1806.03822, 2018.\n\n[148] Abhinav Rastogi, Xiaoxue Zang, Srinivas Sunkara, Raghav Gupta, and Pranav Khaitan.Schema-guided dialogue state tracking task at dstc8. ArXiv, abs/2002.01359, 2020.\n\n[149] Abhilasha Ravichander, Matt Gardner, and Ana Marasovi´c. Condaqa: A contrastive readingcomprehension dataset for reasoning about negation. ArXiv, abs/2211.00295, 2022.\n\n[150] Varshini Reddy, Craig W. Schmidt, Yuval Pinter, and Chris Tanner.How muchis enough?the diminishing returns of tokenization training data, 2025.URLhttps://arxiv.org/abs/2502.20273.\n\n[151] Hammam Riza, Michael Purwoadi, Gunarso, Teduh Uliniansyah, Aw Ai Ti, Sharifah MahaniAljunied, Luong Chi Mai, V. Thang, N. Thai, Vichet Chea, Rapid Sun, Sethserey Sam,Sopheap Seng, K. Soe, K. Nwet, M. Utiyama, and Chenchen Ding. Introduction of the asianlanguage treebank. In Oriental COCOSDA International Conference on Speech Databaseand Assessments, pages 1–6, 2016.\n\n[152] Adam Roberts, Colin Raffel, and Noam Shazeer. How much knowledge can you pack intothe parameters of a language model? In Proceedings of the 2020 Conference on EmpiricalMethods in Natural Language Processing (EMNLP), 2020.\n\n[153] Anna Rogers, Olga Kovaleva, Matthew Downey, and Anna Rumshisky. Getting closer toai complete question answering: A set of prerequisite real tasks. In AAAI Conference onArtificial Intelligence, pages 8722–8731, 2020.\n\n[154] Anna Rogers, Olga Kovaleva, and Anna Rumshisky. A primer in bertology: What we knowabout how BERT works. Transactions of the association for computational linguistics, 8, 2021.\n\n[155] Rachel Rudinger, Vered Shwartz, Jena D. Hwang, Chandra Bhagavatula, Maxwell Forbes,Ronan Le Bras, Noah A. Smith, and Yejin Choi. Thinking like a skeptic: Defeasible inferencein natural language. In Trevor Cohn, Yulan He, and Yang Liu, editors, Findings of the As-sociation for Computational Linguistics: EMNLP 2020, pages 4661–4675, Online, November2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.findings-emnlp.418.URL https://aclanthology.org/2020.findings-emnlp.418/.\n\n23\n\n[156] Matthew Sag and Peter K. Yu.The globalization of copyright exceptions for ai train-ing. Emory Law Journal, 74, 2025. doi: http://dx.doi.org/10.2139/ssrn.4976393. URLhttps://ssrn.com/abstract=4976393.\n\n[157] Swarnadeep Saha, Yixin Nie, and Mohit Bansal. Conjnli: Natural language inference overconjunctive sentences. In Conference on Empirical Methods in Natural Language Processing,pages 8240–8252, 2020.\n\n[158] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande.Communications of the ACM, 64:99 – 106, 2019.\n\n[159] Pamela Samuelson. How to Think About Remedies in the Generative AI Copyright Cases.Lawfare, February 2024.URL https://www.lawfaremedia.org/article/how-to-think-about-remedies-in-the-generative-ai-copyright-cases.\n\n[160] MaartenSap,HannahRashkin,DerekChen,RonanLeBras,andYejinChoi.Socialiqa:Commonsensereasoningaboutsocialinteractions,2019.URLhttps://arxiv.org/abs/1904.09728.\n\n[161] A. Sboev, A. Naumov, and R. Rybka. Data-driven model for emotion detection in russiantexts. In BICAAI, pages 637–642, 2020.\n\n[162] Tal Schuster, Adam Fisch, and R. Barzilay. Get your vitamin c! robust fact verificationwith contrastive evidence. In North American Chapter of the Association for ComputationalLinguistics, pages 624–643, 2021.\n\n[163] Emily Sheng and David C. Uthus. Investigating societal biases in a poetry composition system.ArXiv, abs/2011.02686, 2020.\n\n[164] Mohit Shridhar, Xingdi Yuan, Marc-Alexandre Côté, Yonatan Bisk, Adam Trischler, andMatthew J. Hausknecht. Alfworld: Aligning text and embodied environments for interactivelearning. ArXiv, abs/2010.03768, 2020.\n\n[165] Shivalika Singh, Freddie Vargus, Daniel Dsouza, B\"orje F. Karlsson, Abinaya Mahendiran,Wei-Yin Ko, Herumb Shandilya, Jay Patel, Deividas Mataciunas, Laura OMahony, MikeZhang, Ramith Hettiarachchi, Joseph Wilson, Marina Machado, Luisa Souza Moura,Dominik Krzemi’nski, Hakimeh Fadaei, Irem Ergun, Ifeoma Okoh, Aisha Alaagib, OshanMudannayake, Zaid Alyafeai, Minh Chien Vu, Sebastian Ruder, Surya Guthikonda,Emad A. Alghamdi, Sebastian Gehrmann, Niklas Muennighoff, Max Bartolo, JuliaKreutzer, A. Ustun, Marzieh Fadaee, and Sara Hooker.Aya dataset: An open-accesscollection for multilingual instruction tuning.ArXiv, abs/2402.06619, 2024.URLhttps://api.semanticscholar.org/CorpusID:267617144.\n\n[166] Luca Soldaini and Kyle Lo. peS2o (Pretraining Efficiently on S2ORC) Dataset. Technicalreport, Allen Institute for AI, 2023. ODC-By, https://github.com/allenai/pes2o.\n\n[167] Luca Soldaini, Rodney Kinney, Akshita Bhagia, Dustin Schwenk, David Atkinson, RussellAuthur, Ben Bogin, Khyathi Chandu, Jennifer Dumas, Yanai Elazar, Valentin Hofmann,Ananya Harsh Jha, Sachin Kumar, Li Lucy, Xinxi Lyu, Nathan Lambert, Ian Magnusson, JacobMorrison, Niklas Muennighoff, Aakanksha Naik, Crystal Nam, Matthew E. Peters, AbhilashaRavichander, Kyle Richardson, Zejiang Shen, Emma Strubell, Nishant Subramani, OyvindTafjord, Pete Walsh, Luke Zettlemoyer, Noah A. Smith, Hannaneh Hajishirzi, Iz Beltagy,Dirk Groeneveld, Jesse Dodge, and Kyle Lo. Dolma: an open corpus of three trillion tokensfor language model pretraining research. In Proceedings of the 62nd Annual Meeting of theAssociation for Computational Linguistics, 2024.\n\n[168] Gabriel Stanovsky, Noah A. Smith, and Luke Zettlemoyer. Evaluating gender bias in machinetranslation. ArXiv, abs/1906.00591, 2019.\n\n[169] Pedro Javier Ortiz Suárez, Benoît Sagot, and Laurent Romary. Asynchronous pipeline forprocessing huge corpora on medium to low resource infrastructures. In 7th Workshop on theChallenges in the Management of Large Corpora (CMLC-7). Leibniz-Institut für DeutscheSprache, 2019.\n\n24\n\n[170] Oyvind Tafjord, Matt Gardner, Kevin Lin, and Peter Clark. Quartz: An open-domain dataset ofqualitative relationship questions. In Conference on Empirical Methods in Natural LanguageProcessing, volume abs/1909.03553, 2019.\n\n[171] Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. Commonsenseqa:A question answering challenge targeting commonsense knowledge.arXiv preprintarXiv:1811.00937, 2018.\n\n[172] Niket Tandon, Keisuke Sakaguchi, Bhavana Dalvi, Dheeraj Rajagopal, Peter Clark, MichalGuerquin, Kyle Richardson, and E. Hovy. A dataset for tracking entities in open domainprocedural text. ArXiv, abs/2011.08092, 2020.\n\n[173] Liping Tang, Nikhil Ranjan, Omkar Pangarkar, Xuezhi Liang, Zhen Wang, Li An, BhaskarRao, Linghao Jin, Huijuan Wang, Zhoujun Cheng, Suqi Sun, Cun Mu, Victor Miller, XuezheMa, Yue Peng, Zhengzhong Liu, and Eric P. Xing. Txt360: A top-quality llm pre-trainingdataset requires the perfect blend, 2024.\n\n[174] Ishan Tarunesh, Somak Aditya, and M. Choudhury. Trusting roberta over bert: Insights fromchecklisting the natural language inference task. ArXiv, abs/2107.07229, 2021.\n\n[175] MosaicML NLP Team. Introducing mpt-7b: A new standard for open-source, commerciallyusable llms, 2023. URL www.mosaicml.com/blog/mpt-7b. Accessed: 2023-05-05.\n\n[176] Qwen Team. Qwen3, April 2025. URL https://qwenlm.github.io/blog/qwen3/.\n\n[177] James Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal. Fever: alarge-scale dataset for fact extraction and verification. ArXiv, abs/1803.05355, 2018.\n\n[178] Anvith Thudi, Evianne Rovers, Yangjun Ruan, Tristan Thrush, and Chris J. Mad-dison.Mixmin:Finding data mixtures via convex minimization, 2025.URLhttps://arxiv.org/abs/2502.10510.\n\n[179] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux,Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, AurelienRodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficientfoundation language models, 2023. URL https://arxiv.org/abs/2302.13971.\n\n[180] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei,Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Openfoundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.\n\n[181] UK Parliament.Open parliament license.https://www.parliament.uk/site-information/copyright-parliament/open-parliament-licence/,Unknown.Accessed: 2025-05-09.\n\n[182] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural informationprocessing systems, 30, 2017.\n\n[183] Mathurin Videau, Badr Youbi Idrissi, Daniel Haziza, Luca Wehrstedt, Jade Copet, OlivierTeytaud, and David Lopez-Paz. Meta Lingua: A minimal PyTorch LLM training library, 2024.URL https://github.com/facebookresearch/lingua.\n\n[184] Zhilin Wang, Yi Dong, Jiaqi Zeng, Virginia Adams, Makesh Narsimhan Sreedhar, Daniel Egert,Olivier Delalleau, Jane Polak Scowcroft, Neel Kant, Aidan Swope, and Oleksii Kuchaiev.Helpsteer: Multi-attribute helpfulness dataset for steerlm. ArXiv, abs/2311.09528, 2023.\n\n[185] Maurice Weber, Daniel Fu, Quentin Anthony, Yonatan Oren, Shane Adams, Anton Alexandrov,Xiaozhong Lyu, Huu Nguyen, Xiaozhe Yao, Virginia Adams, Ben Athiwaratkun, RahulChalamala, Kezhen Chen, Max Ryabinin, Tri Dao, Percy Liang, Christopher Ré, Irina Rish,and Ce Zhang. Redpajama: an open dataset for training large language models, 2024. URLhttps://arxiv.org/abs/2411.12372.\n\n25\n\n[186] Kellie Webster, Marta Recasens, Vera Axelrod, and Jason Baldridge. Resolving genderedambiguous pronouns with bert. ArXiv, abs/1906.01161, 2019.\n\n[187] Wei Wei, Quoc V. Le, Andrew M. Dai, and Jia Li.Airdialogue: An environment forgoal-oriented dialogue research. In Conference on Empirical Methods in Natural LanguageProcessing, pages 3844–3854, 2018.\n\n[188] Alexander Wettig, Kyle Lo, Sewon Min, Hannaneh Hajishirzi, Danqi Chen, and Luca Soldaini.Organize the web: Constructing domains enhances pre-training data curation. arXiv preprintarXiv:2502.10341, 2025.\n\n[189] Sang Michael Xie, Hieu Pham, Xuanyi Dong, Nan Du, Hanxiao Liu, Yifeng Lu, Percy S Liang,Quoc V Le, Tengyu Ma, and Adams Wei Yu. Doremi: Optimizing data mixtures speeds uplanguage model pretraining. Advances in Neural Information Processing Systems, 36, 2023.\n\n[190] Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W. Cohen, R. Salakhutdinov,and Christopher D. Manning. Hotpotqa: A dataset for diverse, explainable multi-hop questionanswering. In Conference on Empirical Methods in Natural Language Processing, pages2369–2380, 2018.\n\n[191] Cat Zakrzewski, Nitasha Tiku, and Elizabeth Dwoskin. OpenAI prepares to fight for its lifeas legal troubles mount. The Washington Post, 2024.\n\n[192] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Cana machine really finish your sentence? arXiv preprint arXiv:1905.07830, 2019.\n\n[193] Ge Zhang, Scott Qu, Jiaheng Liu, Chenchen Zhang, Chenghua Lin, Chou Leuang Yu, DannyPan, Esther Cheng, Jie Liu, Qunshu Lin, Raven Yuan, Tuney Zheng, Wei Pang, Xinrun Du,Yiming Liang, Yinghao Ma, Yizhi Li, Ziyang Ma, Bill Lin, Emmanouil Benetos, Huan Yang,Junting Zhou, Kaijing Ma, Minghao Liu, Morry Niu, Noah Wang, Quehry Que, Ruibo Liu,Sine Liu, Shawn Guo, Soren Gao, Wangchunshu Zhou, Xinyue Zhang, Yizhi Zhou, YuboWang, Yuelin Bai, Yuhan Zhang, Yuxiang Zhang, Zenith Wang, Zhenzhu Yang, Zijian Zhao,Jiajun Zhang, Wanli Ouyang, Wenhao Huang, and Wenhu Chen. MAP-Neo: Highly capableand transparent bilingual large language model series. arXiv preprint arXiv:2405.19327, 2024.\n\n[194] Hongming Zhang, Xinran Zhao, and Yangqiu Song. Winowhy: A deep diagnosis of essentialcommonsense knowledge for answering winograd schema challenge. In Annual Meeting ofthe Association for Computational Linguistics, pages 5736–5745, 2020.\n\n[195] Wenting Zhao, Xiang Ren, Jack Hessel, Claire Cardie, Yejin Choi, and Yuntian Deng. Wildchat:1m chatGPT interaction logs in the wild. In The Twelfth International Conference on LearningRepresentations, 2024. URL https://openreview.net/forum?id=Bl8u7ZRlbM.\n\n[196] Ben Zhou, Kyle Richardson, Qiang Ning, Tushar Khot, Ashish Sabharwal, and D. Roth.Temporal reasoning on implicit events from distant supervision. ArXiv, abs/2010.12753, 2020.\n\n26\n\nAppendix\n\nTable of Contents\n\nA Contributions28\n\nBDetailed Description of Sources28B.1Scientific and Scholarly Text. . . . . . . . . . . . . . . . . . . . . . . . . . .28B.2Online Discussions and Forums . . . . . . . . . . . . . . . . . . . . . . . . . .29B.3Government and Legal Texts. . . . . . . . . . . . . . . . . . . . . . . . . . .30B.4Curated Task Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .30B.5Books in the Public Domain . . . . . . . . . . . . . . . . . . . . . . . . . . . .31B.6Open Educational Resources. . . . . . . . . . . . . . . . . . . . . . . . . . .31B.7Wikis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .32B.8Source Code . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .32B.9Transcribed Audio Content. . . . . . . . . . . . . . . . . . . . . . . . . . . .33B.10 Web Text . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .33\n\nC Additional insights on licensing33C.1Why we can’t always trust automatic license detection . . . . . . . . . . . . . .34\n\nD List of Data Provenance Initiative sources34\n\nEList of News sources44\n\nFList of WikiMedia wikis44\n\nG CCCC Source Statistics44\n\nH PeS2o Source Statistics46\n\nIGrowth rates of openly licensed data47\n\nJDetails on filtering pipelines47\n\nK Details on Comma’s pre-training data mixture49\n\nLDetails on Comma’s cool-down data mixture51\n\nM Details on small-scale data ablations52\n\nN Additional Comma results52\n\nO Additional training runs53O.1 Ablations at 1T Tokens. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .53\n\n27\n\nAContributions\n\nFigure 5: Author contributions to this work. Large squares indicate a major contribution and smallsquares indicate a supporting contribution.\n\nBDetailed Description of Sources\n\nBelow, we give a more in-depth overview of the sources that make up the Common Pile, includingspecific license decisions and tools used during collection.\n\nB.1Scientific and Scholarly Text\n\nScientific and scholarly texts are a staple of modern LLM pretraining corpora, appearing in nearly alllarge-scale datasets [e.g. 57, 185, 167] since they expose models to technical terminology, formalreasoning, and long-range document structure—skills that are essential for downstream tasks inscience, education, and question answering. Thanks to open access mandates and academic culturalnorms, many scholarly texts are either in the public domain or are distributed under open licenses.\n\npeS2oTo ensure broad coverage across many scientific disciplines, we include a version ofpeS2o [166] restricted to openly licensed articles. pes2o is derived from S2ORC [104], a cor-pus of openly licensed abstract and full-text papers that have been converted to a structured formatusing Grobid [63]. Starting from Grobid’s XML output, peS2o filters papers that are too short, haveincorrect metadata, are in languages other than English, and contain OCR errors using a combinationof heuristic- and model-based filtering steps. We refer the reader to the datasheet and code for moredetails on this processing pipeline. The subset of peS2o included in the Common Pile starts fromv3 of the corpus, which contains documents from January 1, 1970 to October 6, 2024. We retainfull-text papers with CC BY, CC BY-SA, or CC0 licenses, or that have been labeled as public domain;metadata is provided by the Semantic Scholar APIs [87]. After filtering, this set contains 6.3 million\n\n28\n\npapers, or 35.7 billion whitespace-separated segments. We provide more details on the compositionof this subset in Appendix H.\n\nPubMedPubMed Central (PMC) is an open-access archive of biomedical and life sciences researchpapers maintained by the U.S. National Institutes of Health’s National Library of Medicine. Wecollected papers from PMC whose metadata indicated that the publishing journal had designated aCC BY, CC BY-SA, or CC0 license. PMC stores the text content of each article as a single XML file,which we convert to markdown using pandoc.\n\nArXiv PapersArXiv is an online open-access repository of over 2.4 million scholarly paperscovering fields such as computer science, mathematics, physics, quantitative biology, economics,and more. When uploading papers, authors can choose from a variety of licenses. We includedtext from all papers uploaded under CC BY, CC BY-SA, and CC0 licenses in the Common Pilethrough a three-step pipeline: first, the latex source files for openly licensed papers were downloadedfrom ArXiv’s bulk-access S3 bucket; next, the LATEXML conversion tool was used to convert thesesource files into a single HTML document; finally, the HTML was converted to plaintext using theTrafilatura [10] HTML-processing library.\n\nArXiv AbstractsEach paper uploaded to ArXiv includes structured metadata fields, including anabstract summarizing the paper’s findings and contributions. According to ArXiv’s licensing policy,the metadata for any paper submitted to ArXiv is distributed under the CC0 license, regardless ofthe license of the paper itself. Thus, we include as an additional source the abstracts for every papersubmitted to ArXiv. We source the abstracts from ArXiv’s API via the Open Archives InitiativeProtocol for Metadata Harvesting endpoint and reproduce them as-is.\n\nB.2Online Discussions and Forums\n\nOnline forums are a rich source of multi-turn, user-generated dialogue covering a wide range oftopics. These platforms often feature question–answer pairs, problem-solving discussions, andinformal explanations of technical and non-technical concepts. The Common Pile incorporates onlinediscussions from sources that distribute content under an open license.\n\nStackExchangeWhile StackExchange formerly provided structured XML dumps of all of theircontent, since July of 2024, StackExchange has stopped publishing dumps to the Internet Archive.Instead, each site can provide a logged-in user with a custom URL to download the dump for thatsite. This means that dumps for defunct sites like windowsphone.stackexchange.com are inaccessible.Additionally, in dumps produced by the new export tool, many questions that are available in pastdumps (and accessible on the site) are not present. We therefore extract all questions and answersfrom community uploaded dumps from December of 2024 from the Internet Archive and additionallyextract missing questions and answers from the last official dumps in July of 2024 to account for thedeficiencies listed above. We use a question, its comments, its answers and the comments on eachanswer as a single document. Following the display order on StackExchange, answers are ordered bythe number of votes they received, with the exception that the “accepted answer” always appears first.PyMarkdown was used to convert each comment into plain text.\n\nGitHub ArchiveAccording to GitHub’s terms of service, issues and pull request descriptions—along with their comments—inherit the license of their associated repository. To collect this data,we used the GitHub Archive’s public BigQuery table of events to extracted all issue, pull request,and comment events since 2011 and aggregated them into threads. The table does not include “edit”events so the text from each comment is the original from when it was first posted. We filteredout comments from bots. This resulted in approximately 177 million threads across 19 millionrepositories. We then removed threads whose repositories did not have a Blue Oak Council-approvedlicense. License information for each repository comes from either 1) the “public-data:github_repos”BigQuery Table, 2) metadata from the StackV2, or 3) the GitHub API. License filtering left 10 millionrepositories. PyMarkdown was used to convert from GitHub-flavored markdown to plain text. Whenparsing failed, the raw markdown was kept.\n\nUbuntu IRCLogs of all discussions on the Ubuntu-hosted Internet Relay Chat (IRC) since 2004have been archived and released into the Public Domain. We downloaded all chats from all channelsup until March of 2025. We consider all messages for given channel on a given day as a singledocument. We removed system messages as well as those from known bots.\n\n29\n\nB.3Government and Legal Texts\n\nGovernments produce a vast amount of informational text, ranging from legislation and legal opinionsto scientific reports, public communications, and regulatory notices. This content is explicitly intendedto inform the public, and as such, in many jurisdictions it is published directly into the public domainor under open licenses. In the United States, for example, works authored by federal employees aspart of their official duties are not subject to copyright. Government and legal texts offer languagemodels exposure to formal argumentation, legal reasoning, and procedural language.\n\nUS Government Publishing OfficeThe United States Government Publishing Office (USGPO) isa federal agency responsible for disseminating official documents authored by the U.S. government.The Common Pile v0.1 includes all plain-text documents made available through the USGPO’sGovInfo.gov developer API. This collection comprises over 2.7 million documents, spanning issuesof the Federal Register, congressional hearing transcripts, budget reports, economic indicators, andother federal publications.\n\nUS Patents and Trademark OfficeIn the US, patent documents are released into the publicdomain as government works. Patents follow a highly standardized format with distinct requiredsections for background, detailed description, and claims. We include parents from the US Patentsand Trademark Office (USPTO) as provided by the Google Patents Public Data dataset [77], whichincludes millions of granted patents and published patent applications dating back to 1782. Weprocessed these documents to extract clean text while preserving this structured format. Mathematicalexpressions and equations were converted into LATEX.\n\nCaselaw Access Project and Court ListenerThe Common Pile contains 6.7 million cases fromthe Caselaw Access Project and Court Listener. The Caselaw Access Project consists of nearly 40million pages of U.S. federal and state court decisions and judges’ opinions from the last 365 years.In addition, Court Listener adds over 900 thousand cases scraped from 479 courts. The CaselawAccess Project and Court Listener source legal data from a wide variety of resources such as theHarvard Law Library, the Law Library of Congress, and the Supreme Court Database. From thesesources, we only included documents that were in the public domain. Erroneous OCR errors werefurther corrected after digitization, and additional post-processing was done to fix formatting andparsing.\n\nUK HansardHansard represents the official record of parliamentary proceedings across theUnited Kingdom’s legislative bodies. The Common Pile incorporates records from multiple sources,including debates and written answers from the UK Commons and Lords, devolved legislatures(Scottish Parliament, Senedd in both English and Welsh, Northern Ireland Assembly), LondonMayor’s Questions, and ministerial statements. Data was sourced from ParlParse [131], coveringCommons debates from 1918 forward and Lords proceedings from the 1999 reform. Each documentwas processed to preserve complete parliamentary sessions as cohesive units, maintaining the naturalflow of debate. All content is published under the Open Parliament License [181].\n\nRegulations.govRegulations.gov is an online platform operated by the U.S. General ServicesAdministration that collates newly proposed rules and regulations from federal agencies along withcomments and feedback from the general public. The Common Pile includes all plain-text regulatorydocuments published by U.S. federal agencies on this platform, acquired via the bulk downloadinterface provided by Regulations.gov.\n\nB.4Curated Task Data\n\nCurated datasets that cover specific tasks such as question answering, summarization, or text classifi-cation are often released via open licenses to the research community. While not traditionally partof pretraining corpora, including a small amount of task-oriented data during pretraining can helpmodels acquire early familiarity with task formats and prompt–completion structures.\n\nData Provenance InitiativeThe Data Provenance Initiative is a digital library of superviseddatasets that have been manually annotated with their source and license information [106, 109].We leverage their tooling to filter HuggingFace datasets, based on a range of criteria, includingtheir licenses, which may be particularly relevant for supervised datasets [114]. Specifically, wefilter the data according to these criteria: contains English language or code data, the text is notmodel-generated, the dataset’s audit yielded a open license and the original sources of the data areonly from recognized public domain sources.\n\n30\n\nB.5Books in the Public Domain\n\nBooks represent a time-tested resource for language model pretraining, offering carefully edited,long-form prose that supports learning of narrative coherence and long-range dependency modeling.For these reasons, many large-scale pretraining corpora—including the Pile [57], Dolma [167],and RedPajama [185]—include content from books [41]. In the United States, as of 2024, bookspublished prior to 1929 are in the public domain. Thus, the Common Pile includes public domainbooks drawn from curated collections, covering topics such as literature, science, and history.\n\nBiodiversity Heritage LibraryThe Biodiversity Heritage Library (BHL) is an open-access digitallibrary for biodiversity literature and archives. The Common Pile contains over 42 million publicdomain books and documents from the BHL collection. These works were collected using the bulkdata download interface provided by the BHL and were filtered based on their associated licensemetadata. We use the optical character recognition (OCR)-generated text distributed by BHL.\n\nPre-1929 BooksBooks published in the US before 1929 passed into the public domain on January1, 2024. We used the bibliographic catalog Hathifiles produced by HathiTrust to identify digitizedbooks which were published in the US before 1929. The collection contains over 130,000 booksdigitized and processed by the Internet Archive on behalf of HathiTrust member libraries. The OCRplain text files were downloaded directly from the Internet Archive website.\n\nLibrary of CongressThe Library of Congress (LoC) curates a collection of public domain bookscalled “Selected Digitized Books”. We downloaded over 130,000 English-language books from thispublic domain collection as OCR plain text files using the LoC APIs.\n\nProject GutenbergProject Gutenberg is an online collection of over 75,000 digitized booksavailable as plain text. We use all books that are 1) English and 2) marked as in the Public Domainaccording to the provided metadata. Additionally, we include any books that are part of the pg19 [140]dataset, which only includes books that are over 100 years old. Minimal preprocessing is appliedto remove the Project Gutenberg header and footers, and many scanned books include preambleinformation about who digitized them.\n\nB.6Open Educational Resources\n\nOpen Educational Resources (OERs) are educational materials, typically published under openlicenses, to support free and equitable access to education. These resources include educationalartifacts such as textbooks, lecture notes, lesson plans, syllabi, and problem sets. For languagemodels, OERs offer exposure to instructional formatting and domain-specific information, makingthem valuable for improving performance on knowledge-based downstream tasks. The Common Pileincludes a range of such materials sourced from major OER repositories, including collections ofopen-access books and structured teaching resources.\n\nDirectory of Open Access BooksThe Directory of Open Access Books (DOAB) is an onlineindex of over 94,000 peer-reviewed books curated from trusted open-access publishers. To collectthe openly licensed content from DOAB, we retrieve metadata using their official metadata feed.We then filter the collection to include only English-language books released under CC BY and CCBY-SA licenses. The filtered books are downloaded in PDF format and converted to plaintext usingthe Marker PDF-to-text converter. As an additional validation step, we manually create a whitelist ofopen license statements and retain only texts explicitly containing one of these statements in theirfront- or back-matter.\n\nPressBooksPressBooks is a searchable catalog of over 8,000 open access books. To collect openlylicensed content from PressBooks we construct a search query to retrieve URLs for all books writtenin English and listed as public domain or under CC BY or CC BY-SA licenses. For each matchedbook, we collect its contents directly from the publicly available web version provided by PressBooks.\n\nOERCommonsOERCommons is an online platform where educators share open-access instruc-tional materials—such as textbooks, lesson plans, problem sets, course syllabi, and worksheets—withthe goal of expanding access to affordable education. To collect the openly licensed content availableon OERCommons, we construct a search query to retrieve English-language content released into thepublic domain or under CC BY or CC BY-SA licenses. The resulting documents are converted toplain text directly from the HTML pages hosted on the OERCommons website.\n\n31\n\nLibreTextsLibreTexts is an online platform that provides a catalog of over 3,000 open-accesstextbooks. To collect openly licensed content from LibreTexts we gather links to all textbooks inthe catalog and check each textbook section for a license statement indicating that it is in the publicdomain or under a CC BY, CC BY-SA, or the GNU Free Documentation License. We extract plaintext from these textbook sections directly from the HTML pages hosted on the LibreTexts website.\n\nB.7Wikis\n\nWikis are collaboratively maintained websites that organize information around specific topics ordomains. Their crowd-sourced nature, coupled with community moderation and citation requirements,often results in text that is both informative and well-structured. Prominent examples such asWikipedia have become staples in large-scale language model pretraining corpora due to their breadthof coverage and high quality. In addition, most major wikis are distributed under open licenses suchas CC BY and CC BY-SA. The Common Pile includes content from a range of openly licensed wikisto provide models with structured and well-researched informational text.\n\nWikimediaWe downloaded the official database dumps from March 2025 of the English-languagewikis that are directly managed by the Wikimedia foundation (see Appendix F for a complete list).These database dumps include the wikitext—Mediawiki’s custom markup language—for each page aswell as talk pages, where editors discuss changes made to a page. We only use the most recent versionof each page. We converted wikitext to plain text using wtf_wikipedia after light adjustments informatting to avoid errors in section ordering caused by a bug. Before parsing, we converted wikitextmath into LATEX math using our custom code. Finally, any remaining HTML tags were removed viaregexes.\n\nWikiteamThere are many wikis on the internet that are not managed by the Wikimedia foundation,but do use their MediaWiki software to power their wiki. Many of these wikis have been archivedby wikiteam, a collection of volunteers that create unofficial database dumps of wikis and uploadthem to the Internet Archive. We download all dumps made by wikiteam when the metadata indicatesthe wiki was licensed under CC BY, CC BY-SA, or released into the public domain on the InternetArchive in September of 2024. This results in downloading approximately 330,000 wikis. Whenmultiple dumps of the same wiki exists, we use the most recent dump. The wikitext was convertedto plain text following the same steps as with Wikimedia wikis. After preprocessing, we removeddocuments from wikis that appeared to contain large amounts of license laundering, e.g. those thatwere collections of song lyrics or transcripts.\n\nB.8Source Code\n\nSource code has become an increasingly important component of large-scale language model pretrain-ing corpora, as it enables models to learn syntax, program structure, and problem solving strategiesuseful for both code generation and reasoning tasks. Thanks to the Free and Open Source Software(FOSS) movement, code also happens to be one of the most openly licensed forms of text, withmany software repositories distributed under open licenses such as MIT, BSD, Apache 2.0, and theGNU Free Documentation License (GFDL). The Common Pile includes high-quality, openly licensedsource code from large-scale public code datasets and documentation standards, enabling modelstrained on it to perform better on coding and technical writing tasks.\n\nThe Stack V2The Stack V2 [113] consists of a mixture of openly licensed and unlicensed work.We use the tooling that the Software Heritage Foundation and BigCode created to build our dataset.In particular, we relied on the license detection performed by the creators of Stack V2. When multiplelicenses are detected in a single repository, we make sure that all of them meet our definition of“openly licensed”.\n\nPython Enhancement ProposalsPython Enhancement Proposals, or PEPs, are design documentsthat generally provide a technical specification and rationale for new features of the Python program-ming language. There are been 661 PEPs published. The majority of PEPs are published in the PublicDomain, but 5 were published under the “Open Publication License” and omitted. PEPs are long,highly-polished, and technical in nature and often include code examples paired with their prose.PEPs are authored in ReStructured Text; we used pandoc, version 3.5, to convert them to plain text.\n\n32\n\nB.9Transcribed Audio Content\n\nA historically underutilized source of text data is speech transcribed from audio and video content.Spoken language in educational videos, speeches, and interviews provide an opportunity for modelsto learn conversational speech patterns.\n\nCreative Commons YouTubeYouTube is large-scale video-sharing platform where users have theoption of uploading content under a CC BY license. To collect high-quality speech-based textualcontent and combat the rampant license laundering on YouTube, we manually curated a set of over2,000 YouTube channels that consistently release original openly licensed content containing speech.The resulting collection spans a wide range of genres, including lectures, tutorials, reviews, videoessays, speeches, and vlogs. From these channels, we retrieved over 1.1 million openly licensedvideos comprising more than 470,000 hours of content. Finally, each video was transcribed to textusing the Whisper speech recognition model [138].\n\nB.10Web Text\n\nThe success of modern LLM pre-training relies on text scraped indiscriminately from the web, asweb text covers an extremely diverse range of textual domains. In the Common Pile, we restrict thisapproach to only include web content with clear public domain status or open license statements.\n\nCreative Commons Common CrawlWe sourced text from 52 Common Crawl snapshots, coveringabout half of Common Crawl snapshots available to date and covering all years of operations ofCommon Crawl up to 2024. We found a higher level of duplication across this collection, suggestingthat including more snapshots would lead to a modest increase in total token yield. From thesesnapshots, we extract HTML content using FastWarc [15]. Then, using a regular expression adaptedfrom the C4Corpus project [68], we retain only those pages where a CC BY, CC BY-SA, or CC0license appears. To ensure license accuracy, we manually verified the top 1000 domains by contentvolume, retaining only the 537 domains with confirmed licenses where the Creative Commonsdesignation is applied to all text content rather than only embedded media or a subset of the texton the domain. We extract the main content of these documents and remove boilerplate usingResiliparse [14]. We perform URL-level exact deduplication and use Bloom filters to remove near-duplicates with 80% ngram overlap. We also employ rule-based filters matching Dolma [167];namely, we use C4-derived heuristics [142] to filter pages containing Javascript, Lorem Ipsum, andcurly braces {}. We also apply all Gopher rules [141] to remove low-quality pages. We provide moredetails on the composition of this subset in Appendix G.\n\nFoodistaFoodista is a community-maintained site with recipes, food-related news, and nutritioninformation. All content is licensed under CC BY. Plain text is extracted from the HTML using acustom pipeline that includes extracting title and author information to include at the beginning ofthe text. Additionally, comments on the page are appended to the article after we filter automaticallygenerated comments.\n\nNewsWe scrape the news sites that publish content under CC BY or CC BY-SA according toopennewswire. A full list of sites can be found in Appendix E. Plain text was extracted fromthe HTML using our custom pipeline, including extraction of the title and byline to include at thebeginning of each article.\n\nPublic Domain ReviewThe Public Domain Review is an online journal dedicated to explorationof works of art and literature that have aged into the public domain. We collect all articles publishedin the Public Domain Review under a CC BY-SA license.\n\nCAdditional insights on licensing\n\nThere are many standards we could have chosen for what licenses to include in our dataset. The opensource, knowledge, and culture movements have harmonized on the high level principles described insection 1: “open” means that permission is granted for content to be freely used, studied, modified,and shared for any purpose. This language is found in the Open Knowledge Definition we followas well as the Open Source Institute’s Open Definition, Creative Commons’s statement on OpenCulture, Wikimedia’s Acceptable licenses policy and more. Our work was also developed to beconsistent with the Open movement’s work in the specific context of AI technologies such as the\n\n33\n\nOpen Source Initiative’s Open Source AI Definition and in consultations with leading members ofthe community [9].\n\nC.1Why we can’t always trust automatic license detection\n\nThere are many reasons why identifying the licensing status of internet text with automatic toolingcan be challenging. In this section, we briefly discuss some major themes from our experience.\n\nThere are many ways to say the same thing.While there exist standards for how to express alicense, people don’t always follow those standards and failure to follow the standards doesn’t meanthat the license is invalid. For example, simple string matching on “CC BY” misses a huge amount ofCC BY licensed text because a very common way to denote Creative Commons licenses is using animage badge. Current web-processing tools are substantially stronger at identifying text than images,and the failure rate on sites using image badges is quite high.\n\nLack of understanding of licenses.Most people are not lawyers and do not understand the fulllegal scope and meaning of the licenses that they attempt to put on their text. Developers routinelytweak boilerplate to produce ambiguous language like (“Licensed under MIT-ish terms”) or writecontradictory statements (“All rights reserved / CC-BY”). In general, it is common for people to writequasi-legal language along side a more traditional license. Non-standard licenses require substantialamounts of work to interpret and are not always valid or meaningful.\n\nLicensing signals can be noisy.Even when a developer intends to clearly communicate a specificlicense, contradictions and errors can occur in practice. For example, Longpre et al. [107] found thatthere were substantial disagreements between the terms of service of a website and the restrictionsfound in a robots.txt file. We have not yet found a reliable way to have an automatic system identifylicensed text and therefore frequently resort to manual review by humans.\n\nDList of Data Provenance Initiative sources\n\nThe openly licensed supervised datasets included in the Common Pile are listed in Table 1. Thesedatasets were identified and collected using metadata from the Data Provenance Initiative. For moreinformation on these datasets, consult the Data Provenance Initiative Dataset Explorer.\n\nTable 1: Supervised datasets included in the Common Pile from the Data Provenance Initiativecollection.\n\nCollectionDataset IdentifierLicenses\n\nAgentInstructAgentInstruct-alfworld[164]MIT License\n\nHelpSteerHelpSteer[184]CC BY 4.0\n\nAya Datasetaya-english[165]Apache License 2.0\n\nCommitPackFTcommitpackft-abap[165]MIT License\n\nCommitPackFTcommitpackft-agda[165]MIT License, BSD 3-Clause License\n\nCommitPackFTcommitpackft-apl[165]MIT License, ISC License\n\nCommitPackFTcommitpackft-arc[165]MIT License\n\nCommitPackFTcommitpackft-aspectj[165]Apache License 2.0, BSD 3-ClauseLicense, MIT License\n\nCommitPackFTcommitpackft-ats[165]Apache License 2.0, MIT License\n\nCommitPackFTcommitpackft-blitzmax[165]MIT License\n\nCommitPackFTcommitpackft-bluespec[165]MIT License\n\nCommitPackFTcommitpackft-boo[165]MIT License\n\nContinued on next page\n\n34\n\nCollectionDataset IdentifierLicenses\n\nCommitPackFTcommitpackft-brainfuck[165]Apache License 2.0, BSD 2-ClauseLicense, MIT License\n\nCommitPackFTcommitpackft-bro[165]MIT License, BSD 3-Clause License\n\nCommitPackFTcommitpackft-cartocss[165]MIT License\n\nCommitPackFTcommitpackft-chapel[165]Apache License 2.0, BSD 3-ClauseLicense, MIT License\n\nCommitPackFTcommitpackft-clean[165]Apache License 2.0, MIT License\n\nCommitPackFTcommitpackft-coldfusion[165]Apache License 2.0, MIT License\n\nCommitPackFTcommitpackft-creole[165]Apache License 2.0, MIT License\n\nCommitPackFTcommitpackft-crystal[165]Apache License 2.0, MIT License\n\nCommitPackFTcommitpackft-dns-zone[165]MIT License, BSD 3-Clause License\n\nCommitPackFTcommitpackft-dylan[165]MIT License\n\nCommitPackFTcommitpackft-eiffel[165]MIT License\n\nCommitPackFTcommitpackft-emberscript[165]Apache License 2.0, BSD 3-ClauseLicense, MIT License\n\nCommitPackFTcommitpackft-fancy[165]MIT License, BSD 3-Clause License\n\nCommitPackFTcommitpackft-flux[165]Apache License 2.0, MIT License\n\nCommitPackFTcommitpackft-forth[165]MIT License\n\nCommitPackFTcommitpackft-g-code[165]Apache License 2.0, BSD 3-ClauseLicense, MIT License\n\nCommitPackFTcommitpackft-gdscript[165]Apache License 2.0, CC0 1.0, MITLicense\n\nCommitPackFTcommitpackft-genshi[165]Apache License 2.0, MIT License\n\nCommitPackFTcommitpackft-graphql[165]Apache License 2.0, BSD 3-ClauseLicense, CC0 1.0, MIT License\n\nCommitPackFTcommitpackft-harbour[165]MIT License\n\nCommitPackFTcommitpackft-hlsl[165]Apache License 2.0, MIT License\n\nCommitPackFTcommitpackft-http[165]Apache License 2.0, MIT License\n\nCommitPackFTcommitpackft-idris[165]MIT License, BSD 3-ClauseLicense, BSD 2-Clause License\n\nCommitPackFTcommitpackft-igor-pro[165]MIT License, BSD 3-Clause License\n\nCommitPackFTcommitpackft-inform-7[165]MIT License, BSD 3-Clause License\n\nCommitPackFTcommitpackft-ioke[165]MIT License\n\nCommitPackFTcommitpackft-isabelle[165]MIT License, BSD 2-Clause License\n\nCommitPackFTcommitpackft-jflex[165]MIT License\n\nCommitPackFTcommitpackft-json5[165]MIT License, BSD 3-ClauseLicense, BSD 2-Clause License\n\nCommitPackFTcommitpackft-jsonld[165]Apache License 2.0, BSD 3-ClauseLicense, CC0 1.0, MIT License\n\nCommitPackFTcommitpackft-krl[165]MIT License\n\nCommitPackFTcommitpackft-latte[165]MIT License\n\nCommitPackFTcommitpackft-lean[165]Apache License 2.0, MIT License\n\nCommitPackFTcommitpackft-lfe[165]Apache License 2.0, MIT License\n\nContinued on next page\n\n35\n\nCollectionDataset IdentifierLicenses\n\nCommitPackFTcommitpackft-lilypond[165]MIT License\n\nCommitPackFTcommitpackft-liquid[165]Apache License 2.0, CC0 1.0, MITLicense\n\nCommitPackFTcommitpackft-literate-agda[165]MIT License\n\nCommitPackFTcommitpackft-literate-coffeescript[165]MIT License\n\nCommitPackFTcommitpackft-literate-haskell[165]MIT License, BSD 3-Clause License\n\nCommitPackFTcommitpackft-llvm[165]Apache License 2.0, BSD 3-ClauseLicense, BSD 2-Clause License,MIT License\n\nCommitPackFTcommitpackft-logos[165]Apache License 2.0, BSD 3-ClauseLicense, BSD 2-Clause License,MIT License, ISC License\n\nCommitPackFTcommitpackft-lsl[165]MIT License, BSD 3-Clause License\n\nCommitPackFTcommitpackft-maple[165]MIT License, BSD 3-Clause License\n\nCommitPackFTcommitpackft-mathematica[165]MIT License, CC0 1.0\n\nCommitPackFTcommitpackft-metal[165]Apache License 2.0, MIT License\n\nCommitPackFTcommitpackft-mirah[165]Apache License 2.0, MIT License\n\nCommitPackFTcommitpackft-monkey[165]Apache License 2.0, MIT License\n\nCommitPackFTcommitpackft-moonscript[165]MIT License\n\nCommitPackFTcommitpackft-mtml[165]MIT License\n\nCommitPackFTcommitpackft-mupad[165]Apache License 2.0, BSD 3-ClauseLicense, MIT License\n\nCommitPackFTcommitpackft-nesc[165]MIT License\n\nCommitPackFTcommitpackft-netlinx[165]MIT License\n\nCommitPackFTcommitpackft-ninja[165]Apache License 2.0, BSD 3-ClauseLicense, MIT License\n\nCommitPackFTcommitpackft-nit[165]Apache License 2.0, MIT License\n\nCommitPackFTcommitpackft-nu[165]Apache License 2.0, MIT License\n\nCommitPackFTcommitpackft-ooc[165]MIT License\n\nCommitPackFTcommitpackft-openscad[165]MIT License, CC0 1.0, BSD2-Clause License\n\nCommitPackFTcommitpackft-oz[165]MIT License, BSD 2-Clause License\n\nCommitPackFTcommitpackft-pan[165]Apache License 2.0, MIT License\n\nCommitPackFTcommitpackft-piglatin[165]Apache License 2.0, MIT License\n\nCommitPackFTcommitpackft-pony[165]MIT License, BSD 2-Clause License\n\nCommitPackFTcommitpackft-propeller-spin[165]MIT License\n\nCommitPackFTcommitpackft-pure-data[165]MIT License\n\nCommitPackFTcommitpackft-purebasic[165]MIT License, BSD 3-Clause License\n\nCommitPackFTcommitpackft-purescript[165]Apache License 2.0, BSD 3-ClauseLicense, MIT License\n\nCommitPackFTcommitpackft-ragel-in-ruby-host[165]MIT License\n\nCommitPackFTcommitpackft-rebol[165]Apache License 2.0, MIT License\n\nContinued on next page\n\n36\n\nCollectionDataset IdentifierLicenses\n\nCommitPackFTcommitpackft-red[165]MIT License, BSD 2-Clause License\n\nCommitPackFTcommitpackft-rouge[165]Apache License 2.0, MIT License\n\nCommitPackFTcommitpackft-sage[165]MIT License\n\nCommitPackFTcommitpackft-sas[165]MIT License\n\nCommitPackFTcommitpackft-scaml[165]MIT License, BSD 2-Clause License\n\nCommitPackFTcommitpackft-scilab[165]MIT License, BSD 3-Clause License\n\nCommitPackFTcommitpackft-slash[165]Apache License 2.0, MIT License\n\nCommitPackFTcommitpackft-smt[165]MIT License, BSD 3-Clause License\n\nCommitPackFTcommitpackft-solidity[165]Apache License 2.0, MIT License\n\nCommitPackFTcommitpackft-sourcepawn[165]Apache License 2.0, MIT License\n\nCommitPackFTcommitpackft-squirrel[165]MIT License\n\nCommitPackFTcommitpackft-ston[165]MIT License\n\nCommitPackFTcommitpackft-systemverilog[165]Apache License 2.0, BSD 3-ClauseLicense, MIT License\n\nCommitPackFTcommitpackft-unity3d-asset[165]Apache License 2.0, BSD 3-ClauseLicense, BSD 2-Clause License,MIT License, ISC License, CC0 1.0\n\nCommitPackFTcommitpackft-uno[165]MIT License\n\nCommitPackFTcommitpackft-unrealscript[165]MIT License\n\nCommitPackFTcommitpackft-urweb[165]MIT License, BSD 3-Clause License\n\nCommitPackFTcommitpackft-vcl[165]Apache License 2.0, BSD 3-ClauseLicense, MIT License\n\nCommitPackFTcommitpackft-xbase[165]Apache License 2.0, MIT License\n\nCommitPackFTcommitpackft-xpages[165]Apache License 2.0, MIT License\n\nCommitPackFTcommitpackft-xproc[165]Apache License 2.0, MIT License\n\nCommitPackFTcommitpackft-yacc[165]MIT License, ISC License, BSD2-Clause License\n\nCommitPackFTcommitpackft-zephir[165]MIT License\n\nCommitPackFTcommitpackft-zig[165]MIT License\n\nDolly 15kdolly-brainstorming[165]CC BY-SA 3.0\n\nDolly 15kdolly-classification[165]CC BY-SA 3.0\n\nDolly 15kdolly-closedqa[165]CC BY-SA 3.0\n\nDolly 15kdolly-creative_writing[165]CC BY-SA 3.0\n\nDolly 15kdolly-infoextract[165]CC BY-SA 3.0\n\nDolly 15kdolly-openqa[165]CC BY-SA 3.0\n\nDolly 15kdolly-summarization[165]CC BY-SA 3.0\n\nDialogStudiods-ABCD[165]Apache License 2.0, MIT License\n\nDialogStudiods-ATIS[165]Apache License 2.0, CC BY 4.0\n\nDialogStudiods-ATIS-NER[165]Apache License 2.0, CC BY 4.0\n\nDialogStudiods-AirDialogue[165]Apache License 2.0\n\nDialogStudiods-AntiScam[165]Apache License 2.0, CC0 1.0\n\nDialogStudiods-BANKING77[165]Apache License 2.0, CC BY 4.0\n\nContinued on next page\n\n37\n\nCollectionDataset IdentifierLicenses\n\nDialogStudiods-BANKING77-OOS[165]Apache License 2.0, CC BY 4.0\n\nDialogStudiods-BiTOD[165]Apache License 2.0\n\nDialogStudiods-CLINC-Single-Domain-OOS-banking[165]Apache License 2.0, CC BY 3.0\n\nDialogStudiods-CLINC-Single-Domain-OOS-credit_cards[165]Apache License 2.0, CC BY 3.0\n\nDialogStudiods-CLINC150[165]Apache License 2.0, CC BY-SA 3.0\n\nDialogStudiods-CaSiNo[165]Apache License 2.0, CC BY 4.0\n\nDialogStudiods-CoQA[165]Apache License 2.0, MIT License\n\nDialogStudiods-CoSQL[165]Apache License 2.0, CC BY-SA 4.0\n\nDialogStudiods-ConvAI2[165]Apache License 2.0\n\nDialogStudiods-CraigslistBargains[165]Apache License 2.0, MIT License\n\nDialogStudiods-DART[165]Apache License 2.0, MIT License\n\nDialogStudiods-DSTC8-SGD[165]Apache License 2.0, CC BY-SA 4.0\n\nDialogStudiods-DialogSum[165]Apache License 2.0, MIT License\n\nDialogStudiods-Disambiguation[165]Apache License 2.0, MIT License\n\nDialogStudiods-FeTaQA[165]Apache License 2.0, CC BY-SA 4.0\n\nDialogStudiods-GECOR[165]Apache License 2.0, CC BY 4.0\n\nDialogStudiods-GrailQA[165]Apache License 2.0\n\nDialogStudiods-HDSA-Dialog[165]Apache License 2.0, MIT License\n\nDialogStudiods-HH-RLHF[165]Apache License 2.0, MIT License\n\nDialogStudiods-HWU64[165]Apache License 2.0, CC BY-SA 3.0\n\nDialogStudiods-HybridQA[165]Apache License 2.0, MIT License\n\nDialogStudiods-KETOD[165]Apache License 2.0, MIT License\n\nDialogStudiods-MTOP[165]Apache License 2.0, CC BY-SA 4.0\n\nDialogStudiods-MULTIWOZ2_2[165]Apache License 2.0, MIT License\n\nDialogStudiods-MulDoGO[165]Apache License 2.0, CDLAPermissive 1.0\n\nDialogStudiods-MultiWOZ_2.1[165]Apache License 2.0, MIT License\n\nDialogStudiods-Prosocial[165]Apache License 2.0, MIT License\n\nDialogStudiods-RESTAURANTS8K[165]Apache License 2.0, CC BY 4.0\n\nDialogStudiods-SGD[165]Apache License 2.0, CC BY-SA 4.0\n\nDialogStudiods-SNIPS[165]Apache License 2.0\n\nDialogStudiods-SNIPS-NER[165]Apache License 2.0\n\nDialogStudiods-SParC[165]Apache License 2.0, CC BY-SA 4.0\n\nDialogStudiods-SQA[165]Apache License 2.0, CC BY-SA 4.0\n\nDialogStudiods-STAR[165]Apache License 2.0, MIT License\n\nDialogStudiods-Spider[165]Apache License 2.0, CC BY-SA 4.0\n\nDialogStudiods-TOP[165]Apache License 2.0, CC BY-SA\n\nDialogStudiods-TOP-NER[165]Apache License 2.0, CC BY-SA\n\nDialogStudiods-Taskmaster1[165]Apache License 2.0, CC BY 4.0\n\nContinued on next page\n\n38\n\nCollectionDataset IdentifierLicenses\n\nDialogStudiods-Taskmaster2[165]Apache License 2.0, CC BY 4.0\n\nDialogStudiods-Taskmaster3[165]Apache License 2.0, CC BY 4.0\n\nDialogStudiods-ToTTo[165]Apache License 2.0, CC BY-SA 3.0\n\nDialogStudiods-TweetSumm[165]Apache License 2.0, CC0 1.0\n\nDialogStudiods-WOZ2_0[165]Apache License 2.0\n\nDialogStudiods-WebQSP[165]Apache License 2.0, CC BY 4.0\n\nDialogStudiods-WikiSQL[165]Apache License 2.0, BSD 3-ClauseLicense\n\nDialogStudiods-WikiTQ[165]Apache License 2.0, CC BY-SA 4.0\n\nDialogStudiods-chitchat-dataset[165]Apache License 2.0, MIT License\n\nDialogStudiods-wizard_of_internet[165]Apache License 2.0, CC BY 4.0\n\nDialogStudiods-wizard_of_wikipedia[165]Apache License 2.0, CC BY 4.0\n\nFlan Collection (Chain-of-Thought)fc-cot-cot_gsm8k[34]MIT License\n\nFlan Collection (Chain-of-Thought)fc-cot-cot_strategyqa[59]CC BY-SA 3.0\n\nFlan Collection (Chain-of-Thought)fc-cot-stream_creak[125]MIT License, CC BY-SA 4.0\n\nFlan Collection (Chain-of-Thought)fc-cot-stream_esnli[21]MIT License, CC BY-SA 4.0\n\nFlan Collection (Flan 2021)fc-flan-drop[46]CC BY 4.0\n\nFlan Collection (Flan 2021)fc-flan-e2e_nlg[123]CC BY-SA 4.0\n\nFlan Collection (Flan 2021)fc-flan-natural_questions[89]Apache License 2.0, CC BY-SA 3.0\n\nFlan Collection (Flan 2021)fc-flan-quac[29]CC BY-SA 4.0\n\nFlan Collection (Flan 2021)fc-flan-squad_v1[147]CC BY-SA 4.0\n\nFlan Collection (Flan 2021)fc-flan-squad_v2[147]CC BY-SA 4.0\n\nFlan Collection (Flan 2021)fc-flan-trec[100]CC0 1.0\n\nFlan Collection (Flan 2021)fc-flan-true_case[100]CC0 1.0\n\nFlan Collection (Flan 2021)fc-flan-wiki_lingua_english_en[90]CC BY 3.0\n\nFlan Collection (Flan 2021)fc-flan-winogrande[158]Apache License 2.0, CC BY 4.0\n\nFlan Collection (Flan 2021)fc-flan-wnli[98]CC BY 4.0\n\nFlan Collection (Flan 2021)fc-flan-word_segment[98]CC0 1.0\n\nFlan Collection (Flan 2021)fc-flan-wsc[98]CC BY 4.0\n\nFlan Collection (P3)fc-p3-adversarial_qa[11]CC BY-SA 3.0\n\nFlan Collection (P3)fc-p3-cos_e[144]BSD 3-Clause License\n\nFlan Collection (P3)fc-p3-dbpedia_14[97]CC BY-SA 3.0\n\nFlan Collection (P3)fc-p3-hotpotqa[190]Apache License 2.0, CC BY-SA 4.0\n\nFlan Collection (P3)fc-p3-quarel[153]CC BY 4.0\n\nFlan Collection (P3)fc-p3-quartz[170]CC BY 4.0\n\nFlan Collection (P3)fc-p3-quoref[44]CC BY 4.0\n\nFlan Collection (P3)fc-p3-web_questions[13]CC BY 4.0\n\nFlan Collection (P3)fc-p3-wiki_bio[93]CC BY-SA 3.0\n\nFlan Collection (P3)fc-p3-wiki_hop[93]CC BY-SA 3.0\n\nfc-sni-adversarial_qa[11]CC BY-SA 3.0\n\nContinued on next page\n\n39\n\nCollectionDataset IdentifierLicenses\n\nfc-sni-adverserial_qa[11]MIT License\n\nfc-sni-air_dialogue[187]Apache License 2.0\n\nfc-sni-ancora_ca_ner[187]CC BY 4.0\n\nfc-sni-anem[124]MIT License, CC BY-SA 3.0\n\nfc-sni-argkpApache License 2.0, CC BY-SA 3.0\n\nfc-sni-asian_language_-treebank[151]CC BY 4.0\n\nfc-sni-atomic[75]CC BY 4.0\n\nfc-sni-bard[54]Apache License 2.0\n\nfc-sni-cedr[161]Apache License 2.0\n\nfc-sni-circa[112]CC BY-SA 4.0\n\nfc-sni-clue_cmrc2018[42]CC BY-SA 4.0\n\nfc-sni-coached_conv_pref[139]CC BY 4.0\n\nfc-sni-copa_hrBSD 2-Clause License\n\nfc-sni-crows_pairs[122]CC BY-SA 4.0\n\nfc-sni-cuad[71]CC BY 4.0\n\nfc-sni-defeasible_nli_atomic[155]MIT License\n\nfc-sni-disfl_qa[67]CC BY 4.0\n\nfc-sni-e_snli[21]MIT License\n\nfc-sni-gap[186]Apache License 2.0\n\nfc-sni-hotpotqa[190]Apache License 2.0, CC BY-SA 4.0\n\nfc-sni-human_ratings_of_natural_-language_generation_outputs[190]CC BY 4.0\n\nfc-sni-hybridqa[26]CC BY 4.0, MIT License\n\nfc-sni-iirc[53]CC BY 4.0\n\nfc-sni-jigsaw[53]CC0 1.0\n\nContinued on next page\n\n40\n\nCollectionDataset IdentifierLicenses\n\nfc-sni-librispeech_asr[127]CC BY 4.0\n\nfc-sni-logic2text[27]MIT License\n\nfc-sni-numeric_fused_head[49]MIT License\n\nfc-sni-offenseval_dravidian[49]CC BY 4.0\n\nfc-sni-open_pi[172]CC BY 4.0\n\nfc-sni-paper_reviews_data_set[172]CC BY 4.0\n\nfc-sni-poem_sentiment[163]CC BY 4.0\n\nfc-sni-propara[43]Apache License 2.0\n\nfc-sni-quarel[153]CC BY 4.0\n\nfc-sni-quartz[170]CC BY 4.0\n\nfc-sni-quoref[44]CC BY 4.0\n\nfc-sni-ro_sts_parallel[48]CC BY-SA 4.0\n\nfc-sni-schema_guided_dstc8[148]CC BY-SA 4.0\n\nfc-sni-scitail[86]Apache License 2.0\n\nfc-sni-scitailv1.1[86]Apache License 2.0\n\nfc-sni-semeval_2020_task4[86]CC BY-SA 4.0\n\nfc-sni-sms_spam_collection_v.1[86]CC BY 4.0\n\nfc-sni-splash[86]CC BY-SA 4.0\n\nfc-sni-squad2.0[147]CC BY-SA 4.0\n\nfc-sni-squad_1.1[146]CC BY-SA 4.0\n\nfc-sni-strategyqa[59]MIT License\n\nfc-sni-universal_dependencies___-english_dependency_treebank[59]CC BY-SA 4.0\n\nfc-sni-web_questions[13]CC BY 4.0\n\nfc-sni-wiki_hop[93]CC BY-SA 3.0\n\nContinued on next page\n\n41\n\nCollectionDataset IdentifierLicenses\n\nfc-sni-wikitext[116]CC BY-SA 3.0\n\nfc-sni-winograd_wsc[98]CC BY 4.0\n\nfc-sni-winomt[168]MIT License\n\nfc-sni-winowhy[194]MIT License\n\nfc-sni-wsc; enhanced_wsc[194]CC BY 4.0\n\nfc-sni-wsc_fiexed[194]CC BY-SA 3.0\n\nfc-sni-xcopa[134]CC BY 4.0\n\nfc-sni-xquad[6]CC BY-SA 4.0\n\nOpen Assistantoasst-en[88]Apache License 2.0, CC BY 4.0\n\nOpen Assistant OctoPackoasst-en-octopack[88]Apache License 2.0, CC BY 4.0\n\nOpen Assistant v2oasst2-en[88]Apache License 2.0\n\nOIGoig-unified_canadian_-parliament[88]Apache License 2.0\n\nOIGoig-unified_cuad[88]Apache License 2.0, CC BY 4.0\n\nOIGoig-unified_grade_school_math_-instructions[88]Apache License 2.0, MIT License\n\nOIGoig-unified_nq[88]Apache License 2.0, CC BY-SA 3.0\n\nOIGoig-unified_sqlv1[88]Apache License 2.0, CC BY-SA 4.0\n\nOIGoig-unified_sqlv2[88]Apache License 2.0, CC BY-SA 4.0\n\nOIGoig-unified_squad_v2_more_-neg[88]Apache License 2.0, CC BY-SA 4.0\n\nTasksource Instructtsi-balanced_copa[85]BSD 2-Clause License\n\nTasksource Instructtsi-breaking_nli[60]CC BY-SA 4.0\n\nTasksource Instructtsi-cladder[60]MIT License\n\nTasksource Instructtsi-condaqa[149]Apache License 2.0\n\nTasksource Instructtsi-conj_nli[157]MIT License\n\nTasksource Instructtsi-defeasible_nli-atomic[155]MIT License\n\nTasksource Instructtsi-defeasible_nli-snli[155]MIT License\n\nTasksource Instructtsi-dynasent-dynabench.dynasent.r1.all-r1[135]CC BY 4.0\n\nTasksource Instructtsi-dynasent-dynabench.dynasent.r2.all-r2[135]CC BY 4.0\n\nTasksource Instructtsi-fever_evidence_related-mwong_-_fever_related[177]CC BY-SA 4.0\n\nTasksource Instructtsi-few_nerd-supervised[45]CC BY-SA 4.0\n\nTasksource Instructtsi-fig_qa[102]MIT License\n\nTasksource Instructtsi-fracas[56]MIT License\n\nContinued on next page\n\n42\n\nCollectionDataset IdentifierLicenses\n\nTasksource Instructtsi-hyperpartisan_news[56]CC BY 4.0\n\nTasksource Instructtsi-lex_glue-case_hold[23]Apache License 2.0\n\nTasksource Instructtsi-lonli[174]MIT License\n\nTasksource Instructtsi-moral_stories-full[51]MIT License\n\nTasksource Instructtsi-neqa[51]CC BY 4.0\n\nTasksource Instructtsi-prostApache License 2.0\n\nTasksource Instructtsi-quote_repetitionCC BY 4.0\n\nTasksource Instructtsi-recast-recast_factualityCC BY-SA 4.0\n\nTasksource Instructtsi-recast-recast_megaveridicalityCC BY-SA 4.0\n\nTasksource Instructtsi-recast-recast_nerCC BY-SA 4.0\n\nTasksource Instructtsi-recast-recast_punsCC BY-SA 4.0\n\nTasksource Instructtsi-recast-recast_sentimentCC BY-SA 4.0\n\nTasksource Instructtsi-recast-recast_verbcornerCC BY-SA 4.0\n\nTasksource Instructtsi-recast-recast_verbnetCC BY-SA 4.0\n\nTasksource Instructtsi-redefine_mathCC BY 4.0\n\nTasksource Instructtsi-tracie[196]Apache License 2.0\n\nTasksource Instructtsi-truthful_qa-multiple_-choice[101]Apache License 2.0\n\nTasksource Instructtsi-vitaminc-tals__vitaminc[162]MIT License\n\nTasksource Instructtsi-winowhy[194]MIT License\n\nTasksource Symbol-Tuningtsy-breaking_nli[60]CC BY-SA 4.0\n\nTasksource Symbol-Tuningtsy-cladder[60]MIT License\n\nTasksource Symbol-Tuningtsy-condaqa[149]Apache License 2.0\n\nTasksource Symbol-Tuningtsy-conj_nli[157]MIT License\n\nTasksource Symbol-Tuningtsy-defeasible_nli-atomic[155]MIT License\n\nTasksource Symbol-Tuningtsy-defeasible_nli-snli[155]MIT License\n\nTasksource Symbol-Tuningtsy-dynasent-dynabench.dynasent.r1.all-r1[135]CC BY 4.0\n\nTasksource Symbol-Tuningtsy-dynasent-dynabench.dynasent.r2.all-r2[135]CC BY 4.0\n\nTasksource Symbol-Tuningtsy-fever_evidence_related-mwong__fever_related[177]CC BY-SA 4.0\n\nTasksource Symbol-Tuningtsy-fracas[56]MIT License\n\nTasksource Symbol-Tuningtsy-hyperpartisan_news[56]CC BY 4.0\n\nTasksource Symbol-Tuningtsy-lonli[174]MIT License\n\nTasksource Symbol-Tuningtsy-recast-recast_factuality[174]CC BY-SA 4.0\n\nTasksource Symbol-Tuningtsy-recast-recast_-megaveridicality[174]CC BY-SA 4.0\n\nTasksource Symbol-Tuningtsy-recast-recast_ner[174]CC BY-SA 4.0\n\nTasksource Symbol-Tuningtsy-recast-recast_puns[174]CC BY-SA 4.0\n\nTasksource Symbol-Tuningtsy-recast-recast_sentiment[174]CC BY-SA 4.0\n\nTasksource Symbol-Tuningtsy-recast-recast_verbcorner[174]CC BY-SA 4.0\n\nContinued on next page\n\n43\n\nCollectionDataset IdentifierLicenses\n\nTasksource Symbol-Tuningtsy-recast-recast_verbnet[174]CC BY-SA 4.0\n\nTasksource Symbol-Tuningtsy-tracie[196]Apache License 2.0\n\nTasksource Symbol-Tuningtsy-vitaminc-tals__vitaminc[162]MIT License\n\nTasksource Symbol-Tuningtsy-winowhy[194]MIT License\n\nEList of News sources\n\nThe Common Pile contains a variety of openly licensed news sources released under CC BY andCC BY-SA licenses. The sources licensed under CC BY include: 360info, Africa is a Country, AltNews, Balkan Diskurs, Factly, Freedom of the Press Foundation, Agenzia Fides, Global Voices,Meduza, Mekong Eye, Milwaukee Neighborhood News Service, Minority Africa, New CanadianMedia, SciDev.Net, The Solutions Journalism Exchange, Tasnim News Agency, and ZimFact. Thesources licensed under CC BY-SA include: Oxpeckers, Propastop, and The Public Record.\n\nFList of WikiMedia wikis\n\nOfficial Wikimedia wikis are released under a CC BY-SA license. The Common Pile includes thefollowing Wikimedia wikis: Wikipedia, Wikinews, Wikibooks, Wikiquote, Wikisource, Wikiversity,Wikivoyage, and Wiktionary.\n\nGCCCC Source Statistics\n\nWe provide additional statistics on the CCCC subset of the Common Pile, including the number ofunicode words and documents sourced from each Common Crawl snapshot, in Table 2.\n\nTable 2: Counts of words and documents extracted from 52 snapshots after filtering with our pipeline.\n\nSnapshotUnicode WordsDocuments\n\nCC-MAIN-2013-203,851,018,1975,529,294\n\nCC-MAIN-2013-484,544,197,2526,997,831\n\nCC-MAIN-2014-104,429,217,9416,682,672\n\nCC-MAIN-2014-154,059,132,8735,912,779\n\nCC-MAIN-2014-235,193,195,7658,253,690\n\nCC-MAIN-2014-354,254,690,9456,551,673\n\nCC-MAIN-2014-414,289,814,4496,558,170\n\nCC-MAIN-2014-423,986,284,7416,144,797\n\nCC-MAIN-2014-493,316,075,4524,699,472\n\nCC-MAIN-2014-524,307,765,2896,338,983\n\nCC-MAIN-2015-063,675,982,6795,181,955\n\nCC-MAIN-2015-113,932,442,9005,438,533\n\nCC-MAIN-2015-143,658,107,7654,954,273\n\nContinued on next page\n\n44\n\nSnapshotUnicode WordsDocuments\n\nCC-MAIN-2015-184,451,734,9466,319,757\n\nCC-MAIN-2015-224,285,945,3195,949,267\n\nCC-MAIN-2015-273,639,904,1284,975,152\n\nCC-MAIN-2016-071,588,496,7033,798,207\n\nCC-MAIN-2016-183,228,754,2004,446,815\n\nCC-MAIN-2016-223,217,827,6764,242,762\n\nCC-MAIN-2017-043,852,699,2135,239,605\n\nCC-MAIN-2017-094,186,915,4985,119,171\n\nCC-MAIN-2017-134,950,110,9315,923,670\n\nCC-MAIN-2017-174,684,050,8305,645,725\n\nCC-MAIN-2017-224,683,569,2785,514,717\n\nCC-MAIN-2017-264,744,689,1375,514,047\n\nCC-MAIN-2017-511,981,004,3062,529,289\n\nCC-MAIN-2018-134,816,417,9305,520,099\n\nCC-MAIN-2018-223,921,533,2514,401,956\n\nCC-MAIN-2018-264,506,583,9314,916,546\n\nCC-MAIN-2018-304,936,722,4035,282,886\n\nCC-MAIN-2018-343,865,953,9783,808,725\n\nCC-MAIN-2018-473,933,439,8413,637,947\n\nCC-MAIN-2018-514,745,124,4224,616,832\n\nCC-MAIN-2019-044,475,679,1904,140,277\n\nCC-MAIN-2019-094,287,868,8004,142,190\n\nCC-MAIN-2019-133,966,330,3483,849,631\n\nCC-MAIN-2019-304,179,526,1884,430,572\n\nCC-MAIN-2019-355,144,426,2705,048,106\n\nCC-MAIN-2019-394,572,972,4574,527,430\n\nCC-MAIN-2020-295,200,565,5014,984,248\n\nCC-MAIN-2020-344,458,827,9474,297,009\n\nCC-MAIN-2021-171,768,757,3861,824,942\n\nCC-MAIN-2021-394,599,961,6754,287,356\n\nCC-MAIN-2021-435,337,349,3315,304,846\n\nCC-MAIN-2021-493,980,018,7734,050,641\n\nCC-MAIN-2022-054,517,850,0194,503,863\n\nCC-MAIN-2023-065,135,614,2274,959,915\n\nCC-MAIN-2023-145,117,143,7654,675,097\n\nCC-MAIN-2023-235,461,486,8074,869,627\n\nCC-MAIN-2023-505,881,860,0144,901,306\n\nCC-MAIN-2024-105,164,171,5624,335,071\n\nCC-MAIN-2024-184,745,457,0543,949,186\n\nTotal221,715,271,483259,728,610\n\n45\n\nHPeS2o Source Statistics\n\nAdditional statistics on the composition of the peS2o subset of the Common Pile can be found inTable 3 and Table 4.\n\nTable 3: Distribution of licenses in the peS2o subset.\n\nLicenseTrain SplitValidation Split\n\nCC BY6,088,32537,754CC BY-SA120,1501,231CC036,373121Public domain10,0606\n\nTable 4: Distribution of papers across 23 fields of study, as identified by the Semantic ScholarAPI [87]. A paper may belong to one or more fields of study.\n\nField of StudyTrain SplitValidation Split\n\nMedicine2,435,24423,734\n\nBiology1,518,4788,879\n\nEnvironmentalScience993,4997,601\n\nEngineering656,0215,005\n\nComputer Science462,3203,003\n\nMaterials Science416,0453,166\n\nPhysics413,4611,285\n\nChemistry406,4292,781\n\nPsychology364,4412,126\n\nEducation220,0141,532\n\nBusiness193,536946\n\nEconomics185,716921\n\nAgricultural and FoodSciences333,7762,013\n\nSociology137,2571,535\n\nMathematics135,676199\n\nPolitical Science106,748378\n\nGeology67,258217\n\nGeography44,269257\n\nLinguistics41,737228\n\nHistory36,848192\n\nLaw30,888251\n\nPhilosophy27,518148\n\nArt26,65875\n\n46\n\nIGrowth rates of openly licensed data\n\nOver time, the volume of openly licensed data continues to grow as more creators release contentunder open licenses. In Figure 6, we quantify this growth between 2010 and 2024 by analyzingsubsets of the Common Pile for which reliable creation date metadata is available. We plot thecumulative proportion of data created up to various cutoff dates and find that approximately halfof the Common Pile (around 3.8TB) was created since 2020. This trend provides insight into thegrowing availability of openly licensed data and suggests a promising trajectory for future LLMstrained entirely on openly licensed sources.\n\n20102012201420162018202020222024Cutoff Date\n\n0\n\n20\n\n40\n\n60\n\n80\n\n100\n\nPercentage of Total Size\n\nQuantity of Openly Licensed Data Over Time\n\nAllCodeEducational Resources\n\nOtherOnline ForumsWeb\n\nWikisGovernmentAcademic Papers\n\nFigure 6: The amount of openly licensed text grows steadily over time. We visualize the cumulativeproportion of data created up to various cutoff dates for sources in the Common Pile with reliablecreation date metadata. This includes all sources except for the Caselaw Access Project, DataProvenance Initiative, and the sources covering early 20th century Public Domain books.\n\nJDetails on filtering pipelines\n\nIn subsection 4.1, we detail the steps used to produce the Comma v0.1 training dataset from theraw text in the Common Pile. These include applying filters based on language, text quality, length,likelihood, and toxicity; removing various forms of PII; and removal of source-specific boilerplatetext using regular expressions. The Common Pile contains a diverse range of sources and we thereforedesign separate filtering thresholds for each source. The exact source-specific thresholds used topost-process the Common Pile can be found in Table 5. Additionally, statistics on the pre- andpost-filtered sizes of each source can be found in Table 6.\n\nTable 5: Pre-processing pipelines applied to each source in the Common Pile to construct the Commadataset.\n\nSourceLanguage Text QualityDoc LengthLog-LikelihoodToxicityPIIRegex Filter\n\nArXiv Abstracts–––––YN\n\nArXiv Papers> 0.5––––YN\n\nBiodiversityHeritage Library> 0.5–> 100> -20–NY\n\nCaselaw AccessProject––> 100–> 0.1YN\n\nCC CommonCrawl> 0.5> 0.0001> 100–> 0.1YN\n\nContinued on next page\n\n47\n\nSourceLanguage Text QualityDoc LengthLog-LikelihoodToxicityPIIRegex Filter\n\nData ProvenanceInitiative–––––NN\n\nDatabase ofOpen AccessBooks\n\n> 0.5–> 200–> 0.1YN\n\nFoodista> 0.5–> 100––NN\n\nGitHub Archive> 0.5–> 100–> 0.1YN\n\nLibrary ofCongress–––> -20> 0.1NY\n\nLibreTexts> 0.5–> 700–> 0.1YN\n\nNews> 0.5–> 100––YN\n\nOERCommons> 0.5–> 300–> 0.1YN\n\npeS2o–––––YN\n\nPre-1929 Books–––> -20> 0.1NY\n\nPressBooks> 0.5–> 600–> 0.1YN\n\nProjectGutenberg> 0.5––> -20–NN\n\nPublic DomainReview––> 100––YN\n\nPubMed> 0.5–> 100––YN\n\nPEPs–––––YN\n\nRegulations.gov––> 100––YY\n\nStackExchange> 0.5––––YN\n\nUbuntu IRC> 0.5–> 100–> 0.1YN\n\nUK Hansard> 0.5––––YN\n\nUSGPO–––––NY\n\nUSPTO––> 100> -20–YN\n\nWikimedia> 0.5–> 100––YN\n\nWikiteam> 0.5–> 700–> 0.1YN\n\nCC YouTube> 0.5–> 100–> 0.1YN\n\nTable 6: Raw and filtered sizes of the Common Pile’s constituent datasets.\n\nDocument CountSize (GB)\n\nSourceRawFilteredRawFiltered\n\nArXiv Abstracts2,538,9352,504,6792.42.4\n\nArXiv Papers321,336304,0482119\n\nBiodiversity HeritageLibrary42,418,49815,111,3139635\n\nCaselaw Access Project6,919,2406,735,5257877\n\nContinued on next page\n\n48\n\nDocument CountSize (GB)\n\nSourceRawFilteredRawFiltered\n\nCC Common Crawl51,054,4126,852,13726058\n\nData Provenance Initiative9,688,2113,508,51873\n\nDirectory of Open AccessBooks474,445403,99212.512\n\nFoodista72,09065,6400.090.08\n\nGitHub Archive30,318,77423,358,58054.740.4\n\nLibrary of Congress135,500129,05247.835.6\n\nLibreTexts62,26940,0495.33.6\n\nNews172,308126,6730.40.3\n\nOERCommons9,3395,2490.10.05\n\npeS2o6,294,0206,117,280188.2182.6\n\nPre-1929 Books137,127124,89873.846.3\n\nPressBooks106,88154,4551.50.6\n\nProject Gutenberg71,81055,45426.220.1\n\nPublic Domain Review1,4121,4060.0070.007\n\nPubMed4,068,8673,829,689158.9147.1\n\nPEPs6566550.010.01\n\nRegulations.gov225,196208,3016.15.1\n\nStackExchange33,415,40030,987,814103.789.7\n\nStack V2218,364,13369,588,6074774.7259.9\n\nUbuntu IRC329,115234,9826.35.3\n\nUK Hansard51,55247,909109.6\n\nUSGPO2,732,6772,148,54874.536.1\n\nUSPTO20,294,15217,030,2311003.4661.1\n\nWikimedia63,969,93816,311,57490.557.4\n\nWikiteam219,139,36826,931,807437.513.7\n\nCC YouTube1,129,692998,10421.518.6\n\nTotal692,854,953233,817,1697557.91838.3\n\nKDetails on Comma’s pre-training data mixture\n\nWe estimated the quality of each source in the Common Pile by training a 1.7B-parameter model for28B tokens on each source individually and evaluating the resulting models on the set of “early signal”tasks from [132]. In doing so, we found that the amount of text in each source was poorly correlatedwith text quality, motivating the use of heuristic mixing weights to up-/down-weight different sourcesin our pre-training mix. In Table 7 we list the pre-training mixture weights for each of the sources inthe Common Pile.\n\n49\n\nTable 7: Overview of the data mixing used to up/down-weight individual sources in the CommonPile to construct the Comma v0.1-1T pre-training dataset. Comma v0.1-2T simply repeats this fullmixture twice.\n\nSourceSize (GB)RepeatsEffective Size(GB)Tokens(Billions)Percentage\n\nArXivAbstracts2.4614.43.60.360%\n\nArXiv Papers19.5611729.32.932%\n\nBiodiversityHeritageLibrary\n\n35.50.258.92.20.220%\n\nCaselawAccess Project77.5177.519.41.941%\n\nCC CommonCrawl58.16348.687.18.716%\n\nDataProvenanceInitiative\n\n3.4620.45.10.510%\n\nDatabase ofOpen AccessBooks\n\n12672181.801%\n\nFoodista0.0860.480.120.012%\n\nGitHubArchive40.46242.460.66.064%\n\nLibrary ofCongress35.60.258.92.20.220%\n\nLibreTexts3.6621.65.40.540%\n\nNews0.2561.50.380.038%\n\nOERCommons0.0560.30.080.008%\n\npeS2o182.661,095.6273.927.409%\n\nPre-1929Books46.3146.311.61.161%\n\nPressBooks0.663.60.90.090%\n\nProjectGutenberg20.1120.150.500%\n\nPublic DomainReview0.00760.040.010.001%\n\nPubMed147.11147.136.83.683%\n\nPEPs0.0160.060.020.002%\n\nRegulations.gov5.1630.67.60.761%\n\nStackExchange89.76538.2134.613.469%\n\nStack V2259.92519.813013.009%\n\nContinued on next page\n\n50\n\nSourceSize (GB)RepeatsEffective Size(GB)Tokens(Billions)Percentage\n\nUbuntu IRC5.3631.87.90.791%\n\nUK Hansard9.6657.614.41.441%\n\nUSGPO36.10.2592.30.230%\n\nUSPTO661.10.25165.341.34.133%\n\nWikimedia57.46344.486.18.616%\n\nWikiteam13.7454.813.71.371%\n\nCC YouTube18.6118.64.70.470%\n\nTotal1838.3–3997.4999.3100%\n\nLDetails on Comma’s cool-down data mixture\n\nFollowing Hu et al. [73], we end training with a “cool-down” where we train on 37.7B tokens ofhigh-quality data while linearly decaying the learning rate to 0. We provide the source mixtureweights for this cool-down phase in Table 8.\n\nTable 8: Overview of the data mixing used to up/down-weight individual sources in the CommonPile to construct the training distribution for Comma v0.1-1T’s cool-down phase. Comma v0.1-2Tsimply repeats this full mixture twice.\n\nSourceSize (GB)RepeatsEffective Size(GB)Tokens(Billions)Percentage\n\nArXiv Papers19.50.59.82.46.50%\n\nCC CommonCrawl58.10.317.44.411.63%\n\nDataProvenanceInitiative\n\n3.426.81.74.55%\n\nDatabase ofOpen AccessBooks\n\n12224616.04%\n\nFoodista0.0820.160.040.11%\n\nLibreTexts3.627.21.80.48%\n\nNews0.2520.50.130.33%\n\nOERCommons0.0520.10.030.07%\n\npeS2o182.60.118.34.612.18%\n\nPressBooks0.621.20.30.77%\n\nPublic DomainReview0.00720.0140.0040.01%\n\nContinued on next page\n\n51\n\nSourceSize (GB)RepeatsEffective Size(GB)Tokens(Billions)Percentage\n\nPEPs0.0120.020.0050.02%\n\nStackExchange89.70.2522.45.614.96%\n\nStack V2259.90.126.06.517.04%\n\nWikimedia57.40.4235.715.32%\n\nTotal679.4–149.937.5100%\n\nMDetails on small-scale data ablations\n\nIn subsection 4.3 we report results from a series of small-scale data ablations where we identicallytrained 1.7B parameter models on various openly licensed and unlicensed datasets and evaluate theirperformance on the “early signal” tasks from Penedo et al. [132] to compare their data quality againstthe Common Pile. In Figure 7 we show how the performance of these models evolve over the courseof their training run, highlighting that differences in data quality become apparent very early intraining. Additionally, we provide exact numerical results for each model in Table 9, showing thatthe Common Pile has higher data quality than any previously released openly licensed datasets andthe Pile, and nearly matches the data quality of the OSCAR dataset. To validate that this is not purelydue to the presence of high-quality supervised fine-tuning data from the Data Provenance Initiative(DPI) data source, we also perform an ablation on the Common Pile excluding the DPI data and findthat the final performance of this model is largely unchanged.\n\nFigure 7: A model trained on the Comma dataset consistently outperforms models trained onother corpora of openly licensed text and outperforms the Pile on all but two tasks. We trainidentical 1.7B parameter models on 28B tokens from each dataset following Penedo et al. [132].\n\nNAdditional Comma results\n\nWe provide exact numerical results for Comma v0.1-1T and -2T alongside baseline models resultsacross a variety of knowledge, reasoning, and coding tasks in Table 10 and Table 11 respectively. Wefind that particularly on knowledge-based benchmarks (such as MMLU) and coding benchmarks,\n\n52\n\nTable 9: Comma’s training dataset has higher quality than previous openly-licensed datasetsand unlicensed datasets like the Pile. In the small-scale (1.7B parameter) data ablation setting,we find that Comma’s training dataset yields better models than previous openly licensed datasetsand the Pile, and nearly matches the performance of models trained on OSCAR. Additionally, wefind that removing the high-quality supervised data from the Data Provenance Initiative has marginalaffect on the Comma dataset’s overall quality.\n\nDatasetARCMMLUHSOBQACSQAPIQASIQAAvg.\n\nKL3M31.826.329.928.426.858.238.036.2OLC33.127.533.827.427.759.438.537.3Common Corpus34.227.033.630.226.461.037.737.6Comma (no DPI)37.728.737.631.030.863.839.840.0Comma38.029.539.932.429.665.839.440.8\n\nThe Pile37.027.835.828.631.566.838.239.6OSCAR35.427.640.830.432.169.739.740.9FineWeb38.029.148.234.233.673.440.343.7\n\nComma v0.1-1T and -2T outperform baseline models trained on an equivalent amount (1T or 2Ttokens, respectively) of unlicensed text.\n\nTable 10: Comparison between Comma v0.1-1T and baseline models trained with similar resources (7billion parameters, 1 trillion tokens) across a variety of knowledge, reasoning, and coding benchmarks.\n\nModelARC-CARC-EMMLUBoolQHSOBQACSQAPIQASIQAHEvalMBPPAvg.\n\nRPJ-INCITE42.868.427.868.670.349.457.776.046.911.115.948.6LLaMA44.567.934.875.476.251.261.877.250.319.927.953.4StableLM50.865.445.271.775.648.257.277.048.223.132.054.0MPT46.570.530.274.277.648.663.377.349.127.333.254.3OpenLLaMA44.567.240.372.672.650.862.878.049.727.633.954.5Comma v0.1-1T52.868.442.475.762.647.059.470.850.836.535.554.7\n\nQwen357.274.577.086.177.050.866.478.255.094.567.571.3\n\nTable 11: Performance of Comma v0.1-2T and a variety of budget-matched baseline models.\n\nModelARC-CARC-EMMLUBoolQHSOBQACSQAPIQASIQAHEvalMBPPAvg.\n\nOLMo Twin45.267.528.271.773.448.061.877.948.518.227.551.6Llama 248.569.545.880.276.248.462.876.750.826.128.555.8Comma v0.1 2T45.871.849.878.664.446.264.072.552.344.241.557.4DeepSeekLLM49.567.748.571.774.152.066.677.851.643.143.858.8\n\nOAdditional training runs\n\nTo explore the sensitivity of our Comma v0.1 results to hyperparameter choices, we perform a seriesof additional 7B parameter/1T token training runs on AMD MI300A GPUs with slight alterationsto the training recipe. Due to both a desire to reach the same 1T token target rapidly, and the lowersingle-GPU throughput on the system available for these ablations, for all additional runs the thetraining batch size is 8.3M (223) versus the 2.1M (221) tokens per step of Comma v0.1. Unlessotherwise specified, we did not use the two phase training process described in subsection 4.4 (i.e. noseparate high-quality cooldown phase is run and we do not perform checkpoint averaging at the endof training and before evaluation).\n\nO.1Ablations at 1T Tokens\n\nWe first performed a set of training runs for 125,000 steps, resulting in 1.048T total tokens (referredto as “1T” for brevity).\n\n53\n\n“8M Batch”We perform a run with nearly the same training hyperparameters as Comma v0.1-1T,except with a larger 8M token batch size. We also use a single phase training setup; the base datamixture (Table 7) is run for the entire duration to 1T tokens. The learning rate schedule is 2,000 stepsof warm-up from 0 to a peak of 1e −3 with 123,000 steps of decay to a minimum of 1.8e −9.\n\n“Curriculum”In this experimental run, a different data mixture is used in each of three trainingstages of equal duration (we also use the modified hyperparameters from “8M Batch” ablationabove). The first stage of the curriculum comprises data from only the Common Pile’s largest sources(mostly USPTO, Table 13). The second stage uses the same data mixture as Comma v0.1’s mainpre-training phase (“phase I”), but run for only 1/3 of the duration. Finally, the third and last stage ofthe curriculum up-weights Common Pile’s highest quality, benchmark-relevant sources (Table 14).\n\nWe provide exact numerical results for Comma v0.1 and alternate Comma runs performed withdifferent hyperparameters and data mixture curricula across a variety of knowledge, reasoning, andcoding benchmarks in Table 12. We find that the 8M Batch and Curriculum ablations are roughlycomparable on average to the main Comma v0.1-1T run, with the notable exception that both ablationsslightly outperform Comma v0.1-1T on the coding benchmarks. We conclude that the benchmarkresults reported for Comma v0.1-1T in subsection 4.4 seem relatively robust to minor changes intraining hyperparameters, dataset mixture curriculum (assuming similar amounts of most data splitsappear at some time during training), and the software environment and GPU hardware used to trainthe model.\n\nTable 12: Comparison between our main Comma v0.1-1T training run and alternate runs performedwith different hyperparameters and data mixture curricula across a variety of knowledge, reasoning,and coding benchmarks. For “Main”, we report the performance of Comma v0.1-1T without averagingthe cooldown checkpoints so that it is a fair comparison.\n\nModelARC-CARC-EMMLUBoolQHSOBQACSQAPIQASIQAHEvalMBPPAvg.\n\nCurriculum45.269.141.474.760.846.859.170.548.638.134.653.58M Batch47.269.642.969.962.947.056.970.450.536.837.253.8Main50.868.440.272.962.346.259.571.051.232.134.653.6\n\nTable 13: Overview of the data mixing used to up/down-weight individual sources for the Stage 1 ofthe Curriculum ablation run. In this table we omit the size columns for brevity.\n\nSourceRepeatsTokens(Billions)Percentage\n\nUSPTO1.4125233.566.81%\n\nPre-1929Books5.6565.418.71%\n\nStack V2(HTML)11.312.83.65%\n\nUSGPO1.4112.83.65%\n\nLibrary ofCongress1.4112.63.59%\n\nBiodiversityHeritageLibrary\n\n1.4112.523.58%\n\nTotal–349.4100%\n\n54\n\nTable 14: Overview of the data mixing used to up/down-weight individual sources for the Stage 3 ofthe Curriculum ablation run. In this table we omit the size columns for brevity.\n\nSourceRepeatsTokens(Billions)Percentage\n\nStack V2163.818.519%\n\nDatabase ofOpen AccessBooks\n\n6185.230%\n\nWikimedia686.124.981%\n\nStackExchange2.556.116.259%\n\npeS2o145.613.241%\n\nCC CommonCrawl343.612.638%\n\nArXiv Papers524.47.063%\n\nDataProvenanceInitiative\n\n65.11.485%\n\nPressBooks60.870.251%\n\nLibreTexts60.540.157%\n\nNews60.370.108%\n\nFoodista60.120.036%\n\nOERCommons60.080.023%\n\nPEPs60.020.005%\n\nPublic DomainReview60.010.003%\n\nTotal–344.7100%\n\n55",
  "raw_text": "arXiv:2506.05209v1  [cs.CL]  5 Jun 2025\n\nThe Common Pile v0.1: An 8TB Dataset of PublicDomain and Openly Licensed Text\n\nNikhil Kandpal∗1,2Brian Lester∗1,2Colin Raffel∗1,2,3Sebastian Majstorovic4Stella Biderman4Baber Abbasi4Luca Soldaini5Enrico Shippole6A. Feder Cooper†7Aviya Skowron4John Kirchenbauer8Shayne Longpre9LintangSutawika4,10Alon Albalak‡11Zhenlin Xu12Guilherme Penedo3Loubna Ben Allal3ElieBakouch3John David Pressman4Honglu Fan4,13Dashiell Stander4Guangyu Song4AaronGokaslan7Tom Goldstein8Brian R. Bartoldson14Bhavya Kailkhura14Tyler Murray5\n\n1University of Toronto2Vector Institute3Hugging Face4EleutherAI5The Allen Institute forArtificial Intelligence6Teraflop AI7Cornell University8University of Maryland, College Park9MIT10CMU11Lila Sciences12Independent13poolside14Lawrence Livermore NationalLaboratory\n\nAbstract\n\nLarge language models (LLMs) are typically trained on enormous quantities ofunlicensed text, a practice that has led to scrutiny due to possible intellectualproperty infringement and ethical concerns. Training LLMs on openly licensedtext presents a first step towards addressing these issues, but prior data collectionefforts have yielded datasets too small or low-quality to produce performant LLMs.To address this gap, we collect, curate, and release the Common Pile v0.1, aneight terabyte collection of openly licensed text designed for LLM pretraining.The Common Pile comprises content from 30 sources that span diverse domainsincluding research papers, code, books, encyclopedias, educational materials,audio transcripts, and more. Crucially, we validate our efforts by training two7 billion parameter LLMs on text from the Common Pile: Comma v0.1-1T andComma v0.1-2T, trained on 1 and 2 trillion tokens respectively. Both modelsattain competitive performance to LLMs trained on unlicensed text with similarcomputational budgets, such as Llama 1 and 2 7B. In addition to releasing theCommon Pile v0.1 itself, we also release the code used in its creation as well asthe training mixture and checkpoints for the Comma v0.1 models.\n\n1Introduction\n\nA critical stage of large language model (LLM) development is pretraining [72, 136, 142], where anLLM is trained to predict the next token (i.e., word or subword unit) in a corpus of unstructured text.Pretraining is widely regarded as the foundation for strong downstream performance, as it enablesLLMs to learn the structure of natural language [32, 110, 154] and accumulate a broad base of worldknowledge [133, 152]. In an effort to push the capabilities of LLMs, pre-training datasets have grownsteadily over time [143], with modern datasets containing trillions of tokens [132, 167, 193]. To meetthis increasing demand for pre-training data, the de facto approach has been to leverage the publicInternet as a source of text [57, 95, 108, 132, 142].\n\nWhile the web provides a diverse and continuously growing supply of text, much of this content—under most legal frameworks—is protected by copyright. Yet, this text is routinely used to pretrain\n\n∗Equal contribution. For a list of author contributions, see Appendix A. †Work done while a graduatestudent at Cornell University. ‡Work done while at SynthLabs.\n\nPreprint.\n\nStack V2\n\nPEPs\n\nUSPTO\n\nCAP\n\nUSGPO\n\nUK Hansard\n\nRegulations.gov\n\nWikiteam\n\nWikimedia\n\nCCCC\n\nNews\n\nFoodista\n\nPDR\n\npeS2o\n\nPubMed\n\nArXiv Papers\n\nArXiv Abstracts\n\nStack Exchange\n\nGitHub Archive\n\nUbuntu IRC\n\nBHL\n\nPre-1929 Books\n\nLibrary of Congress\n\nProject Gutenberg\n\nCC YouTube\n\nDPI\n\nDOAB\n\nPressBooks\n\nLibreTexts\n\nOER Commons\n\n1MB\n\n100MB\n\n10GB\n\n1TB\n\nDataset Size\n\nCode(4775 GB) Government & Legal\n\n(1172 GB)Wikis(528 GB)Web(260 GB)\n\nAcademic\n\nPapers(370 GB)Online Forums\n\n(165 GB)\n\nPublic Domain\n\nBooks(244 GB)Other(29 GB)\n\nEducational\n\nResources\n\n(15 GB)\n\nFigure 1: The Common Pile is an 8TB dataset of openly licensed text curated from 30 diversesources. The sources comprising the Common Pile are shown above, categorized by textual domain.\n\nLLMs, often without compensation to the creators of this content. Recent estimates suggest thatcompensating the authors of pre-training data, even at conservatively low wage rates, would costbillions of US dollars [82]. While copyright exemptions for text and data mining exist in somejurisdictions [69, 79, 92, 130, 156], many rights holders have objected to the uncompensated use oftheir work, resulting in numerous lawsuits against LLM developers [24, 191] that could carry financialdamages in the billions [40, 96, 159]. Beyond questions of intellectual property (IP) law, the use ofweb-scraped data also raises ethical concerns [9], as content creators rarely explicitly consent to thedownstream use of their work for LLM training. In fact, recent evidence suggests that many contentowners may not consent to its use as LLM training data, as shown by a sharp mid-2023 increase inwebsites blocking AI crawlers [107], following growing awareness of web data being used to trainmodels. Finally, while open models trained on publicly released pre-training datasets [18, 64, 103]support research into the study of learning dynamics [50, 76, 84], memorization [17, 22], dataauditing [47, 128, 145], and more, the use of unlicensed training data heavily limits the ability ofmodel trainers to share their datasets, and has previously resulted in DMCA takedowns of datasetssuch as the Pile [57].\n\nThe current landscape reflects a growing divide between LLM developers and content creators. Wesubmit that a natural first step toward resolving this tension is to ask: Is it possible to train performantlanguage models using only public domain and openly licensed text? We define “openly licensed”text as content that follows the Open Knowledge Foundation’s Open Definition 2.1 (further detailedin section 2 and Appendix C), which refers to content where the copyright holder has granted explicitpermission for the content to be freely accessed, used, modified, and shared for any purpose. Ourprimary contribution in this paper is to demonstrate that this is indeed possible by collecting, curating,and releasing the Common Pile v0.1, an 8TB dataset that—to our knowledge—constitutes the largestcollection of openly licensed text to date. The Common Pile comprises 30 text sources (detailed insection 3), covering diverse domains including research publications, open-source code, governmentdocuments, historical books, educational resources, audio transcripts, and more. Crucially, wedemonstrate that after appropriate filtering, deduplication, and reweighting, the Common Pile v0.1can be used as the foundation for competitive LLMs. Specifically, we train Comma v0.1-1T andComma v0.1-2T, a pair of 7-billion-parameter models with comparable performance to budget-matched models trained on unlicensed datasets such as Llama 1 and 2 7B. In the spirit of opennessand transparency, we release the Common Pile v0.1, both Comma v0.1 models and their filtered anddeduplicated pre-training dataset, and all data collection and processing code.\n\n2What do we mean by “openly licensed”?\n\nCopyright law grants content creators certain rights, such as exclusive rights (with certain exceptions)to reproduce, distribute, and create derivatives of their original works. Although copyright laws varyacross jurisdictions, original, creative works (that are “fixed” in a tangible medium, such as physicallyor digitally [see, e.g., 1]) typically fall within the scope of copyright. Works in the public domain [38]have had their copyrights expire (after a legally dictated time period), were never eligible for copyrightprotection due to specific carve-outs (e.g., government documents in the U.S. [2]), or were otherwisededicated to the public domain by their copyright owners (e.g., with a CC0 license [35]). Copyright\n\n2\n\nowners can license their protected works, allowing others to adapt and reuse them under specifiedterms. For example, Creative Commons (CC) Licenses (except CC0) grant the right to “reproduceand Share the Licensed Material, in whole or in part; and produce, reproduce, and Share AdaptedMaterial” [36]. For a more in-depth and accessible discussion about licenses and generative AI, seeLee et al. [96, Parts II.I–II.J].\n\nFor the Common Pile, we collect and curate public domain and openly licensed text, where weconsider “openly licensed” to mean any license that meets the Open Knowledge Foundation’s OpenDefinition 2.1. Some prominent examples of licenses that are considered to be “open” under thisdefinition include CC BY [37], CC BY-SA [39], and software licenses certified by the Blue OakCouncil (e.g., the MIT license) [20]. We note that CC NC (non-commercial) and ND (no derivatives)licenses are not considered open under this definition and we therefore do not include contentdistributed under these licenses. While the use of an open license does not necessarily imply that therights holder has specifically contemplated use of their content to train LLMs, most open licensesinclude text like “the above rights may be exercised in all media and formats whether now known orhereafter devised” [37]. Overall, we consider our use of openly licensed data to be a substantial firststep towards ethical pre-training dataset curation.\n\n2.1License due diligence\n\nLicense launderingThere is a large quantity of data on the internet with incorrect, ambiguous, ormissing licensing metadata [96, 106]. A common pitfall is “license laundering,” where a copyrightedwork is redistributed (typically by a non-rights holder) with an incorrect license. License launderingcan undermine our ability to confidently source openly licensed content since it implies that wecannot always trust the license distributed with a piece of content. To address this issue, we setstrict standards for data sourcing, only including data from sources where we were confident thatthe licensing information was provided by the copyright holder, which ultimately led us to excludecertain sources such as OpenAlex [80, 126], YouTube Commons [74], and the Hacker News dataseton Kaggle.\n\nUse of collection licensesA related issue is the licensing status of compilations of existing works.Many training corpora are released under open licenses, but these licenses do not necessarily alignwith the licensing status of the underlying documents [96, Part II.A]. As an example, the ODC-Bylicense has been commonly used for large-scale web corpora such as Dolma [167], FineWeb [132],and TxT360 [173]. ODC-By, by definition, does not extend to individual documents within thecorpus; therefore, the copyright of documents in these collections is still controlled by the documentauthors, and does not imply that the text itself is openly licensed.\n\nLLM-generated synthetic datasetsDatasets containing text generated by LLMs trained on un-licensed data have been released under open licenses [e.g. 195]. It has not yet been establishedwhether it is permissible to apply arbitrary licenses to the generations of an LLM that was trained onunlicensed data [96]. We therefore take a conservative stance and avoid synthetic content that wasgenerated by an LLM.\n\nCaveatsDespite our best efforts at due diligence, data that falls outside of our curatorial principlesand choices may have still ended up in our dataset. License laundering is a notoriously hardproblem to identify exhaustively in practice [96]. Copyright owners may also change the license theyassociate with their content. Since we collected and curated the Common Pile v0.1 in late 2024, thelicensing information we include and rely on may not be completely aligned with more recent updates.Further, some documents that we collect that are in the public domain or are openly licensed maycontain material with unclear status (e.g., quoted snippets of in-copyright books in public domainU.S. government publications). Finally, we note that while it is relatively straightforward to obeyattribution requirements when redistributing data, attributing model predictions back to the trainingdata points that influenced them remains an active area of research [129, 28].\n\n2.2Comparisons with related work\n\nOur work is not the first that aims to construct a dataset of openly licensed and/or public domain datafor the purposes of training machine learning models. Past efforts include CommonCanvas [61], acollection of approximately 70 million Creative Commons-licensed images designed for trainingimage generation models, the PG19 dataset [140] of public domain novels sourced from Project\n\n3\n\nGutenberg used for benchmarking language models, the C4Corpus tools for sourcing CreativeCommons text from Common Crawl snapshots [68], and many datasets comprising CC BY-SA-licensed text from Wikipedia [66, 115].\n\nMore relevant to our work are the recent Open License Corpus (OLC) [119], Common Corpus [74, 91],and KL3M [78] datasets, which were constructed for use as LLM pre-training data. On the whole,OLC uses similar selection criteria to ours, including text that is in the public domain or is openlylicensed. However, OLC also includes conversations scraped from Hacker News, which does not havean open license. Additionally, OLC is considerably smaller than the Common Pile v0.1, comprisingdata from 12 sources (vs. 30 for Common Pile) totaling 0.85 TB of text (vs. 7.6 TB for CommonPile). Common Corpus also uses a similar set of allowable licenses/copyright statuses (e.g. CCBY, CC BY-SA, public domain, MIT-style, etc.) although the specific licenses/statuses are notclear because Common Corpus does not retain full per-document licensing information across allsources. Additionally, Common Corpus incorporates data from OpenAlex [126] which is knownto provide inaccurate licensing information [e.g., 80]. Furthermore, while the Common Pile andCommon Corpus are similar in size (7.6 TB vs. 7.4 TB), Common Corpus targets a broader set oflanguages and therefore contains significantly less English text. Conversely, KL3M does not considerCC BY-SA to be acceptable and, as a result, almost exclusively consists of government documents.Accordingly, the Common Pile is much larger than KL3M (3 TB), and is built from significantlymore diverse data sources (Figure 1 & Section 3). In subsection 4.3, we compare the Common Pilev0.1 to these datasets in a controlled setting, ultimately showing that it produces substantially moreperformant LLMs.\n\n3Composition of the Common Pile\n\nThe Common Pile comprises content drawn from a wide range of domains, including scholarlypublications, government documents, online discussions, books, open educational resources, andmore. In this section, we provide an overview of each of the domains contained in the Common Pileand briefly discuss their constituent data sources. In-depth discussion of each source is provided inAppendix B.\n\nScientific and scholarly texts, which are often distributed under open licenses due to open accessmandates, appear in many LLM pre-training datasets [e.g. 57, 167, 185] since they expose models totechnical terminology, formal reasoning, and long-range document structure. To attain broad coverageof scholarly text, we filter peS2o [166] (a collection text extracted from open-access scientific PDFsbased on S2ORC [104]) to only retain openly licensed research papers. For medical-domain text,we collect text from openly licensed articles in the U.S. National Institutes of Health’s NationalLibrary of Medicine’s PubMed Central archive. Additionally, we collect data from ArXiv, whichcontains over 2.4 million articles in the quantitative sciences, most of which are uploaded as LaTeXsource and may be distributed under various licenses chosen by a given article’s author. We includeopenly licensed articles sourced from ArXiv’s bulk-access S3 bucket and parsed using LATEXMLand Trafilatura [10]. Furthermore, according to ArXiv’s licensing policy, all metadata (includingabstracts) of articles posted to ArXiv are distributed under the CC0 license; we therefore include theabstracts for all ArXiv papers in the Common Pile, regardless of the paper’s full-text license.\n\nOnline discussion forums comprise multi-turn question-answer pairs and discussions and thereforecan be useful for training language models to follow conversational structure as well as for improvingperformance on question answering and dialogue-centric tasks. StackExchange is a collection ofwebsites that host user-provided questions and answers and allow their redistribution under a CCBY-SA license. We leverage the user-provided StackExchange dumps from the Internet Archive andformat questions/answers in the same order they appear on StackExchange, using PyMarkdown toconvert each comment into plain text. Additionally, we collect text from issues, pull requests, andcomments on GitHub, which, according to GitHub’s terms of service, inherit the license of theirassociated repository. We extract this content from repositories with Blue Oak Council-approvedlicenses from the GitHub Archive. Finally, we include logs of all discussions on the Ubuntu-hostedInternet Relay Chat (IRC) since 2004, which are released into public domain.\n\nGovernment and legal texts are often published directly into the public domain or under openlicenses. For example, in the US, text written by federal government employees is considered to be inthe public domain. We therefore include all plain-text documents made available through the United\n\n4\n\nStates Government Publishing Office (USGPO)’s GovInfo.gov developer API. Additionally, weinclude all plain text regulatory documents published by U.S. federal agencies from Regulations.gov,an online platform that hosts newly proposed rules and regulations from federal agencies. TheCommon Pile also incorporates US Patents and Trademark Office (USPTO) patent documentssourced from the Google Patents Public Data dataset [77], containing millions of public domainpatents and published patent applications dating back to 1782. Similarly, the Hansard (the officialrecord of parliamentary proceedings) of the United Kingdom is distributed under the Open ParliamentLicense, which stipulates similar terms to the CC BY license. We source UK Hansard data fromParlParse [131], covering Commons debates from 1918 forward and Lords proceedings from the1999 reform. For legal text, we leverage the Caselaw Access Project (comprising 40 million pages ofU.S. federal and state court decisions and judges’ opinions from the last 365 years) and Court Listener(including 900 thousand cases scraped from 479 courts). Only legal texts in the public domain wereselected for the Common Pile.\n\nCurated task datasets are typically designed for fine-tuning on specific downstream tasks such asquestion answering, summarization, or text classification. To source datasets that are distributedunder an open license and only contain content owned by the dataset’s rights holder (to avoid licenselaundering), we use metadata and redistributed datasets from the Data Provenance Initiative [106, 109].Full details on the datasets we include are available in Appendix D.\n\nBooks, particularly historic text, can fall into the public domain due to copyright expiration—forexample, in the United States, books published prior to 1929 are currently in the public domain. Wesource public domain books from various sources, including the Biodiversity Heritage Library (BHL),an open-access digital library for biodiversity literature and archives; pre-1929 books digitized by theInternet Archive on behalf of HathiTrust member libraries; the collection of public domain bookscalled “Selected Digitized Books” released by the Library of Congress; and select books from ProjectGutenberg, an online collection of over 75,000 digitized books, most of which are in the publicdomain.\n\nOpen Educational Resources (OERs) are educational materials (e.g. textbooks, lecture notes, lessonplans, etc.), typically published under Creative Commons licenses that support free and equitableaccess to education. We collect data from multiple OER repositories, including the Directory ofOpen Access Books (DOAB), an online index of over 94,000 peer-reviewed books curated fromtrusted open-access publishers; PressBooks, a searchable catalog of over 8,000 open access books;OERCommons, an online platform where educators share open-access instructional materials; andLibreTexts, a catalog of over 3,000 open-access textbooks.\n\nWikis are topic- or domain-specific encyclopedic websites that are collaboratively written, maintained,and moderated. Historical and cultural precedent has led many wikis to have an open license. Wedownloaded the official database dumps of wikitext (Mediawiki’s custom markup language) ofthe English-language wikis that are directly managed by the Wikimedia foundation and convertedwikitext to plain text using wtf_wikipedia. For wikis not managed by Wikimedia, we make use ofwikiteam’s unofficial database dumps and apply the same conversion process.\n\nSource code has proven to be a useful part of LLM pre-training corpora, not only to support codingabilities but also to improve reasoning [7, 113, 120]. Due to the Free and Open Source Software(FOSS) movement, a great deal of source code is distributed with an open license. We leverageprior work done by the Software Heritage Foundation and BigCode to compile the openly licensedsubset of the Stack V2 [113], based on the license detection performed by the creators of StackV2. Additionally we collected all Python Enhancement Proposals (PEPs)—design documents thatgenerally provide a technical specification and rationale for new features of the Python programminglanguage—that were released into the public domain.\n\nYouTube allows users to upload content under a CC BY license. We therefore sourced and transcribedspeech-heavy CC BY videos from YouTube. To avoid license laundering and focus on high-qualityspeech-based textual content, we manually curated a set of over 2,000 YouTube channels that releaseoriginal openly licensed content containing speech. From these channels, we retrieved and transcribed(using Whisper [138]) over 1.1 million openly licensed videos comprising more than 470,000 hoursof content.\n\nWeb text is a common source of LLM pre-training data. A small fraction of content on the webis distributed under open licenses. To recover a portion of this content, we process 52 Common\n\n5\n\nCrawl snapshots using a regular expression (regex) adapted from the C4Corpus project [68] to retainpages that include a CC BY, CC BY-SA, or CC0 marker. This regex naturally results in many falsepositives (e.g., it would retain a page that included and provided attribution for a CC BY image butotherwise contained unlicensed content), so we manually verified the top 1000 domains by contentvolume, retaining only those for which all content was assigned a Creative Commons license. Textwas extracted using a pipeline similar to the one used for Dolma [167]. We provide more detailson the composition of our web-sourced text, called CCCC, in Appendix G. Apart from CCCC,we additionally manually collected data from a few select sites, including Foodista, a community-maintained site with recipes and food-related news as well as nutrition information; news sites thatpublish content under CC BY or CC BY-SA according to Open Newswire; and the Public DomainReview, an online journal dedicated to exploration of works of art and literature that have aged intothe public domain.\n\n4Assessing the Common Pile v0.1’s quality\n\nThe utility of an LLM pre-training dataset is mostly assessed in terms of whether or not it can beused to train performant LLMs. To validate our efforts in curating the Common Pile, we use it asthe basis of an LLM pre-training dataset created through additional filtering (subsection 4.1) andrebalancing (subsection 4.2). Then, we perform a controlled data ablation study (subsection 4.3)where we train otherwise-identical LLMs on different pre-training datasets, including prior datasetscomprised of openly licensed text mentioned in subsection 2.2 as well as a selection of representativepre-training datasets of unlicensed text. Finally, we train Comma v0.1-1T and Comma v0.1-2T, 7billion parameter LLMs trained on 1 and 2 trillion tokens (respectively) of Common Pile-sourcedcontent, and compare them to models with a similar parameter count and training budget that weretrained on unlicensed text (subsection 4.4).\n\n4.1Dataset preprocessing and filtering\n\nBefore training a language model, it is considered important to “clean” data in hopes of retaining onlyhigh-quality text under some notion of quality [4, 105]. Consequently, before training on data fromthe Common Pile (which is distributed in a relatively “raw” format), we independently preprocessedeach of the Common Pile’s non-code datasets using pipelines implemented with the Dolma dataprocessing toolkit [167].\n\nSince the Common Pile v0.1 focuses primarily on English content, we apply language identificationusing a FastText classifier [81] to filter out non-English text. When processing web text fromCCCC, we employ the text quality classifier adapted from DataComp-LM [99] with an extremelylow threshold to remove noisy text. We remove documents with pervasive OCR errors using thelikelihood-based filtering approach from [166], which removes documents that are assigned anexcessively low log-likelihood under a unigram language model constructed from the Trillion WordCorpus [117]. To reduce the prevalence of toxic or inappropriate content, we apply a pair of FastTexttoxicity classifiers implemented in Dolma [167] that were trained on the Jigsaw Toxic CommentClassification Challenge dataset [30]. We apply regex-based personally identifiable information(PII) redaction to remove email addresses, phone numbers, and IP addresses, and replace themwith <EMAIL_ADDRESS>, <PHONE_NUMBER>, and <IP_ADDRESS> respectively. Finally, we performsource-specific regex filtering to remove repetitive or boilerplate text (e.g., page numbers, documentpreambles, license statements, etc.). For a detailed breakdown of the pre-processing applied to eachdataset, see Table 5 (appendix).\n\nAfter filtering, we perform global document-level fuzzy deduplication across all sources, as excessivedata duplication is known to harm language modeling performance [94] and increase memorization[83]. We use the bloom filter-based deduplication functionality from Dolma [167] and deem twodocuments duplicates if they share more than 90% of their 20-grams.\n\nFor code data from the Stack v2, we apply the Red Pajama V1 [185] code filtering heuristics.These include filters based on the mean and maximum line length in a document, the proportion ofalphanumeric characters, and the ratio of alphabetical characters to tokens. After this initial filter, weadopt the process used by SmolLM2 [5] where we keep only code in Python, C, C++, SQL, Java,PHP, Rust, Javascript, Typescript, Go, Ruby, Markdown, C#, Swift, or shell and filter this set usinglanguage-specific quality classifiers to retain only educational and well-documented code. We use\n\n6\n\nFigure 2: The Common Pile consistently outperforms other openly licensed corpora as a pre-training dataset. Following the setup from Penedo et al. [132], we train and evaluate 1.7B parametermodels on 28B tokens of data from each dataset. Stars denote benchmarks on which the modeltrained using the Common Pile outperforms all other models.\n\na lower threshold to filter out low-quality code than was used for SmolLM2, resulting in a largerset of post-filtered text. Finally, we extract plaintext from HTML documents in the Stack V2 usingTrafilatura [10] and apply our standard plaintext filtering pipeline including language, length, toxicity,and PII filtering.\n\n4.2Data mixing\n\nRecent work [3, 178, 189] has shown that up- or down-weighting pre-training data sources inaccordance with some notion of data quality can produce more performant models. Indeed, thesources in the Common Pile vary drastically in their characteristics, and we don’t necessarily expectthat our largest sources contain the highest quality text. For example, patent text sourced fromthe USPTO (our second-largest source) exhibits substantially different wording, terminology, andrepetition than typical natural language. Consequently, we anticipate that appropriately mixing thesources in the Common Pile (rather than simply combining all sources, i.e., mixing in proportion tosource size) is of particular importance. Additionally, while LLM pre-training datasets have beencontinuously scaled to avoid the diminishing returns that result from repeating data [94], recent workhas highlighted that repeating high-quality data can be preferable to avoiding repetition by trainingon low-quality data [120, 52].\n\nTo determine mixing weights, we first trained per-source language models using the procedureoutlined in subsection 4.3 below for 28 billion tokens on all sources that were sufficiently large tobe repeated less than four times at this data budget. Based on the performance of these per-sourcemodels, we heuristically set mixing weights to up- and down-weight high- and low-performancesources respectively while targeting a maximum of six repetitions over the course of a 1 trillion tokentraining run. Additionally, we assumed that our smaller sources were high quality and set their mixingrates such that they were also repeated six times over the course of 1 trillion tokens. The resultingmixture and per-source repetition rates are given in Table 7 (appendix). We also experimented withusing MixMin [178] to automatically determine mixing weights but found that it did not improveover our heuristically determined mixture.\n\nBecause we use this dataset mixture to train the Comma v0.1 models (subsection 4.4) and becauseit comprises a heavily filtered and remixed version of the Common Pile v0.1, we refer to it as the“Comma dataset” to distinguish it from the Common Pile itself.\n\n4.3Controlled dataset quality experiments\n\nAs a preliminary measure of the Common Pile’s quality, we adopt the experimental setting of Penedoet al. [132] and identically train models on the Comma dataset and various preexisting datasets. Byusing a controlled setting across datasets, we can assert that differences in model performance stemprimarily from the quality of each dataset. Specifically, we train 1.7 billion parameter decoder-onlyTransformer models [182] that follow the Llama architecture [179] on 28 billion tokens of data fromeach dataset, tokenized using the GPT-2 tokenizer [137]. We follow the hyperparameters and setup\n\n7\n\nof Penedo et al. [132] exactly, except that we used a weight decay of 0.2 instead of 0.1 due to slightlyimproved performance (possibly due to the large amount of repetition in the Comma dataset).\n\nEach model was then evaluated using the set of “early signal” tasks identified by Penedo et al. [132]which cover commonsense reasoning and knowledge capabilities; specifically, we evaluate zero-shotperformance on ARC [33], MMLU [70], HellaSwag (HSwag) [192], OpenBookQA (OBQA) [118],CommonSenseQA (CSQA) [171], PIQA [19], and SocialIQA (SIQA) [160]. We omit Winogrande(which was used in Penedo et al. [132]) because it is included in the set of datasets we sourced fromthe Data Provenance Initiative; consequently all of the tasks we evaluate on are “unseen” by allmodels. We highlight that a significant portion of the Comma dataset is code, but none of the taskswe evaluate on measure code capabilities. While it is possible that we could improve performanceby omitting code data in this setting, we retained code for reliable reporting of the Comma dataset’sperformance.\n\nAs baselines, we compare to the prior datasets that aim to provide open licensed text discussed insubsection 2.2: OLC [119], Common Corpus [91], and KL3M [78]. We additionally compare to thePile, as it one of the only LLM pre-training datasets that contains a comparable number of diversesources to the Common Pile (22 vs. 30). Finally, we report the performance of two web text-basedunlicensed pre-training datasets: OSCAR [169], which incorporates relatively little filtering; andFineWeb [132], an recent dataset that reflects current best practice for LLM pre-training datasetcuration.\n\nThe resulting performance of each model is shown in Figure 2, with detailed results in Table 9(appendix). Notably, the Comma dataset-based model outperforms the models trained OLC, CommonCorpus, and KL3M across all benchmarks and outperforms the Pile-based model on all but twobenchmarks. While the performance of the FineWeb-based model is the best on most benchmarks,the Comma dataset-based model performs best on the scientific and scholarly knowledge-basedbenchmarks MMLU and ARC, possibly due to the Common Pile’s large proportion of domain-relevant text. On the other hand, on the commonsense reasoning datasets HellaSwag, PIQA, andCommonSenseQA, the model trained on the Comma dataset has significantly worse performancethan models trained on the Pile, OSCAR, and FineWeb, possibly indicating a lack of relevant datain the Common Pile. We additionally note that recent work [188] highlights that performance onHellaSwag is most heavily influenced by coverage of certain domains and topics such as personalblogs, tutorials, hobbies, and sports, which are poorly represented in the Common Pile. Overall,these findings confirm that the Comma dataset performs best among datasets that aim to contain onlyopenly licensed data and is also a strong candidate in general, particularly when targeting scientificand scholarly applications.\n\nWe note that the Comma dataset is the only dataset we evaluate that explicitly includes task-like datadue to inclusion of data from the Data Provenance Initiative (DPI). To verify that this does not conferan unfair advantage, we trained an additional model on the Comma dataset with all sources retainedexcept for the DPI-sourced data. Removing this source had a minimal impact on model performance(full results in Table 9), with a notable decrease only on HellaSwag, possibly suggesting that the DPIdata contains domain-relevant data for this benchmark that other sources lack.\n\n4.4Comma v0.1\n\nHaving established that Comma’s dataset produces models with competitive performance whencompared to other datasets, we now validate our efforts at larger, more realistic scales. Specifically,we train Comma v0.1-1T and Comma v0.1-2T, a pair of 7 billion-parameter LLMs trained on 1 and 2trillion tokens of text respectively, and compare with other models trained using similar computationalbudgets.\n\nTokenizationWhile training a tokenizer on unlicensed text is less likely to raise ethical or IP-relatedissues than training an LLM, we nevertheless trained a custom tokenizer on the Comma dataset toensure that our entire modeling pipeline was based on openly licensed data. In addition, the differentcharacteristics of our dataset likely makes existing tokenizers (which are often trained on web text)suboptimal. We therefore trained a BPE-based [55] tokenizer using the Hugging Face tokenizerslibrary using a vocabulary size of 64,000. We follow the same splitting regex as Llama 3.2 [62] andthe Hugging Face ByteLevel preprocessor; no Unicode normalization was used. The tokenizer wastrained on a 600GB sample [150] of text from the Comma dataset.\n\n8\n\nARC-CARC-EMMLUBoolQHSwagOBQACSQAPIQASIQAHumEvalMBPP0\n\n20\n\n40\n\n60\n\n80\n\n100\n\nPerformance\n\nKnowledge/ReasoningCoding\n\nComma v0.1-1TLLaMAMPTRPJ-INCITEQwen3\n\nFigure 3: Compared to models trained with similar resources (7 billion parameters, 1 trilliontokens), Comma v0.1-1T is the strongest model on several standard benchmarks. To contextual-ize these results, we include Qwen3 8B (trained on 36 trillion tokens) as a “current best-practices”upper bound. Stars denote benchmarks on which Comma v0.1-1T outperforms all other compute-matched models (i.e., all models other than Qwen3). Full numerical results are provided in Table 10(appendix).\n\nTraining setupWe trained Comma v0.1-1T and -2T using the lingua framework [183]. We baseour model architecture and training hyperparameters on lingua’s Llama-7B configuration, whichclosely follows the conventions set by the Llama series of models [179, 62]. For Comma v0.1-1T, wetrained with an effective batch size of 512 length-4096 sequences using the AdamW [111] optimizerand a weight decay of 0.2. For the Comma v0.1 variant trained on 2 trillion tokens, we increased thebatch size to 2048 length-4096 sequences.\n\nWe performed two stage training, with a first stage following a cosine learning rate schedule and thesecond stage being a ‘cool-down” [73], where we train only on a subset of high-quality sources usingthe mixing weights provided in Table 8 (appendix) while decaying the learning rate linearly to 0.Comma v0.1-1T had a first stage of 460,000 steps with 2,000 steps of warmup, an initial learningrate of 1e−3, a minimum learning rate of 1e−9, a cosine schedule period of 500,000 steps, and18,000 steps of decay. For Comma v0.1-2T, the first stage instead had 230,000 steps with a maximumand minimum learning rate of 2e−3 and 2e−9 respectively and a period of 250,000 steps, with9,000 steps of decay in the second stage. For both models, we average together ten evenly spacedcheckpoints from the cool-down phase to produce a final model as suggested by Grattafiori et al.[62]. Apart from our main Comma v0.1-1T and -2T training runs, we completed several additionalruns to better understand how hyper-parameters impact the model, including using a different batchsize and following a three-stage (rather than two-stage) curriculum. Overall, the results of these runswere consistent with the findings from our main training runs. Additional details can be found inAppendix O.\n\nEvaluationWe evaluate the Comma v0.1 models on the suite of benchmarks used by Groeneveldet al. [64] in addition to two additional code benchmarks. Specifically, we evaluate models onARC [33], MMLU [70], BoolQ [31], HellaSwag [192], OpenBookQA [118], CommonsenseQA [171],PIQA [19], and SIQA [160] to probe world knowledge and reasoning and HumanEval [25] andMBPP [8] to evaluate coding capabilities. Following Groeneveld et al. [64], we evaluate usingOLMES [65], using a zero-shot format for all tasks except MMLU, which uses a 5-shot format. Forthe coding tasks, we report pass@10 accuracies.\n\nBaseline modelsFor fairness, we primarily compare to prior models with the same parameter countand token budget. Since we are not aware of any such models trained on openly licensed data, wecompare only to models trained on unlicensed data. For Comma v0.1-1T, we compare to Llama 17B [179], MPT-7B [175], RPJ-INCITE-7B [185], StableLM-7B [12], and OpenLLaMA-7B [58]. Forthe two trillion token variant, we compare to OLMo Twin (specifically OLMo-7B-Twin-2T) [64],Llama 2 7B [180], and DeepSeekLLM [16]. Over time, the token budgets of open pre-trained LLMshave continually grown [143], and current standard practice is to pretrain on significantly more than1 or 2 trillion tokens. Consequently, recent models tend to outperform our baselines, which werereleased in 2023 and 2024. To provide a state-of-the-art point of reference, we additionally includeresults for the recently released Qwen3 8B [176], which was trained for 36 trillion tokens. Weemphasize that we cannot reliably compare to a model with a 36× or 18× larger training budget andwe primarily include it as a point of reference.\n\n9\n\nARC-CARC-EMMLUBoolQHSwagOBQACSQAPIQASIQAHumEvalMBPP0\n\n20\n\n40\n\n60\n\n80\n\n100\n\nPerformance\n\nKnowledge/ReasoningCoding\n\nComma v0.1-2TOLMo TwinLLaMA 2DeepSeekLLMQwen3\n\nFigure 4: Comma v0.1-2T is also competitive with budget-matched models (7 billion parameters,2 trillion tokens) trained on unlicensed data. We additionally include Qwen3 8B as a higher budgetupper bound. Stars denote benchmarks where Comma v0.1-2T outperforms budget-matched models.Full numerical results are provided in Table 11 (appendix).\n\nResultsAs shown in Figure 3, Comma v0.1-1T outperforms budget-matched baseline models onover half of the benchmarks tested. In line with our results from subsection 4.3, we observe thatComma v0.1-1T excels on knowledge-based benchmarks like ARC-C and MMLU, but lags behindon HellaSwag and PIQA. Comma v0.1-1T is also particularly strong at code-related tasks where itoutperforms baseline models by a wide margin. Comparisons to StableLM and OpenLLama can befound in Table 10 (appendix), but show similar trends.\n\nOur promising results from training on 1 trillion tokens motivated us to experiment with longertraining durations. To test whether the filtered dataset supports training durations beyond 1T, wetrained Comma v0.1-2T simply by repeating the same data mixture used for Comma v0.1-1Tapproximately twice. We note that this pre-training mixture involves repeating certain sources anexcessive number of times (up to 16 passes for some sources). Prior work suggests that these extremelevels of data repetition may result in diminishing returns [121]. However, these experiments stillgive us a preliminary picture of the performance achievable under a larger budget.\n\nThe performance of Comma v0.1-2T is compared with budget-matched models in Figure 4. Notably,we find that Comma v0.1-2T is competitive with OLMo, Llama 2, and DeepSeekLLM, with especiallystrong performance on MMLU, SIQA, ARC-E, and the coding tasks. We emphasize that the Commav0.1-2T result here is likely not a best-case 2T-token run using the Common Pile v0.1 due toexcessive repetition, and better performance could likely be attained through a 2T-specific mixtureand curriculum. Nevertheless, this result highlights the promise of larger scale training runs based onthe Common Pile. Qwen3 8B’s superior performance across most benchmarks confirms the benefitof larger training budgets and motivates future efforts on scaling up the Common Pile.\n\n5Conclusion\n\nWe release Common Pile v0.1, an 8TB corpus that—to our knowledge—constitutes the largest datasetbuilt exclusively from openly licensed text. Alongside our dataset, we release Comma v0.1-1T and-2T, two performant 7-billion-parameter LLMs trained on text from the Common Pile, as well asthe filtered and rebalanced data mixture we used for training. Our results demonstrate that not onlyis the Common Pile the strongest dataset for pretraining under an open-license constraint, but alsothat it produces models comparable to those trained on an equivalent amount of unlicensed data.This positive result holds promise for future of open-license pretraining, especially if the researchcommunity invests in collecting larger quantities of openly licensed text data in the future. Ultimately,we believe that the Common Pile v0.1 represents the first step on the path towards a more ethicallanguage model ecosystem, where performance need not come at the cost of creator rights and legaltransparency.\n\nAcknowledgments\n\nWe thank Chris Maddison, Anvith Thudi, Pierre-Carl Langlais, Alec Radford, Adam Roberts, SewonMin, and Weijia Shi for fruitful discussions and constructive feedback. An early draft of this work\n\n10\n\nwas shared at the Dataset Convening hosted by the Mozilla Foundation and EleutherAI. We thank theparticipants for their discussion and feedback.\n\nThis work was supported by funding from the Mozilla Foundation and Sutter Hill Ventures. Weacknowledge the support of the Natural Sciences and Engineering Research Council of Canada(NSERC). Researchers funded through the NSERC-CSE Research Communities Grants do notrepresent the Communications Security Establishment Canada or the Government of Canada. Anyresearch, opinions or positions they produce as part of this initiative do not represent the officialviews of the Government of Canada.\n\nParts of this work were performed under the auspices of the U.S. Department of Energy by LawrenceLivermore National Laboratory under Contract DE-AC52-07NA27344 and was supported by theLLNL-LDRD Program under Project No. 24-ERD-010 and Project No. 24-SI-008 (LLNL-CONF-2006420).\n\nReferences\n\n[1] 17 U.S. Code § 102. Subject matter of copyright: In general, December 1990. URL https://www.law.cornell.edu/uscode/text/17/102.\n\n[2] 17 U.S. Code § 105. Subject matter of copyright: United States Government works, December2024. URL https://www.law.cornell.edu/uscode/text/17/105.\n\n[3] Alon Albalak, Liangming Pan, Colin Raffel, and William Yang Wang. Efficient online datamixing for language model pre-training. arXiv preprint arXiv:2312.02406, 2023.\n\n[4] Alon Albalak, Yanai Elazar, Sang Michael Xie, Shayne Longpre, Nathan Lambert, XinyiWang, Niklas Muennighoff, Bairu Hou, Liangming Pan, Haewon Jeong, et al. A survey ondata selection for language models. Transactions on Machine Learning Research, 2024.\n\n[5] Loubna Ben Allal, Anton Lozhkov, Elie Bakouch, Gabriel Martín Blázquez, GuilhermePenedo, Lewis Tunstall, Andrés Marafioti, Hynek Kydlíˇcek, Agustín Piqueres Lajarín, VaibhavSrivastav, Joshua Lochner, Caleb Fahlgren, Xuan-Son Nguyen, Clémentine Fourrier, BenBurtenshaw, Hugo Larcher, Haojun Zhao, Cyril Zakka, Mathieu Morlon, Colin Raffel, Leandrovon Werra, and Thomas Wolf. Smollm2: When smol goes big – data-centric training of asmall language model, 2025. URL https://arxiv.org/abs/2502.02737.\n\n[6] Mikel Artetxe, Sebastian Ruder, and Dani Yogatama. On the cross-lingual transferabilityof monolingual representations. In Annual Meeting of the Association for ComputationalLinguistics, pages 4623–4637, 2019.\n\n[7] Viraat Aryabumi, Yixuan Su, Raymond Ma, Adrien Morisot, Ivan Zhang, Acyr Locatelli,Marzieh Fadaee, Ahmet Üstün, and Sara Hooker. To code, or not to code? exploring impact ofcode in pre-training. arXiv preprint arXiv:2408.10914, 2024.\n\n[8] Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, DavidDohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with largelanguage models. arXiv preprint arXiv:2108.07732, 2021.\n\n[9] Stefan Baack, Stella Biderman, Kasia Odrozek, Aviya Skowron, Ayah Bdeir, Jillian Bom-marito, Jennifer Ding, Maximilian Gahntz, Paul Keller, Pierre-Carl Langlais, Greg Lindahl,Sebastian Majstorovic, Nik Marda, Guilherme Penedo, Maarten Van Segbroeck, Jennifer Wang,Leandro von Werra, Mitchell Baker, Julie Belião, Kasia Chmielinski, Marzieh Fadaee, LisaGutermuth, Hynek Kydlíˇcek, Greg Leppert, EM Lewis-Jong, Solana Larsen, Shayne Long-pre, Angela Oduor Lungati, Cullen Miller, Victor Miller, Max Ryabinin, Kathleen Siminyu,Andrew Strait, Mark Surman, Anna Tumadóttir, Maurice Weber, Rebecca Weiss, Lee White,and Thomas Wolf. Towards best practices for open datasets for llm training, 2025. URLhttps://arxiv.org/abs/2501.08365.\n\n[10] Adrien Barbaresi. Trafilatura: A Web Scraping Library and Command-Line Tool for TextDiscovery and Extraction. In Proceedings of the Joint Conference of the 59th Annual Meetingof the Association for Computational Linguistics and the 11th International Joint Conferenceon Natural Language Processing: System Demonstrations, pages 122–131. Association for\n\n11\n\nComputational Linguistics, 2021. URL https://aclanthology.org/2021.acl-demo.15.\n\n[11] Max Bartolo, A. Roberts, Johannes Welbl, Sebastian Riedel, and Pontus Stenetorp. Beat theai: Investigating adversarial human annotation for reading comprehension. Transactions of theAssociation for Computational Linguistics, 8:662–678, 2020.\n\n[12] Marco Bellagente, Jonathan Tow, Dakota Mahan, Duy Phung, Maksym Zhuravinskyi, ReshinthAdithyan, James Baicoianu, Ben Brooks, Nathan Cooper, Ashish Datta, et al. Stable lm 2 1.6b technical report. arXiv preprint arXiv:2402.17834, 2024.\n\n[13] Jonathan Berant, A. Chou, Roy Frostig, and Percy Liang. Semantic parsing on freebase fromquestion-answer pairs. In Conference on Empirical Methods in Natural Language Processing,pages 1533–1544, 2013.\n\n[14] Janek Bevendorff, Benno Stein, Matthias Hagen, and Martin Potthast. Elastic ChatNoir:Search Engine for the ClueWeb and the Common Crawl. In Leif Azzopardi, Allan Hanbury,Gabriella Pasi, and Benjamin Piwowarski, editors, Advances in Information Retrieval. 40thEuropean Conference on IR Research (ECIR 2018), Lecture Notes in Computer Science, BerlinHeidelberg New York, March 2018. Springer.\n\n[15] Janek Bevendorff, Martin Potthast, and Benno Stein. FastWARC: Optimizing Large-ScaleWeb Archive Analytics. In Andreas Wagner, Christian Guetl, Michael Granitzer, and StefanVoigt, editors, 3rd International Symposium on Open Search Technology (OSSYM 2021).International Open Search Symposium, October 2021.\n\n[16] Xiao Bi, Deli Chen, Guanting Chen, Shanhuang Chen, Damai Dai, Chengqi Deng, HonghuiDing, Kai Dong, Qiushi Du, Zhe Fu, et al. Deepseek llm: Scaling open-source languagemodels with longtermism. arXiv preprint arXiv:2401.02954, 2024.\n\n[17] Stella Biderman, Usvsn Prashanth, Lintang Sutawika, Hailey Schoelkopf, Quentin Anthony,Shivanshu Purohit, and Edward Raff. Emergent and predictable memorization in large languagemodels. Advances in Neural Information Processing Systems, 36:28072–28090, 2023.\n\n[18] Stella Biderman, Hailey Schoelkopf, Quentin Anthony, Herbie Bradley, Kyle O’Brien, EricHallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff,Aviya Skowron, Lintang Sutawika, and Oskar van der Wal. Pythia: A suite for analyzing largelanguage models across training and scaling, 2023. URL https://arxiv.org/abs/2304.01373.\n\n[19] Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, and Yejin Choi. Piqa: Reasoningabout physical commonsense in natural language, 2019. URL https://arxiv.org/abs/1911.11641.\n\n[20] Blue Oak Council. License List (version 15), 2025. URL https://blueoakcouncil.org/list.\n\n[21] Oana-Maria Camburu, Tim Rocktäschel, Thomas Lukasiewicz, and Phil Blunsom. e-snli: Nat-ural language inference with natural language explanations. In Neural Information ProcessingSystems, pages 9560–9572, 2018.\n\n[22] Nicholas Carlini, Daphne Ippolito, Matthew Jagielski, Katherine Lee, Florian Tramer, andChiyuan Zhang. Quantifying memorization across neural language models. In The EleventhInternational Conference on Learning Representations, 2022.\n\n[23] Ilias Chalkidis, Abhik Jana, D. Hartung, M. Bommarito, Ion Androutsopoulos, D. Katz, andNikolaos Aletras. Lexglue: A benchmark dataset for legal language understanding in english.In Annual Meeting of the Association for Computational Linguistics, pages 4310–4330, 2021.\n\n[24] Chat GPT Is Eating the World, 2024. URL https://chatgptiseatingtheworld.com.\n\n12\n\n[25] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto,Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, RaulPuri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, BrookeChan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, MohammadBavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, MatthiasPlappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, AlexNichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain,William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra,Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer,Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and WojciechZaremba. Evaluating large language models trained on code, 2021.\n\n[26] Wenhu Chen, Hanwen Zha, Zhiyu Chen, Wenhan Xiong, Hong Wang, and William YangWang. HybridQA: A dataset of multi-hop question answering over tabular and textual data. InTrevor Cohn, Yulan He, and Yang Liu, editors, Findings of the Association for ComputationalLinguistics: EMNLP 2020, pages 1026–1036, Online, November 2020. Association forComputational Linguistics.doi: 10.18653/v1/2020.findings-emnlp.91.URL https://aclanthology.org/2020.findings-emnlp.91/.\n\n[27] Zhiyu Chen, Wenhu Chen, Hanwen Zha, Xiyou Zhou, Yunkai Zhang, Sairam Sundaresan, andWilliam Yang Wang. Logic2text: High-fidelity natural language generation from logical forms.ArXiv, abs/2004.14579, 2020.\n\n[28] Sang Keun Choe, Hwijeen Ahn, Juhan Bae, Kewen Zhao, Minsoo Kang, Youngseog Chung,Adithya Pratapa, Willie Neiswanger, Emma Strubell, Teruko Mitamura, Jeff Schneider, EduardHovy, Roger Grosse, and Eric Xing. What is your data worth to gpt? llm-scale data valuationwith influence functions, 2024. URL https://arxiv.org/abs/2405.13954.\n\n[29] Eunsol Choi, He He, Mohit Iyyer, Mark Yatskar, Wen tau Yih, Yejin Choi, Percy Liang, andLuke Zettlemoyer. Quac: Question answering in context. In Conference on Empirical Methodsin Natural Language Processing, pages 2174–2184, 2018.\n\n[30] cjadams, Jeffrey Sorensen, Julia Elliott, Lucas Dixon, Mark McDonald, nithum, and WillCukierski. Toxic comment classification challenge. https://kaggle.com/competitions/jigsaw-toxic-comment-classification-challenge, 2017. Kaggle.\n\n[31] Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, andKristina Toutanova. Boolq: Exploring the surprising difficulty of natural yes/no questions.arXiv preprint arXiv:1905.10044, 2019.\n\n[32] Kevin Clark, Urvashi Khandelwal, Omer Levy, and Christopher D Manning. What doesBERT look at? an analysis of BERT’s attention. In Proceedings of the 2019 ACL WorkshopBlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, 2019.\n\n[33] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick,and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoningchallenge. arXiv preprint arXiv:1803.05457, 2018.\n\n[34] Karl Cobbe, V. Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser,Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and JohnSchulman. Training verifiers to solve math word problems. ArXiv, abs/2110.14168, 2021.\n\n[35] Creative Commons. CC0 1.0 Universal (CC0 1.0) Public Domain Dedication, 2025. URL\n\nhttps://creativecommons.org/publicdomain/zero/1.0/.\n\n[36] Creative Commons. Creative Commons Attribution 4.0 International License § 2(a)(1)(A),2025. URL https://creativecommons.org/licenses/by/4.0/legalcode.\n\n[37] Creative Commons. Creative Commons Attribution 4.0 International License § 3(a)(1)(A)(i),2025. URL https://creativecommons.org/licenses/by/4.0/legalcode.\n\n[38] Creative Commons. Public Domain Mark 1.0, 2025. URL https://creativecommons.org/publicdomain/mark/1.\n\n13\n\n[39] Creative Commons. Creative Commons Attribution-ShareAlike 4.0 International License,2025. URL https://creativecommons.org/licenses/by-sa/4.0/.\n\n[40] A. Feder Cooper and James Grimmelmann. The Files are in the Computer: Copyright,Memorization, and Generative AI. arXiv preprint arXiv:2404.12590, 2024.\n\n[41] A. Feder Cooper, Aaron Gokaslan, Amy B. Cyphert, Christopher De Sa, Mark A. Lemley,Daniel E. Ho, and Percy Liang. Extracting memorized pieces of (copyrighted) books fromopen-weight language models. arXiv preprint arXiv:2505.12546, 2025.\n\n[42] Yiming Cui, Ting Liu, Li Xiao, Zhipeng Chen, Wentao Ma, Wanxiang Che, Shijin Wang,and Guoping Hu. A span-extraction dataset for chinese machine reading comprehension. InEMNLP-IJCNLP, pages 5882–5888, 2019.\n\n[43] Bhavana Dalvi, Lifu Huang, Niket Tandon, Wen tau Yih, and Peter Clark. Tracking statechanges in procedural text: a challenge dataset and models for process paragraph comprehen-sion. In North American Chapter of the Association for Computational Linguistics, pages1595–1604, 2018.\n\n[44] Pradeep Dasigi, Nelson F. Liu, Ana Marasovi´c, Noah A. Smith, and Matt Gardner. Quoref: Areading comprehension dataset with questions requiring coreferential reasoning. In Conferenceon Empirical Methods in Natural Language Processing, volume abs/1908.05803, 2019.\n\n[45] Ning Ding, Guangwei Xu, Yulin Chen, Xiaobin Wang, Xu Han, Pengjun Xie, HaitaoZheng, and Zhiyuan Liu. Few-nerd: A few-shot named entity recognition dataset. ArXiv,abs/2105.07464, 2021.\n\n[46] Dheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, and MattGardner. Drop: A reading comprehension benchmark requiring discrete reasoning overparagraphs. In North American Chapter of the Association for Computational Linguistics,pages 2368–2378, 2019.\n\n[47] Michael Duan, Anshuman Suri, Niloofar Mireshghallah, Sewon Min, Weijia Shi, Luke Zettle-moyer, Yulia Tsvetkov, Yejin Choi, David Evans, and Hannaneh Hajishirzi. Do membershipinference attacks work on large language models? arXiv preprint arXiv:2402.07841, 2024.\n\n[48] S. Dumitrescu, Petru Rebeja, Beáta L˝orincz, Mihaela G˘aman, M. Ilie, Andrei Pruteanu,Adriana Stan, Luciana Morogan, Traian Rebedea, and Sebastian Ruder. Liro: Benchmark andleaderboard for romanian language tasks. In NeurIPS Datasets and Benchmarks, 2021.\n\n[49] Yanai Elazar and Yoav Goldberg. Where’s my head? definition, data set, and models fornumeric fused-head identification and resolution. Transactions of the Association for Compu-tational Linguistics, 7:519–535, 2019.\n\n[50] Yanai Elazar, Nora Kassner, Shauli Ravfogel, Amir Feder, Abhilasha Ravichander, MariusMosbach, Yonatan Belinkov, Hinrich Schütze, and Yoav Goldberg. Measuring causal effectsof data statistics on language model’sfactual’predictions. arXiv preprint arXiv:2207.14251,2022.\n\n[51] Denis Emelin, Ronan Le Bras, Jena D. Hwang, Maxwell Forbes, and Yejin Choi. Moralstories: Situated reasoning about norms, intents, actions, and their consequences. ArXiv,abs/2012.15738, 2020.\n\n[52] Alex Fang, Hadi Pouransari, Matt Jordan, Alexander Toshev, Vaishaal Shankar, LudwigSchmidt, and Tom Gunter. Datasets, documents, and repetitions: The practicalities of unequaldata quality. arXiv preprint arXiv:2503.07879, 2025.\n\n[53] James Ferguson, Matt Gardner, Tushar Khot, and Pradeep Dasigi. Iirc: A dataset of incompleteinformation reading comprehension questions. In Conference on Empirical Methods in NaturalLanguage Processing, pages 1137–1147, 2020.\n\n[54] Nancy Fulda, Nathan Tibbetts, Zachary Brown, and D. Wingate. Harvesting common-sensenavigational knowledge for robotics from uncurated text corpora. In Conference on RobotLearning, pages 525–534, 2017.\n\n14\n\n[55] Philip Gage. A new algorithm for data compression. The C Users Journal archive, 12:23–38,1994. URL https://api.semanticscholar.org/CorpusID:59804030.\n\n[56] N. Gale, G. Heath, E. Cameron, S. Rashid, and S. Redwood. Using the framework method forthe analysis of qualitative data in multi-disciplinary health research. BMC Medical ResearchMethodology, 13:117 – 117, 2013.\n\n[57] Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster,Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy.The pile: An 800gb dataset of diverse text for language modeling, 2020. URL https://arxiv.org/abs/2101.00027.\n\n[58] Xinyang Geng and Hao Liu. Openllama: An open reproduction of llama, May 2023. URL\n\nhttps://github.com/openlm-research/open_llama.\n\n[59] Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, D. Roth, and Jonathan Berant. Didaristotle use a laptop? a question answering benchmark with implicit reasoning strategies.Transactions of the Association for Computational Linguistics, 9:346–361, 2021.\n\n[60] Max Glockner, Vered Shwartz, and Yoav Goldberg. Breaking nli systems with sentences thatrequire simple lexical inferences. ArXiv, abs/1805.02266, 2018.\n\n[61] Aaron Gokaslan, A. Feder Cooper, Jasmine Collins, Landan Seguin, Austin Jacobson, MihirPatel, Jonathan Frankle, Cory Stephenson, and Volodymyr Kuleshov. CommonCanvas: OpenDiffusion Models Trained on Creative-Commons Images. In Proceedings of the IEEE/CVFConference on Computer Vision and Pattern Recognition (CVPR), pages 8250–8260, June2024.\n\n[62] Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian,Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, Amy Yang,Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravanku-mar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, Aurelien Rodriguez, AustenGregerson, Ava Spataru, Baptiste Roziere, Bethany Biron, Binh Tang, Bobbie Chern, Char-lotte Caucheteux, Chaya Nayak, Chloe Bi, Chris Marra, Chris McConnell, Christian Keller,Christophe Touret, Chunyang Wu, Corinne Wong, Cristian Canton Ferrer, Cyrus Nikolaidis,Damien Allonsius, Daniel Song, Danielle Pintz, Danny Livshits, Danny Wyatt, David Esiobu,Dhruv Choudhary, Dhruv Mahajan, Diego Garcia-Olano, Diego Perino, Dieuwke Hupkes,Egor Lakomkin, Ehab AlBadawy, Elina Lobanova, Emily Dinan, Eric Michael Smith, FilipRadenovic, Francisco Guzmán, Frank Zhang, Gabriel Synnaeve, Gabrielle Lee, Georgia LewisAnderson, Govind Thattai, Graeme Nail, Gregoire Mialon, Guan Pang, Guillem Cucurell,Hailey Nguyen, Hannah Korevaar, Hu Xu, Hugo Touvron, Iliyan Zarov, Imanol Arrieta Ibarra,Isabel Kloumann, Ishan Misra, Ivan Evtimov, Jack Zhang, Jade Copet, Jaewon Lee, JanGeffert, Jana Vranes, Jason Park, Jay Mahadeokar, Jeet Shah, Jelmer van der Linde, JenniferBillock, Jenny Hong, Jenya Lee, Jeremy Fu, Jianfeng Chi, Jianyu Huang, Jiawen Liu, JieWang, Jiecao Yu, Joanna Bitton, Joe Spisak, Jongsoo Park, Joseph Rocca, Joshua Johnstun,Joshua Saxe, Junteng Jia, Kalyan Vasuden Alwala, Karthik Prasad, Kartikeya Upasani, KatePlawiak, Ke Li, Kenneth Heafield, Kevin Stone, Khalid El-Arini, Krithika Iyer, Kshitiz Malik,Kuenley Chiu, Kunal Bhalla, Kushal Lakhotia, Lauren Rantala-Yeary, Laurens van der Maaten,Lawrence Chen, Liang Tan, Liz Jenkins, Louis Martin, Lovish Madaan, Lubo Malo, LukasBlecher, Lukas Landzaat, Luke de Oliveira, Madeline Muzzi, Mahesh Pasupuleti, MannatSingh, Manohar Paluri, Marcin Kardas, Maria Tsimpoukelli, Mathew Oldham, Mathieu Rita,Maya Pavlova, Melanie Kambadur, Mike Lewis, Min Si, Mitesh Kumar Singh, Mona Hassan,Naman Goyal, Narjes Torabi, Nikolay Bashlykov, Nikolay Bogoychev, Niladri Chatterji, NingZhang, Olivier Duchenne, Onur Çelebi, Patrick Alrassy, Pengchuan Zhang, Pengwei Li, PetarVasic, Peter Weng, Prajjwal Bhargava, Pratik Dubal, Praveen Krishnan, Punit Singh Koura,Puxin Xu, Qing He, Qingxiao Dong, Ragavan Srinivasan, Raj Ganapathy, Ramon Calderer,Ricardo Silveira Cabral, Robert Stojnic, Roberta Raileanu, Rohan Maheswari, Rohit Gird-har, Rohit Patel, Romain Sauvestre, Ronnie Polidoro, Roshan Sumbaly, Ross Taylor, RuanSilva, Rui Hou, Rui Wang, Saghar Hosseini, Sahana Chennabasappa, Sanjay Singh, SeanBell, Seohyun Sonia Kim, Sergey Edunov, Shaoliang Nie, Sharan Narang, Sharath Raparthy,Sheng Shen, Shengye Wan, Shruti Bhosale, Shun Zhang, Simon Vandenhende, Soumya Batra,\n\n15\n\nSpencer Whitman, Sten Sootla, Stephane Collot, Suchin Gururangan, Sydney Borodinsky,Tamar Herman, Tara Fowler, Tarek Sheasha, Thomas Georgiou, Thomas Scialom, TobiasSpeckbacher, Todor Mihaylov, Tong Xiao, Ujjwal Karn, Vedanuj Goswami, Vibhor Gupta, Vi-gnesh Ramanathan, Viktor Kerkez, Vincent Gonguet, Virginie Do, Vish Vogeti, Vítor Albiero,Vladan Petrovic, Weiwei Chu, Wenhan Xiong, Wenyin Fu, Whitney Meers, Xavier Martinet,Xiaodong Wang, Xiaofang Wang, Xiaoqing Ellen Tan, Xide Xia, Xinfeng Xie, Xuchao Jia,Xuewei Wang, Yaelle Goldschlag, Yashesh Gaur, Yasmine Babaei, Yi Wen, Yiwen Song,Yuchen Zhang, Yue Li, Yuning Mao, Zacharie Delpierre Coudert, Zheng Yan, ZhengxingChen, Zoe Papakipos, Aaditya Singh, Aayushi Srivastava, Abha Jain, Adam Kelsey, AdamShajnfeld, Adithya Gangidi, Adolfo Victoria, Ahuva Goldstand, Ajay Menon, Ajay Sharma,Alex Boesenberg, Alexei Baevski, Allie Feinstein, Amanda Kallet, Amit Sangani, Amos Teo,Anam Yunus, Andrei Lupu, Andres Alvarado, Andrew Caples, Andrew Gu, Andrew Ho,Andrew Poulton, Andrew Ryan, Ankit Ramchandani, Annie Dong, Annie Franco, Anuj Goyal,Aparajita Saraf, Arkabandhu Chowdhury, Ashley Gabriel, Ashwin Bharambe, Assaf Eisenman,Azadeh Yazdan, Beau James, Ben Maurer, Benjamin Leonhardi, Bernie Huang, Beth Loyd,Beto De Paola, Bhargavi Paranjape, Bing Liu, Bo Wu, Boyu Ni, Braden Hancock, Bram Wasti,Brandon Spence, Brani Stojkovic, Brian Gamido, Britt Montalvo, Carl Parker, Carly Burton,Catalina Mejia, Ce Liu, Changhan Wang, Changkyu Kim, Chao Zhou, Chester Hu, Ching-Hsiang Chu, Chris Cai, Chris Tindal, Christoph Feichtenhofer, Cynthia Gao, Damon Civin,Dana Beaty, Daniel Kreymer, Daniel Li, David Adkins, David Xu, Davide Testuggine, DeliaDavid, Devi Parikh, Diana Liskovich, Didem Foss, Dingkang Wang, Duc Le, Dustin Holland,Edward Dowling, Eissa Jamil, Elaine Montgomery, Eleonora Presani, Emily Hahn, EmilyWood, Eric-Tuan Le, Erik Brinkman, Esteban Arcaute, Evan Dunbar, Evan Smothers, Fei Sun,Felix Kreuk, Feng Tian, Filippos Kokkinos, Firat Ozgenel, Francesco Caggioni, Frank Kanayet,Frank Seide, Gabriela Medina Florez, Gabriella Schwarz, Gada Badeer, Georgia Swee, GilHalpern, Grant Herman, Grigory Sizov, Guangyi, Zhang, Guna Lakshminarayanan, HakanInan, Hamid Shojanazeri, Han Zou, Hannah Wang, Hanwen Zha, Haroun Habeeb, HarrisonRudolph, Helen Suk, Henry Aspegren, Hunter Goldman, Hongyuan Zhan, Ibrahim Damlaj,Igor Molybog, Igor Tufanov, Ilias Leontiadis, Irina-Elena Veliche, Itai Gat, Jake Weissman,James Geboski, James Kohli, Janice Lam, Japhet Asher, Jean-Baptiste Gaya, Jeff Marcus, JeffTang, Jennifer Chan, Jenny Zhen, Jeremy Reizenstein, Jeremy Teboul, Jessica Zhong, JianJin, Jingyi Yang, Joe Cummings, Jon Carvill, Jon Shepard, Jonathan McPhie, Jonathan Torres,Josh Ginsburg, Junjie Wang, Kai Wu, Kam Hou U, Karan Saxena, Kartikay Khandelwal,Katayoun Zand, Kathy Matosich, Kaushik Veeraraghavan, Kelly Michelena, Keqian Li, KiranJagadeesh, Kun Huang, Kunal Chawla, Kyle Huang, Lailin Chen, Lakshya Garg, Lavender A,Leandro Silva, Lee Bell, Lei Zhang, Liangpeng Guo, Licheng Yu, Liron Moshkovich, LucaWehrstedt, Madian Khabsa, Manav Avalani, Manish Bhatt, Martynas Mankus, Matan Hasson,Matthew Lennie, Matthias Reso, Maxim Groshev, Maxim Naumov, Maya Lathi, MeghanKeneally, Miao Liu, Michael L. Seltzer, Michal Valko, Michelle Restrepo, Mihir Patel, MikVyatskov, Mikayel Samvelyan, Mike Clark, Mike Macey, Mike Wang, Miquel Jubert Hermoso,Mo Metanat, Mohammad Rastegari, Munish Bansal, Nandhini Santhanam, Natascha Parks,Natasha White, Navyata Bawa, Nayan Singhal, Nick Egebo, Nicolas Usunier, Nikhil Mehta,Nikolay Pavlovich Laptev, Ning Dong, Norman Cheng, Oleg Chernoguz, Olivia Hart, OmkarSalpekar, Ozlem Kalinli, Parkin Kent, Parth Parekh, Paul Saab, Pavan Balaji, Pedro Rittner,Philip Bontrager, Pierre Roux, Piotr Dollar, Polina Zvyagina, Prashant Ratanchandani, PritishYuvraj, Qian Liang, Rachad Alao, Rachel Rodriguez, Rafi Ayub, Raghotham Murthy, RaghuNayani, Rahul Mitra, Rangaprabhu Parthasarathy, Raymond Li, Rebekkah Hogan, RobinBattey, Rocky Wang, Russ Howes, Ruty Rinott, Sachin Mehta, Sachin Siby, Sai Jayesh Bondu,Samyak Datta, Sara Chugh, Sara Hunt, Sargun Dhillon, Sasha Sidorov, Satadru Pan, SaurabhMahajan, Saurabh Verma, Seiji Yamamoto, Sharadh Ramaswamy, Shaun Lindsay, Shaun Lind-say, Sheng Feng, Shenghao Lin, Shengxin Cindy Zha, Shishir Patil, Shiva Shankar, ShuqiangZhang, Shuqiang Zhang, Sinong Wang, Sneha Agarwal, Soji Sajuyigbe, Soumith Chintala,Stephanie Max, Stephen Chen, Steve Kehoe, Steve Satterfield, Sudarshan Govindaprasad,Sumit Gupta, Summer Deng, Sungmin Cho, Sunny Virk, Suraj Subramanian, Sy Choudhury,Sydney Goldman, Tal Remez, Tamar Glaser, Tamara Best, Thilo Koehler, Thomas Robinson,Tianhe Li, Tianjun Zhang, Tim Matthews, Timothy Chou, Tzook Shaked, Varun Vontimitta,Victoria Ajayi, Victoria Montanez, Vijai Mohan, Vinay Satish Kumar, Vishal Mangla, VladIonescu, Vlad Poenaru, Vlad Tiberiu Mihailescu, Vladimir Ivanov, Wei Li, Wenchen Wang,Wenwen Jiang, Wes Bouaziz, Will Constable, Xiaocheng Tang, Xiaojian Wu, Xiaolan Wang,\n\n16\n\nXilun Wu, Xinbo Gao, Yaniv Kleinman, Yanjun Chen, Ye Hu, Ye Jia, Ye Qi, Yenda Li, YilinZhang, Ying Zhang, Yossi Adi, Youngjin Nam, Yu, Wang, Yu Zhao, Yuchen Hao, YundiQian, Yunlu Li, Yuzi He, Zach Rait, Zachary DeVito, Zef Rosnbrick, Zhaoduo Wen, ZhenyuYang, Zhiwei Zhao, and Zhiyu Ma. The Llama 3 Herd of Models, November 2024. URL\n\nhttp://arxiv.org/abs/2407.21783. arXiv:2407.21783 [cs].\n\n[63] Grobid. Grobid. https://github.com/kermitt2/grobid, 2008–2025.\n\n[64] Dirk Groeneveld, Iz Beltagy, Pete Walsh, Akshita Bhagia, Rodney Kinney, Oyvind Tafjord,Ananya Harsh Jha, Hamish Ivison, Ian Magnusson, Yizhong Wang, Shane Arora, David Atkin-son, Russell Authur, Khyathi Raghavi Chandu, Arman Cohan, Jennifer Dumas, Yanai Elazar,Yuling Gu, Jack Hessel, Tushar Khot, William Merrill, Jacob Morrison, Niklas Muennighoff,Aakanksha Naik, Crystal Nam, Matthew E. Peters, Valentina Pyatkin, Abhilasha Ravichander,Dustin Schwenk, Saurabh Shah, Will Smith, Emma Strubell, Nishant Subramani, MitchellWortsman, Pradeep Dasigi, Nathan Lambert, Kyle Richardson, Luke Zettlemoyer, Jesse Dodge,Kyle Lo, Luca Soldaini, Noah A. Smith, and Hannaneh Hajishirzi. Olmo: Accelerating thescience of language models, 2024. URL https://arxiv.org/abs/2402.00838.\n\n[65] Yuling Gu, Oyvind Tafjord, Bailey Kuehl, Dany Haddad, Jesse Dodge, and HannanehHajishirzi.Olmes: A standard for language model evaluations, 2025.URL https://arxiv.org/abs/2406.08446.\n\n[66] Mandy Guo, Zihang Dai, Denny Vrandeˇci´c, and Rami Al-Rfou. Wiki-40b: Multilinguallanguage model dataset. In Proceedings of the Twelfth Language Resources and EvaluationConference, pages 2440–2452, 2020.\n\n[67] Aditya Gupta, Jiacheng Xu, Shyam Upadhyay, Diyi Yang, and Manaal Faruqui.Disfl-qa: A benchmark dataset for understanding disfluencies in question answering.ArXiv,abs/2106.04016, 2021.\n\n[68] Ivan Habernal, Omnia Zayed, and Iryna Gurevych. C4Corpus: Multilingual web-size corpuswith free license. In Nicoletta Calzolari, Khalid Choukri, Thierry Declerck, Sara Goggi,Marko Grobelnik, Bente Maegaard, Joseph Mariani, Helene Mazo, Asuncion Moreno, JanOdijk, and Stelios Piperidis, editors, Proceedings of the Tenth International Conference onLanguage Resources and Evaluation (LREC‘16), pages 914–922, Portorož, Slovenia, May2016. European Language Resources Association (ELRA). URL https://aclanthology.org/L16-1146/.\n\n[69] Seth Hays.AI Training and Copyright Infringement:Solutions from Asia, Octo-ber 2024.URL https://www.techpolicy.press/ai-training-and-copyright-infringement-solutions-from-asia/.\n\n[70] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, andJacob Steinhardt. Measuring massive multitask language understanding. arXiv preprintarXiv:2009.03300, 2020.\n\n[71] Dan Hendrycks, Collin Burns, Anya Chen, and Spencer Ball. Cuad: An expert-annotated nlpdataset for legal contract review. ArXiv, abs/2103.06268, 2021.\n\n[72] Jeremy Howard and Sebastian Ruder. Universal language model fine-tuning for text classi-fication. In Proceedings of the 56th Annual Meeting of the Association for ComputationalLinguistics, 2018.\n\n[73] Shengding Hu, Yuge Tu, Xu Han, Chaoqun He, Ganqu Cui, Xiang Long, Zhi Zheng, YeweiFang, Yuxiang Huang, Weilin Zhao, et al. Minicpm: Unveiling the potential of small languagemodels with scalable training strategies. arXiv preprint arXiv:2404.06395, 2024.\n\n[74] HuggingFace: Common Corpus, 2025.URL https://huggingface.co/datasets/PleIAs/common_corpus.\n\n[75] Jena D. Hwang, Chandra Bhagavatula, Ronan Le Bras, Jeff Da, Keisuke Sakaguchi, AntoineBosselut, and Yejin Choi. Comet-atomic 2020: On symbolic and neural commonsenseknowledge graphs. In AAAI Conference on Artificial Intelligence, pages 6384–6392, 2020.\n\n17\n\n[76] Adam Ibrahim, Benjamin Thérien, Kshitij Gupta, Mats L Richter, Quentin Anthony, TimothéeLesort, Eugene Belilovsky, and Irina Rish. Simple and scalable strategies to continuallypre-train large language models. arXiv preprint arXiv:2403.08763, 2024.\n\n[77] IFI CLAIMS Patent Services and Google. Google patents public data. https://patents.google.com/, 2023. Licensed under a Creative Commons Attribution 4.0 InternationalLicense.\n\n[78] Michael J Bommarito II, Jillian Bommarito, and Daniel Martin Katz. The kl3m data project:Copyright-clean training resources for large language models, 2025. URL https://arxiv.org/abs/2504.07854.\n\n[79] Infocomm Media Development Authority of Singapore (IMDA), Aicadium, and AI VerifyFoundation. Model AI Governance Framework for Generative AI: Fostering a Trusted Ecosys-tem, May 2024. URL https://aiverifyfoundation.sg/wp-content/uploads/2024/05/Model-AI-Governance-Framework-for-Generative-AI-May-2024-1-1.pdf.\n\n[80] Najko Jahn, Nick Haupka, and Anne Hobert. Analysing and reclassifying open access informa-tion in OpenAlex, 2023. URL https://subugoe.github.io/scholcomm_analytics/posts/oalex_oa_status/?utm_source=chatgpt.com.\n\n[81] Armand Joulin, Edouard Grave, Piotr Bojanowski, and Tomas Mikolov. Bag of tricks forefficient text classification. In Proceedings of the 15th Conference of the European Chapterof the Association for Computational Linguistics: Volume 2, Short Papers, pages 427–431.Association for Computational Linguistics, April 2017.\n\n[82] Nikhil Kandpal and Colin Raffel. Position: The most expensive part of an llm should be itstraining data. arXiv preprint arXiv:2504.12427, 2025.\n\n[83] Nikhil Kandpal, Eric Wallace, and Colin Raffel. Deduplicating training data mitigates privacyrisks in language models, 2022. URL https://arxiv.org/abs/2202.06539.\n\n[84] Nikhil Kandpal, Haikang Deng, Adam Roberts, Eric Wallace, and Colin Raffel. Largelanguage models struggle to learn long-tail knowledge. In International Conference onMachine Learning, pages 15696–15707. PMLR, 2023.\n\n[85] Pride Kavumba, Naoya Inoue, Benjamin Heinzerling, Keshav Singh, Paul Reisert, and KentaroInui. When choosing plausible alternatives, clever hans can be clever. ArXiv, abs/1911.00225,2019.\n\n[86] Tushar Khot, Ashish Sabharwal, and Peter Clark. Scitail: A textual entailment dataset fromscience question answering. In AAAI Conference on Artificial Intelligence, pages 5189–5197,2018.\n\n[87] Rodney Michael Kinney, Chloe Anastasiades, Russell Authur, Iz Beltagy, Jonathan Bragg,Alexandra Buraczynski, Isabel Cachola, Stefan Candra, Yoganand Chandrasekhar, ArmanCohan, Miles Crawford, Doug Downey, Jason Dunkelberger, Oren Etzioni, Rob Evans, SergeyFeldman, Joseph Gorney, David W. Graham, F.Q. Hu, Regan Huff, Daniel King, SebastianKohlmeier, Bailey Kuehl, Michael Langan, Daniel Lin, Haokun Liu, Kyle Lo, Jaron Lochner,Kelsey MacMillan, Tyler C. Murray, Christopher Newell, Smita R Rao, Shaurya Rohatgi,Paul Sayre, Zejiang Shen, Amanpreet Singh, Luca Soldaini, Shivashankar Subramanian,A. Tanaka, Alex D Wade, Linda M. Wagner, Lucy Lu Wang, Christopher Wilhelm, CarolineWu, Jiangjiang Yang, Angele Zamarron, Madeleine van Zuylen, and Daniel S. Weld. TheSemantic Scholar Open Data Platform. ArXiv, abs/2301.10140, 2023. URL https://api.semanticscholar.org/CorpusID:256194545.\n\n[88] Andreas Kopf, Yannic Kilcher, Dimitri von Rutte, Sotiris Anagnostidis, Zhi Rui Tam,K. Stevens, Abdullah Barhoum, Nguyen Minh Duc, Oliver Stanley, Rich’ard Nagyfi,ES Shahul, Sameer Suri, David Glushkov, Arnav Dantuluri, Andrew Maguire, ChristophSchuhmann, Huu Nguyen, and A. Mattick. Openassistant conversations - democratizing largelanguage model alignment. ArXiv, abs/2304.07327, 2023.\n\n18\n\n[89] T. Kwiatkowski, J. Palomaki, Olivia Redfield, Michael Collins, Ankur P. Parikh, Chris Alberti,D. Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones,Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc V. Le, and SlavPetrov. Natural questions: A benchmark for question answering research. Transactions of theAssociation for Computational Linguistics, 7:453–466, 2019.\n\n[90] Faisal Ladhak, Esin Durmus, Claire Cardie, and K. McKeown. Wikilingua: A new benchmarkdataset for multilingual abstractive summarization. ArXiv, abs/2010.03093, 2020.\n\n[91] Pierre-Carl Langlais. Releasing Common Corpus: the largest public domain dataset for trainingLLMs, 2024. URL https://huggingface.co/blog/Pclanglais/common-corpus.\n\n[92] LDP Headquarters for the Promotion of Digital Society and Project Team onthe Evolution and Implementation of AIs.AI White Paper 2024:New Strate-giesinStageII,Towardtheworld’smostAI-friendlycountry,April2024.URL https://aiverifyfoundation.sg/wp-content/uploads/2024/05/Model-AI-Governance-Framework-for-Generative-AI-May-2024-1-1.pdf.\n\n[93] R. Lebret, David Grangier, and Michael Auli. Neural text generation from structured datawith application to the biography domain. In Conference on Empirical Methods in NaturalLanguage Processing, pages 1203–1213, 2016.\n\n[94] Katherine Lee, Daphne Ippolito, Andrew Nystrom, Chiyuan Zhang, Douglas Eck, ChrisCallison-Burch, and Nicholas Carlini. Deduplicating training data makes language modelsbetter, 2022. URL https://arxiv.org/abs/2107.06499.\n\n[95] Katherine Lee, A. Feder Cooper, James Grimmelmann, and Daphne Ippolito. AI and Law:The Next Generation. SSRN, 2023. http://dx.doi.org/10.2139/ssrn.4580739.\n\n[96] Katherine Lee, A. Feder Cooper, and James Grimmelmann. Talkin’ ’Bout AI Generation:Copyright and the Generative-AI Supply Chain. arXiv preprint arXiv:2309.08133, 2023.\n\n[97] Jens Lehmann, Robert Isele, Max Jakob, Anja Jentzsch, D. Kontokostas, Pablo N. Mendes,Sebastian Hellmann, M. Morsey, Patrick van Kleef, S. Auer, and Christian Bizer. Dbpedia - alarge-scale, multilingual knowledge base extracted from wikipedia. Semantic Web, 6:167–195,2015.\n\n[98] H. Levesque, E. Davis, and L. Morgenstern. The winograd schema challenge. In AAAI SpringSymposium: Logical Formalizations of Commonsense Reasoning, 2011.\n\n[99] Jeffrey Li, Alex Fang, Georgios Smyrnis, Maor Ivgi, Matt Jordan, Samir Gadre, HritikBansal, Etash Guha, Sedrick Keh, Kushal Arora, Saurabh Garg, Rui Xin, Niklas Muennighoff,Reinhard Heckel, Jean Mercat, Mayee Chen, Suchin Gururangan, Mitchell Wortsman, AlonAlbalak, Yonatan Bitton, Marianna Nezhurina, Amro Abbas, Cheng-Yu Hsieh, Dhruba Ghosh,Josh Gardner, Maciej Kilian, Hanlin Zhang, Rulin Shao, Sarah Pratt, Sunny Sanyal, GabrielIlharco, Giannis Daras, Kalyani Marathe, Aaron Gokaslan, Jieyu Zhang, Khyathi Chandu,Thao Nguyen, Igor Vasiljevic, Sham Kakade, Shuran Song, Sujay Sanghavi, Fartash Faghri,Sewoong Oh, Luke Zettlemoyer, Kyle Lo, Alaaeldin El-Nouby, Hadi Pouransari, AlexanderToshev, Stephanie Wang, Dirk Groeneveld, Luca Soldaini, Pang Wei Koh, Jenia Jitsev, ThomasKollar, Alexandros G. Dimakis, Yair Carmon, Achal Dave, Ludwig Schmidt, and VaishaalShankar. Datacomp-lm: In search of the next generation of training sets for language models,2025. URL https://arxiv.org/abs/2406.11794.\n\n[100] Xin Li and D. Roth.Learning question classifiers.In International Conference onComputational Linguistics, pages 1–7, 2002.\n\n[101] Stephanie C. Lin, Jacob Hilton, and Owain Evans. Truthfulqa: Measuring how models mimichuman falsehoods. In Annual Meeting of the Association for Computational Linguistics, pages3214–3252, 2021.\n\n[102] Emmy Liu, Chenxuan Cui, Kenneth Zheng, and Graham Neubig. Testing the ability oflanguage models to interpret figurative language. ArXiv, abs/2204.12632, 2022.\n\n19\n\n[103] Zhengzhong Liu, Aurick Qiao, Willie Neiswanger, Hongyi Wang, Bowen Tan, TianhuaTao, Junbo Li, Yuqi Wang, Suqi Sun, Omkar Pangarkar, Richard Fan, Yi Gu, Victor Miller,Yonghao Zhuang, Guowei He, Haonan Li, Fajri Koto, Liping Tang, Nikhil Ranjan, ZhiqiangShen, Xuguang Ren, Roberto Iriondo, Cun Mu, Zhiting Hu, Mark Schulze, Preslav Nakov,Tim Baldwin, and Eric P. Xing. Llm360: Towards fully transparent open-source llms, 2023.URL https://arxiv.org/abs/2312.06550.\n\n[104] Kyle Lo, Lucy Lu Wang, Mark Neumann, Rodney Kinney, and Daniel Weld.S2ORC:The semantic scholar open research corpus. In Proceedings of the 58th Annual Meetingof the Association for Computational Linguistics, pages 4969–4983, Online, July 2020.Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.447. URLhttps://www.aclweb.org/anthology/2020.acl-main.447.\n\n[105] Shayne Longpre, Stella Biderman, Alon Albalak, Hailey Schoelkopf, Daniel McDuff, SayashKapoor, Kevin Klyman, Kyle Lo, Gabriel Ilharco, Nay San, et al. The responsible foundationmodel development cheatsheet: A review of tools & resources. Transactions on MachineLearning Research, 2024.\n\n[106] Shayne Longpre, Robert Mahari, Anthony Chen, Naana Obeng-Marnu, Damien Sileo, WilliamBrannon, Niklas Muennighoff, Nathan Khazam, Jad Kabbara, Kartik Perisetla, Xinyi (Alexis)Wu, Enrico Shippole, Kurt Bollacker, Tongshuang Wu, Luis Villa, Sandy Pentland, andSara Hooker. A large-scale audit of dataset licensing and attribution in AI. Nature MachineIntelligence, 6(8):975–987, August 2024. doi: 10/gt8f5p.\n\n[107] Shayne Longpre, Robert Mahari, Ariel Lee, Campbell Lund, Hamidah Oderinwale, WilliamBrannon, Nayan Saxena, Naana Obeng-Marnu, Tobin South, Cole Hunter, Kevin Klyman,Christopher Klamm, Hailey Schoelkopf, Nikhil Singh, Manuel Cherep, Ahmad Anis, An Dinh,Caroline Chitongo, Da Yin, Damien Sileo, Deividas Mataciunas, Diganta Misra, EmadAlghamdi, Enrico Shippole, Jianguo Zhang, Joanna Materzynska, Kun Qian, Kush Tiwary,Lester Miranda, Manan Dey, Minnie Liang, Mohammed Hamdy, Niklas Muennighoff,Seonghyeon Ye, Seungone Kim, Shrestha Mohanty, Vipul Gupta, Vivek Sharma, Vu MinhChien, Xuhui Zhou, Yizhi Li, Caiming Xiong, Luis Villa, Stella Biderman, Hanlin Li, DaphneIppolito, Sara Hooker, Jad Kabbara, and Sandy Pentland. Consent in crisis: The rapid declineof the AI data commons. Advances in Neural Information Processing Systems, 37, 2024.\n\n[108] Shayne Longpre, Robert Mahari, Naana Obeng-Marnu, William Brannon, Tobin South, KatyGero, Sandy Pentland, and Jad Kabbara. Data authenticity, consent, & provenance for ai areall broken: what will it take to fix them? arXiv preprint arXiv:2404.12691, 2024.\n\n[109] Shayne Longpre, Nikhil Singh, Manuel Cherep, Kushagra Tiwary, Joanna Materzynska,William Brannon, Robert Mahari, Naana Obeng-Marnu, Manan Dey, Mohammed Hamdy,et al.Bridging the data provenance gap across text, speech and video.arXiv preprintarXiv:2412.17847, 2024.\n\n[110] Shayne Longpre, Gregory Yauney, Emily Reif, Katherine Lee, Adam Roberts, Barret Zoph,Denny Zhou, Jason Wei, Kevin Robinson, David Mimno, et al. A pretrainer’s guide to trainingdata: Measuring the effects of data age, domain coverage, quality, & toxicity. In Proceedingsof the 2024 Conference of the North American Chapter of the Association for ComputationalLinguistics: Human Language Technologies (Volume 1: Long Papers), pages 3245–3276, 2024.\n\n[111] Ilya Loshchilov and Frank Hutter.Decoupled weight decay regularization.InInternationalConferenceonLearningRepresentations,2019.URLhttps://openreview.net/forum?id=Bkg6RiCqY7.\n\n[112] Annie Louis, D. Roth, and Filip Radlinski. “i’d rather just go to bed”’: Understanding indirectanswers. In Conference on Empirical Methods in Natural Language Processing, volumeabs/2010.03450, 2020.\n\n[113] Anton Lozhkov, Raymond Li, Loubna Ben Allal, Federico Cassano, Joel Lamy-Poirier,Nouamane Tazi, Ao Tang, Dmytro Pykhtar, Jiawei Liu, Yuxiang Wei, Tianyang Liu, Max Tian,Denis Kocetkov, Arthur Zucker, Younes Belkada, Zijian Wang, Qian Liu, Dmitry Abulkhanov,Indraneil Paul, Zhuang Li, Wen-Ding Li, Megan Risdal, Jia Li, Jian Zhu, Terry Yue Zhuo,\n\n20\n\nEvgenii Zheltonozhskii, Nii Osae Osae Dade, Wenhao Yu, Lucas Krauß, Naman Jain, YixuanSu, Xuanli He, Manan Dey, Edoardo Abati, Yekun Chai, Niklas Muennighoff, XiangruTang, Muhtasham Oblokulov, Christopher Akiki, Marc Marone, Chenghao Mou, MayankMishra, Alex Gu, Binyuan Hui, Tri Dao, Armel Zebaze, Olivier Dehaene, Nicolas Patry,Canwen Xu, Julian McAuley, Han Hu, Torsten Scholak, Sebastien Paquet, Jennifer Robinson,Carolyn Jane Anderson, Nicolas Chapados, Mostofa Patwary, Nima Tajbakhsh, Yacine Jernite,Carlos Muñoz Ferrandis, Lingming Zhang, Sean Hughes, Thomas Wolf, Arjun Guha, Leandrovon Werra, and Harm de Vries. Starcoder 2 and the stack v2: The next generation, 2024.\n\n[114] Robert Mahari and Shayne Longpre. Discit ergo est: Training data provenance and fair use.Robert Mahari and Shayne Longpre, Discit ergo est: Training Data Provenance And Fair Use,Dynamics of Generative AI (ed. Thibault Schrepel & Volker Stocker), Network Law Review,Winter, 2023.\n\n[115] Matt Mahoney. Large text compression benchmark, 2011.\n\n[116] Stephen Merity, Caiming Xiong, James Bradbury, and R. Socher. Pointer sentinel mixturemodels. ArXiv, abs/1609.07843, 2016.\n\n[117] Jean-BaptisteMichel,YuanKuiShen,AvivaPresserAiden,AdrianVeres,Matthew K. Gray,The Google Books Team,Joseph P. Pickett,Dale Hoiberg,Dan Clancy,Peter Norvig,Jon Orwant,Steven Pinker,Martin A. Nowak,andErez Lieberman Aiden.Quantitative analysis of culture using millions of digitizedbooks.Science, 331(6014):176–182, 2011.doi:10.1126/science.1199644.URLhttps://www.science.org/doi/abs/10.1126/science.1199644.\n\n[118] Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a suit of armor conductelectricity? a new dataset for open book question answering. arXiv preprint arXiv:1809.02789,2018.\n\n[119] Sewon Min, Suchin Gururangan, Eric Wallace, Weijia Shi, Hannaneh Hajishirzi, Noah A.Smith, and Luke Zettlemoyer. SILO language models: Isolating legal risk in a nonparametricdatastore. In The Twelfth International Conference on Learning Representations, 2024. URLhttps://openreview.net/forum?id=ruk0nyQPec.\n\n[120] Niklas Muennighoff, Alexander Rush, Boaz Barak, Teven Le Scao, Nouamane Tazi,Aleksandra Piktus, Sampo Pyysalo, Thomas Wolf, and Colin Raffel. Scaling data-constrainedlanguage models. Advances in Neural Information Processing Systems, 36, 2023.\n\n[121] Niklas Muennighoff, Alexander Rush, Boaz Barak, Teven Le Scao, Nouamane Tazi, Alek-sandra Piktus, Sampo Pyysalo, Thomas Wolf, and Colin A Raffel. Scaling data-constrainedlanguage models. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine,editors, Advances in Neural Information Processing Systems, volume 36, pages 50358–50376.Curran Associates, Inc., 2023. URL https://proceedings.neurips.cc/paper_files/paper/2023/file/9d89448b63ce1e2e8dc7af72c984c196-Paper-Conference.pdf.\n\n[122] Nikita Nangia, Clara Vania, Rasika Bhalerao, and Samuel R. Bowman. Crows-pairs: Achallenge dataset for measuring social biases in masked language models. In Conference onEmpirical Methods in Natural Language Processing, pages 1953–1967, 2020.\n\n[123] Jekaterina Novikova, Ondrej Dusek, and Verena Rieser. The e2e dataset: New challengesfor end-to-end generation. ArXiv, abs/1706.09254, 2017.\n\n[124] Tomoko Ohta, Sampo Pyysalo, Junichi Tsujii, and S. Ananiadou. Open-domain anatomicalentity mention detection. In Annual Meeting of the Association for Computational Linguistics,pages 27–36, 2012.\n\n[125] Yasumasa Onoe, Michael J.Q. Zhang, Eunsol Choi, and Greg Durrett. Creak: A dataset forcommonsense reasoning over entity knowledge. ArXiv, abs/2109.01653, 2021.\n\n[126] OpenAlex, 2025. URL https://openalex.org.\n\n21\n\n[127] Vassil Panayotov, Guoguo Chen, Daniel Povey, and S. Khudanpur. Librispeech: An asr corpusbased on public domain audio books. 2015 IEEE International Conference on Acoustics,Speech and Signal Processing (ICASSP), pages 5206–5210, 2015.\n\n[128] Ashwinee Panda, Xinyu Tang, Milad Nasr, Christopher A Choquette-Choo, and Prateek Mittal.Privacy auditing of large language models. arXiv preprint arXiv:2503.06808, 2025.\n\n[129] SungMinPark,KristianGeorgiev,AndrewIlyas,GuillaumeLeclerc,andAleksander Madry.Trak:Attributing model behavior at scale,2023.URLhttps://arxiv.org/abs/2303.14186.\n\n[130] European Parliament and Council of the European Union.Directive (eu) 2019/790,2019.URL https://eur-lex.europa.eu/legal-content/EN/ALL/?uri=CELEX:32019L0790#art_3.\n\n[131] ParlParse. Parser for uk parliament proceedings. https://parser.theyworkforyou.com/,2025. Accessed: 2025-05-09.\n\n[132] Guilherme Penedo, Hynek Kydlíˇcek, Anton Lozhkov, Margaret Mitchell, Colin Raffel,Leandro Von Werra, and Thomas Wolf. The FineWeb datasets: Decanting the web for thefinest text data at scale. Advances in Neural Information Processing Systems, 37, 2024.\n\n[133] Fabio Petroni, Tim Rocktäschel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, YuxiangWu, and Alexander Miller. Language models as knowledge bases? In Proceedings of the2019 Conference on Empirical Methods in Natural Language Processing, 2019.\n\n[134] E. Ponti, Goran Glavavs, Olga Majewska, Qianchu Liu, Ivan Vulic, and A. Korhonen. Xcopa:A multilingual dataset for causal commonsense reasoning. In Conference on EmpiricalMethods in Natural Language Processing, pages 2362–2376, 2020.\n\n[135] Christopher Potts, Zhengxuan Wu, Atticus Geiger, and Douwe Kiela. Dynasent: A dynamicbenchmark for sentiment analysis. ArXiv, abs/2012.15349, 2020.\n\n[136] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving languageunderstanding by generative pre-training, 2018.\n\n[137] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever.Language models are unsupervised multitask learners, 2019.\n\n[138] Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and IlyaSutskever.Robust speech recognition via large-scale weak supervision, 2022.URLhttps://arxiv.org/abs/2212.04356.\n\n[139] Filip Radlinski, K. Balog, B. Byrne, and K. Krishnamoorthi.Coached conversationalpreference elicitation: A case study in understanding movie preferences.In SIGDIALConferences, pages 353–360, 2019.\n\n[140] Jack W Rae, Anna Potapenko, Siddhant M Jayakumar, Chloe Hillier, and Timothy P Lillicrap.Compressive transformers for long-range sequence modelling. arXiv preprint, 2019. URLhttps://arxiv.org/abs/1911.05507.\n\n[141] Jack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song,John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, Eliza Rutherford, Tom Hen-nigan, Jacob Menick, Albin Cassirer, Richard Powell, George van den Driessche, Lisa AnneHendricks, Maribeth Rauh, Po-Sen Huang, Amelia Glaese, Johannes Welbl, SumanthDathathri, Saffron Huang, Jonathan Uesato, John F. J. Mellor, Irina Higgins, Antonia Creswell,Nat McAleese, Amy Wu, Erich Elsen, Siddhant M. Jayakumar, Elena Buchatskaya, David Bud-den, Esme Sutherland, Karen Simonyan, Michela Paganini, L. Sifre, Lena Martens, Xiang Lor-raine Li, Adhiguna Kuncoro, Aida Nematzadeh, Elena Gribovskaya, Domenic Donato, An-geliki Lazaridou, Arthur Mensch, Jean-Baptiste Lespiau, Maria Tsimpoukelli, N. K. Grigorev,Doug Fritz, Thibault Sottiaux, Mantas Pajarskas, Tobias Pohlen, Zhitao Gong, Daniel Toyama,Cyprien de Masson d’Autume, Yujia Li, Tayfun Terzi, Vladimir Mikulik, Igor Babuschkin,Aidan Clark, Diego de Las Casas, Aurelia Guy, Chris Jones, James Bradbury, Matthew G.\n\n22\n\nJohnson, Blake A. Hechtman, Laura Weidinger, Iason Gabriel, William S. Isaac, EdwardLockhart, Simon Osindero, Laura Rimell, Chris Dyer, Oriol Vinyals, Kareem W. Ayoub, JeffStanway, L. L. Bennett, Demis Hassabis, Koray Kavukcuoglu, and Geoffrey Irving. Scalinglanguage models: Methods, analysis & insights from training gopher. ArXiv, abs/2112.11446,2021. URL https://api.semanticscholar.org/CorpusID:245353475.\n\n[142] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena,Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unifiedtext-to-text transformer. Journal of machine learning research, 21(140), 2020.\n\n[143] Robi Rahman and David Owen.The size of datasets used to train language modelsdoubles approximately every seven months, 2024.URL https://epoch.ai/data-insights/dataset-size-trend. Accessed: 2025-05-08.\n\n[144] Nazneen Rajani, Bryan McCann, Caiming Xiong, and R. Socher. Explain yourself! leveraginglanguage models for commonsense reasoning. ArXiv, abs/1906.02361, 2019.\n\n[145] Inioluwa Deborah Raji, Peggy Xu, Colleen Honigsberg, and Daniel Ho. Outsider oversight:Designing a third party audit ecosystem for ai governance. In Proceedings of the 2022AAAI/ACM Conference on AI, Ethics, and Society, pages 557–571, 2022.\n\n[146] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. Squad: 100,000+questions for machine comprehension of text. In Conference on Empirical Methods in NaturalLanguage Processing, pages 2383–2392, 2016.\n\n[147] Pranav Rajpurkar, Robin Jia, and Percy Liang. Know what you don’t know: Unanswerablequestions for squad. In Annual Meeting of the Association for Computational Linguistics,volume abs/1806.03822, 2018.\n\n[148] Abhinav Rastogi, Xiaoxue Zang, Srinivas Sunkara, Raghav Gupta, and Pranav Khaitan.Schema-guided dialogue state tracking task at dstc8. ArXiv, abs/2002.01359, 2020.\n\n[149] Abhilasha Ravichander, Matt Gardner, and Ana Marasovi´c. Condaqa: A contrastive readingcomprehension dataset for reasoning about negation. ArXiv, abs/2211.00295, 2022.\n\n[150] Varshini Reddy, Craig W. Schmidt, Yuval Pinter, and Chris Tanner.How muchis enough?the diminishing returns of tokenization training data, 2025.URLhttps://arxiv.org/abs/2502.20273.\n\n[151] Hammam Riza, Michael Purwoadi, Gunarso, Teduh Uliniansyah, Aw Ai Ti, Sharifah MahaniAljunied, Luong Chi Mai, V. Thang, N. Thai, Vichet Chea, Rapid Sun, Sethserey Sam,Sopheap Seng, K. Soe, K. Nwet, M. Utiyama, and Chenchen Ding. Introduction of the asianlanguage treebank. In Oriental COCOSDA International Conference on Speech Databaseand Assessments, pages 1–6, 2016.\n\n[152] Adam Roberts, Colin Raffel, and Noam Shazeer. How much knowledge can you pack intothe parameters of a language model? In Proceedings of the 2020 Conference on EmpiricalMethods in Natural Language Processing (EMNLP), 2020.\n\n[153] Anna Rogers, Olga Kovaleva, Matthew Downey, and Anna Rumshisky. Getting closer toai complete question answering: A set of prerequisite real tasks. In AAAI Conference onArtificial Intelligence, pages 8722–8731, 2020.\n\n[154] Anna Rogers, Olga Kovaleva, and Anna Rumshisky. A primer in bertology: What we knowabout how BERT works. Transactions of the association for computational linguistics, 8, 2021.\n\n[155] Rachel Rudinger, Vered Shwartz, Jena D. Hwang, Chandra Bhagavatula, Maxwell Forbes,Ronan Le Bras, Noah A. Smith, and Yejin Choi. Thinking like a skeptic: Defeasible inferencein natural language. In Trevor Cohn, Yulan He, and Yang Liu, editors, Findings of the As-sociation for Computational Linguistics: EMNLP 2020, pages 4661–4675, Online, November2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.findings-emnlp.418.URL https://aclanthology.org/2020.findings-emnlp.418/.\n\n23\n\n[156] Matthew Sag and Peter K. Yu.The globalization of copyright exceptions for ai train-ing. Emory Law Journal, 74, 2025. doi: http://dx.doi.org/10.2139/ssrn.4976393. URLhttps://ssrn.com/abstract=4976393.\n\n[157] Swarnadeep Saha, Yixin Nie, and Mohit Bansal. Conjnli: Natural language inference overconjunctive sentences. In Conference on Empirical Methods in Natural Language Processing,pages 8240–8252, 2020.\n\n[158] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande.Communications of the ACM, 64:99 – 106, 2019.\n\n[159] Pamela Samuelson. How to Think About Remedies in the Generative AI Copyright Cases.Lawfare, February 2024.URL https://www.lawfaremedia.org/article/how-to-think-about-remedies-in-the-generative-ai-copyright-cases.\n\n[160] MaartenSap,HannahRashkin,DerekChen,RonanLeBras,andYejinChoi.Socialiqa:Commonsensereasoningaboutsocialinteractions,2019.URLhttps://arxiv.org/abs/1904.09728.\n\n[161] A. Sboev, A. Naumov, and R. Rybka. Data-driven model for emotion detection in russiantexts. In BICAAI, pages 637–642, 2020.\n\n[162] Tal Schuster, Adam Fisch, and R. Barzilay. Get your vitamin c! robust fact verificationwith contrastive evidence. In North American Chapter of the Association for ComputationalLinguistics, pages 624–643, 2021.\n\n[163] Emily Sheng and David C. Uthus. Investigating societal biases in a poetry composition system.ArXiv, abs/2011.02686, 2020.\n\n[164] Mohit Shridhar, Xingdi Yuan, Marc-Alexandre Côté, Yonatan Bisk, Adam Trischler, andMatthew J. Hausknecht. Alfworld: Aligning text and embodied environments for interactivelearning. ArXiv, abs/2010.03768, 2020.\n\n[165] Shivalika Singh, Freddie Vargus, Daniel Dsouza, B\"orje F. Karlsson, Abinaya Mahendiran,Wei-Yin Ko, Herumb Shandilya, Jay Patel, Deividas Mataciunas, Laura OMahony, MikeZhang, Ramith Hettiarachchi, Joseph Wilson, Marina Machado, Luisa Souza Moura,Dominik Krzemi’nski, Hakimeh Fadaei, Irem Ergun, Ifeoma Okoh, Aisha Alaagib, OshanMudannayake, Zaid Alyafeai, Minh Chien Vu, Sebastian Ruder, Surya Guthikonda,Emad A. Alghamdi, Sebastian Gehrmann, Niklas Muennighoff, Max Bartolo, JuliaKreutzer, A. Ustun, Marzieh Fadaee, and Sara Hooker.Aya dataset: An open-accesscollection for multilingual instruction tuning.ArXiv, abs/2402.06619, 2024.URLhttps://api.semanticscholar.org/CorpusID:267617144.\n\n[166] Luca Soldaini and Kyle Lo. peS2o (Pretraining Efficiently on S2ORC) Dataset. Technicalreport, Allen Institute for AI, 2023. ODC-By, https://github.com/allenai/pes2o.\n\n[167] Luca Soldaini, Rodney Kinney, Akshita Bhagia, Dustin Schwenk, David Atkinson, RussellAuthur, Ben Bogin, Khyathi Chandu, Jennifer Dumas, Yanai Elazar, Valentin Hofmann,Ananya Harsh Jha, Sachin Kumar, Li Lucy, Xinxi Lyu, Nathan Lambert, Ian Magnusson, JacobMorrison, Niklas Muennighoff, Aakanksha Naik, Crystal Nam, Matthew E. Peters, AbhilashaRavichander, Kyle Richardson, Zejiang Shen, Emma Strubell, Nishant Subramani, OyvindTafjord, Pete Walsh, Luke Zettlemoyer, Noah A. Smith, Hannaneh Hajishirzi, Iz Beltagy,Dirk Groeneveld, Jesse Dodge, and Kyle Lo. Dolma: an open corpus of three trillion tokensfor language model pretraining research. In Proceedings of the 62nd Annual Meeting of theAssociation for Computational Linguistics, 2024.\n\n[168] Gabriel Stanovsky, Noah A. Smith, and Luke Zettlemoyer. Evaluating gender bias in machinetranslation. ArXiv, abs/1906.00591, 2019.\n\n[169] Pedro Javier Ortiz Suárez, Benoît Sagot, and Laurent Romary. Asynchronous pipeline forprocessing huge corpora on medium to low resource infrastructures. In 7th Workshop on theChallenges in the Management of Large Corpora (CMLC-7). Leibniz-Institut für DeutscheSprache, 2019.\n\n24\n\n[170] Oyvind Tafjord, Matt Gardner, Kevin Lin, and Peter Clark. Quartz: An open-domain dataset ofqualitative relationship questions. In Conference on Empirical Methods in Natural LanguageProcessing, volume abs/1909.03553, 2019.\n\n[171] Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. Commonsenseqa:A question answering challenge targeting commonsense knowledge.arXiv preprintarXiv:1811.00937, 2018.\n\n[172] Niket Tandon, Keisuke Sakaguchi, Bhavana Dalvi, Dheeraj Rajagopal, Peter Clark, MichalGuerquin, Kyle Richardson, and E. Hovy. A dataset for tracking entities in open domainprocedural text. ArXiv, abs/2011.08092, 2020.\n\n[173] Liping Tang, Nikhil Ranjan, Omkar Pangarkar, Xuezhi Liang, Zhen Wang, Li An, BhaskarRao, Linghao Jin, Huijuan Wang, Zhoujun Cheng, Suqi Sun, Cun Mu, Victor Miller, XuezheMa, Yue Peng, Zhengzhong Liu, and Eric P. Xing. Txt360: A top-quality llm pre-trainingdataset requires the perfect blend, 2024.\n\n[174] Ishan Tarunesh, Somak Aditya, and M. Choudhury. Trusting roberta over bert: Insights fromchecklisting the natural language inference task. ArXiv, abs/2107.07229, 2021.\n\n[175] MosaicML NLP Team. Introducing mpt-7b: A new standard for open-source, commerciallyusable llms, 2023. URL www.mosaicml.com/blog/mpt-7b. Accessed: 2023-05-05.\n\n[176] Qwen Team. Qwen3, April 2025. URL https://qwenlm.github.io/blog/qwen3/.\n\n[177] James Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal. Fever: alarge-scale dataset for fact extraction and verification. ArXiv, abs/1803.05355, 2018.\n\n[178] Anvith Thudi, Evianne Rovers, Yangjun Ruan, Tristan Thrush, and Chris J. Mad-dison.Mixmin:Finding data mixtures via convex minimization, 2025.URLhttps://arxiv.org/abs/2502.10510.\n\n[179] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux,Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, AurelienRodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficientfoundation language models, 2023. URL https://arxiv.org/abs/2302.13971.\n\n[180] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei,Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Openfoundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.\n\n[181] UK Parliament.Open parliament license.https://www.parliament.uk/site-information/copyright-parliament/open-parliament-licence/,Unknown.Accessed: 2025-05-09.\n\n[182] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural informationprocessing systems, 30, 2017.\n\n[183] Mathurin Videau, Badr Youbi Idrissi, Daniel Haziza, Luca Wehrstedt, Jade Copet, OlivierTeytaud, and David Lopez-Paz. Meta Lingua: A minimal PyTorch LLM training library, 2024.URL https://github.com/facebookresearch/lingua.\n\n[184] Zhilin Wang, Yi Dong, Jiaqi Zeng, Virginia Adams, Makesh Narsimhan Sreedhar, Daniel Egert,Olivier Delalleau, Jane Polak Scowcroft, Neel Kant, Aidan Swope, and Oleksii Kuchaiev.Helpsteer: Multi-attribute helpfulness dataset for steerlm. ArXiv, abs/2311.09528, 2023.\n\n[185] Maurice Weber, Daniel Fu, Quentin Anthony, Yonatan Oren, Shane Adams, Anton Alexandrov,Xiaozhong Lyu, Huu Nguyen, Xiaozhe Yao, Virginia Adams, Ben Athiwaratkun, RahulChalamala, Kezhen Chen, Max Ryabinin, Tri Dao, Percy Liang, Christopher Ré, Irina Rish,and Ce Zhang. Redpajama: an open dataset for training large language models, 2024. URLhttps://arxiv.org/abs/2411.12372.\n\n25\n\n[186] Kellie Webster, Marta Recasens, Vera Axelrod, and Jason Baldridge. Resolving genderedambiguous pronouns with bert. ArXiv, abs/1906.01161, 2019.\n\n[187] Wei Wei, Quoc V. Le, Andrew M. Dai, and Jia Li.Airdialogue: An environment forgoal-oriented dialogue research. In Conference on Empirical Methods in Natural LanguageProcessing, pages 3844–3854, 2018.\n\n[188] Alexander Wettig, Kyle Lo, Sewon Min, Hannaneh Hajishirzi, Danqi Chen, and Luca Soldaini.Organize the web: Constructing domains enhances pre-training data curation. arXiv preprintarXiv:2502.10341, 2025.\n\n[189] Sang Michael Xie, Hieu Pham, Xuanyi Dong, Nan Du, Hanxiao Liu, Yifeng Lu, Percy S Liang,Quoc V Le, Tengyu Ma, and Adams Wei Yu. Doremi: Optimizing data mixtures speeds uplanguage model pretraining. Advances in Neural Information Processing Systems, 36, 2023.\n\n[190] Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W. Cohen, R. Salakhutdinov,and Christopher D. Manning. Hotpotqa: A dataset for diverse, explainable multi-hop questionanswering. In Conference on Empirical Methods in Natural Language Processing, pages2369–2380, 2018.\n\n[191] Cat Zakrzewski, Nitasha Tiku, and Elizabeth Dwoskin. OpenAI prepares to fight for its lifeas legal troubles mount. The Washington Post, 2024.\n\n[192] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Cana machine really finish your sentence? arXiv preprint arXiv:1905.07830, 2019.\n\n[193] Ge Zhang, Scott Qu, Jiaheng Liu, Chenchen Zhang, Chenghua Lin, Chou Leuang Yu, DannyPan, Esther Cheng, Jie Liu, Qunshu Lin, Raven Yuan, Tuney Zheng, Wei Pang, Xinrun Du,Yiming Liang, Yinghao Ma, Yizhi Li, Ziyang Ma, Bill Lin, Emmanouil Benetos, Huan Yang,Junting Zhou, Kaijing Ma, Minghao Liu, Morry Niu, Noah Wang, Quehry Que, Ruibo Liu,Sine Liu, Shawn Guo, Soren Gao, Wangchunshu Zhou, Xinyue Zhang, Yizhi Zhou, YuboWang, Yuelin Bai, Yuhan Zhang, Yuxiang Zhang, Zenith Wang, Zhenzhu Yang, Zijian Zhao,Jiajun Zhang, Wanli Ouyang, Wenhao Huang, and Wenhu Chen. MAP-Neo: Highly capableand transparent bilingual large language model series. arXiv preprint arXiv:2405.19327, 2024.\n\n[194] Hongming Zhang, Xinran Zhao, and Yangqiu Song. Winowhy: A deep diagnosis of essentialcommonsense knowledge for answering winograd schema challenge. In Annual Meeting ofthe Association for Computational Linguistics, pages 5736–5745, 2020.\n\n[195] Wenting Zhao, Xiang Ren, Jack Hessel, Claire Cardie, Yejin Choi, and Yuntian Deng. Wildchat:1m chatGPT interaction logs in the wild. In The Twelfth International Conference on LearningRepresentations, 2024. URL https://openreview.net/forum?id=Bl8u7ZRlbM.\n\n[196] Ben Zhou, Kyle Richardson, Qiang Ning, Tushar Khot, Ashish Sabharwal, and D. Roth.Temporal reasoning on implicit events from distant supervision. ArXiv, abs/2010.12753, 2020.\n\n26\n\nAppendix\n\nTable of Contents\n\nA Contributions28\n\nBDetailed Description of Sources28B.1Scientific and Scholarly Text. . . . . . . . . . . . . . . . . . . . . . . . . . .28B.2Online Discussions and Forums . . . . . . . . . . . . . . . . . . . . . . . . . .29B.3Government and Legal Texts. . . . . . . . . . . . . . . . . . . . . . . . . . .30B.4Curated Task Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .30B.5Books in the Public Domain . . . . . . . . . . . . . . . . . . . . . . . . . . . .31B.6Open Educational Resources. . . . . . . . . . . . . . . . . . . . . . . . . . .31B.7Wikis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .32B.8Source Code . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .32B.9Transcribed Audio Content. . . . . . . . . . . . . . . . . . . . . . . . . . . .33B.10 Web Text . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .33\n\nC Additional insights on licensing33C.1Why we can’t always trust automatic license detection . . . . . . . . . . . . . .34\n\nD List of Data Provenance Initiative sources34\n\nEList of News sources44\n\nFList of WikiMedia wikis44\n\nG CCCC Source Statistics44\n\nH PeS2o Source Statistics46\n\nIGrowth rates of openly licensed data47\n\nJDetails on filtering pipelines47\n\nK Details on Comma’s pre-training data mixture49\n\nLDetails on Comma’s cool-down data mixture51\n\nM Details on small-scale data ablations52\n\nN Additional Comma results52\n\nO Additional training runs53O.1 Ablations at 1T Tokens. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .53\n\n27\n\nAContributions\n\nFigure 5: Author contributions to this work. Large squares indicate a major contribution and smallsquares indicate a supporting contribution.\n\nBDetailed Description of Sources\n\nBelow, we give a more in-depth overview of the sources that make up the Common Pile, includingspecific license decisions and tools used during collection.\n\nB.1Scientific and Scholarly Text\n\nScientific and scholarly texts are a staple of modern LLM pretraining corpora, appearing in nearly alllarge-scale datasets [e.g. 57, 185, 167] since they expose models to technical terminology, formalreasoning, and long-range document structure—skills that are essential for downstream tasks inscience, education, and question answering. Thanks to open access mandates and academic culturalnorms, many scholarly texts are either in the public domain or are distributed under open licenses.\n\npeS2oTo ensure broad coverage across many scientific disciplines, we include a version ofpeS2o [166] restricted to openly licensed articles. pes2o is derived from S2ORC [104], a cor-pus of openly licensed abstract and full-text papers that have been converted to a structured formatusing Grobid [63]. Starting from Grobid’s XML output, peS2o filters papers that are too short, haveincorrect metadata, are in languages other than English, and contain OCR errors using a combinationof heuristic- and model-based filtering steps. We refer the reader to the datasheet and code for moredetails on this processing pipeline. The subset of peS2o included in the Common Pile starts fromv3 of the corpus, which contains documents from January 1, 1970 to October 6, 2024. We retainfull-text papers with CC BY, CC BY-SA, or CC0 licenses, or that have been labeled as public domain;metadata is provided by the Semantic Scholar APIs [87]. After filtering, this set contains 6.3 million\n\n28\n\npapers, or 35.7 billion whitespace-separated segments. We provide more details on the compositionof this subset in Appendix H.\n\nPubMedPubMed Central (PMC) is an open-access archive of biomedical and life sciences researchpapers maintained by the U.S. National Institutes of Health’s National Library of Medicine. Wecollected papers from PMC whose metadata indicated that the publishing journal had designated aCC BY, CC BY-SA, or CC0 license. PMC stores the text content of each article as a single XML file,which we convert to markdown using pandoc.\n\nArXiv PapersArXiv is an online open-access repository of over 2.4 million scholarly paperscovering fields such as computer science, mathematics, physics, quantitative biology, economics,and more. When uploading papers, authors can choose from a variety of licenses. We includedtext from all papers uploaded under CC BY, CC BY-SA, and CC0 licenses in the Common Pilethrough a three-step pipeline: first, the latex source files for openly licensed papers were downloadedfrom ArXiv’s bulk-access S3 bucket; next, the LATEXML conversion tool was used to convert thesesource files into a single HTML document; finally, the HTML was converted to plaintext using theTrafilatura [10] HTML-processing library.\n\nArXiv AbstractsEach paper uploaded to ArXiv includes structured metadata fields, including anabstract summarizing the paper’s findings and contributions. According to ArXiv’s licensing policy,the metadata for any paper submitted to ArXiv is distributed under the CC0 license, regardless ofthe license of the paper itself. Thus, we include as an additional source the abstracts for every papersubmitted to ArXiv. We source the abstracts from ArXiv’s API via the Open Archives InitiativeProtocol for Metadata Harvesting endpoint and reproduce them as-is.\n\nB.2Online Discussions and Forums\n\nOnline forums are a rich source of multi-turn, user-generated dialogue covering a wide range oftopics. These platforms often feature question–answer pairs, problem-solving discussions, andinformal explanations of technical and non-technical concepts. The Common Pile incorporates onlinediscussions from sources that distribute content under an open license.\n\nStackExchangeWhile StackExchange formerly provided structured XML dumps of all of theircontent, since July of 2024, StackExchange has stopped publishing dumps to the Internet Archive.Instead, each site can provide a logged-in user with a custom URL to download the dump for thatsite. This means that dumps for defunct sites like windowsphone.stackexchange.com are inaccessible.Additionally, in dumps produced by the new export tool, many questions that are available in pastdumps (and accessible on the site) are not present. We therefore extract all questions and answersfrom community uploaded dumps from December of 2024 from the Internet Archive and additionallyextract missing questions and answers from the last official dumps in July of 2024 to account for thedeficiencies listed above. We use a question, its comments, its answers and the comments on eachanswer as a single document. Following the display order on StackExchange, answers are ordered bythe number of votes they received, with the exception that the “accepted answer” always appears first.PyMarkdown was used to convert each comment into plain text.\n\nGitHub ArchiveAccording to GitHub’s terms of service, issues and pull request descriptions—along with their comments—inherit the license of their associated repository. To collect this data,we used the GitHub Archive’s public BigQuery table of events to extracted all issue, pull request,and comment events since 2011 and aggregated them into threads. The table does not include “edit”events so the text from each comment is the original from when it was first posted. We filteredout comments from bots. This resulted in approximately 177 million threads across 19 millionrepositories. We then removed threads whose repositories did not have a Blue Oak Council-approvedlicense. License information for each repository comes from either 1) the “public-data:github_repos”BigQuery Table, 2) metadata from the StackV2, or 3) the GitHub API. License filtering left 10 millionrepositories. PyMarkdown was used to convert from GitHub-flavored markdown to plain text. Whenparsing failed, the raw markdown was kept.\n\nUbuntu IRCLogs of all discussions on the Ubuntu-hosted Internet Relay Chat (IRC) since 2004have been archived and released into the Public Domain. We downloaded all chats from all channelsup until March of 2025. We consider all messages for given channel on a given day as a singledocument. We removed system messages as well as those from known bots.\n\n29\n\nB.3Government and Legal Texts\n\nGovernments produce a vast amount of informational text, ranging from legislation and legal opinionsto scientific reports, public communications, and regulatory notices. This content is explicitly intendedto inform the public, and as such, in many jurisdictions it is published directly into the public domainor under open licenses. In the United States, for example, works authored by federal employees aspart of their official duties are not subject to copyright. Government and legal texts offer languagemodels exposure to formal argumentation, legal reasoning, and procedural language.\n\nUS Government Publishing OfficeThe United States Government Publishing Office (USGPO) isa federal agency responsible for disseminating official documents authored by the U.S. government.The Common Pile v0.1 includes all plain-text documents made available through the USGPO’sGovInfo.gov developer API. This collection comprises over 2.7 million documents, spanning issuesof the Federal Register, congressional hearing transcripts, budget reports, economic indicators, andother federal publications.\n\nUS Patents and Trademark OfficeIn the US, patent documents are released into the publicdomain as government works. Patents follow a highly standardized format with distinct requiredsections for background, detailed description, and claims. We include parents from the US Patentsand Trademark Office (USPTO) as provided by the Google Patents Public Data dataset [77], whichincludes millions of granted patents and published patent applications dating back to 1782. Weprocessed these documents to extract clean text while preserving this structured format. Mathematicalexpressions and equations were converted into LATEX.\n\nCaselaw Access Project and Court ListenerThe Common Pile contains 6.7 million cases fromthe Caselaw Access Project and Court Listener. The Caselaw Access Project consists of nearly 40million pages of U.S. federal and state court decisions and judges’ opinions from the last 365 years.In addition, Court Listener adds over 900 thousand cases scraped from 479 courts. The CaselawAccess Project and Court Listener source legal data from a wide variety of resources such as theHarvard Law Library, the Law Library of Congress, and the Supreme Court Database. From thesesources, we only included documents that were in the public domain. Erroneous OCR errors werefurther corrected after digitization, and additional post-processing was done to fix formatting andparsing.\n\nUK HansardHansard represents the official record of parliamentary proceedings across theUnited Kingdom’s legislative bodies. The Common Pile incorporates records from multiple sources,including debates and written answers from the UK Commons and Lords, devolved legislatures(Scottish Parliament, Senedd in both English and Welsh, Northern Ireland Assembly), LondonMayor’s Questions, and ministerial statements. Data was sourced from ParlParse [131], coveringCommons debates from 1918 forward and Lords proceedings from the 1999 reform. Each documentwas processed to preserve complete parliamentary sessions as cohesive units, maintaining the naturalflow of debate. All content is published under the Open Parliament License [181].\n\nRegulations.govRegulations.gov is an online platform operated by the U.S. General ServicesAdministration that collates newly proposed rules and regulations from federal agencies along withcomments and feedback from the general public. The Common Pile includes all plain-text regulatorydocuments published by U.S. federal agencies on this platform, acquired via the bulk downloadinterface provided by Regulations.gov.\n\nB.4Curated Task Data\n\nCurated datasets that cover specific tasks such as question answering, summarization, or text classifi-cation are often released via open licenses to the research community. While not traditionally partof pretraining corpora, including a small amount of task-oriented data during pretraining can helpmodels acquire early familiarity with task formats and prompt–completion structures.\n\nData Provenance InitiativeThe Data Provenance Initiative is a digital library of superviseddatasets that have been manually annotated with their source and license information [106, 109].We leverage their tooling to filter HuggingFace datasets, based on a range of criteria, includingtheir licenses, which may be particularly relevant for supervised datasets [114]. Specifically, wefilter the data according to these criteria: contains English language or code data, the text is notmodel-generated, the dataset’s audit yielded a open license and the original sources of the data areonly from recognized public domain sources.\n\n30\n\nB.5Books in the Public Domain\n\nBooks represent a time-tested resource for language model pretraining, offering carefully edited,long-form prose that supports learning of narrative coherence and long-range dependency modeling.For these reasons, many large-scale pretraining corpora—including the Pile [57], Dolma [167],and RedPajama [185]—include content from books [41]. In the United States, as of 2024, bookspublished prior to 1929 are in the public domain. Thus, the Common Pile includes public domainbooks drawn from curated collections, covering topics such as literature, science, and history.\n\nBiodiversity Heritage LibraryThe Biodiversity Heritage Library (BHL) is an open-access digitallibrary for biodiversity literature and archives. The Common Pile contains over 42 million publicdomain books and documents from the BHL collection. These works were collected using the bulkdata download interface provided by the BHL and were filtered based on their associated licensemetadata. We use the optical character recognition (OCR)-generated text distributed by BHL.\n\nPre-1929 BooksBooks published in the US before 1929 passed into the public domain on January1, 2024. We used the bibliographic catalog Hathifiles produced by HathiTrust to identify digitizedbooks which were published in the US before 1929. The collection contains over 130,000 booksdigitized and processed by the Internet Archive on behalf of HathiTrust member libraries. The OCRplain text files were downloaded directly from the Internet Archive website.\n\nLibrary of CongressThe Library of Congress (LoC) curates a collection of public domain bookscalled “Selected Digitized Books”. We downloaded over 130,000 English-language books from thispublic domain collection as OCR plain text files using the LoC APIs.\n\nProject GutenbergProject Gutenberg is an online collection of over 75,000 digitized booksavailable as plain text. We use all books that are 1) English and 2) marked as in the Public Domainaccording to the provided metadata. Additionally, we include any books that are part of the pg19 [140]dataset, which only includes books that are over 100 years old. Minimal preprocessing is appliedto remove the Project Gutenberg header and footers, and many scanned books include preambleinformation about who digitized them.\n\nB.6Open Educational Resources\n\nOpen Educational Resources (OERs) are educational materials, typically published under openlicenses, to support free and equitable access to education. These resources include educationalartifacts such as textbooks, lecture notes, lesson plans, syllabi, and problem sets. For languagemodels, OERs offer exposure to instructional formatting and domain-specific information, makingthem valuable for improving performance on knowledge-based downstream tasks. The Common Pileincludes a range of such materials sourced from major OER repositories, including collections ofopen-access books and structured teaching resources.\n\nDirectory of Open Access BooksThe Directory of Open Access Books (DOAB) is an onlineindex of over 94,000 peer-reviewed books curated from trusted open-access publishers. To collectthe openly licensed content from DOAB, we retrieve metadata using their official metadata feed.We then filter the collection to include only English-language books released under CC BY and CCBY-SA licenses. The filtered books are downloaded in PDF format and converted to plaintext usingthe Marker PDF-to-text converter. As an additional validation step, we manually create a whitelist ofopen license statements and retain only texts explicitly containing one of these statements in theirfront- or back-matter.\n\nPressBooksPressBooks is a searchable catalog of over 8,000 open access books. To collect openlylicensed content from PressBooks we construct a search query to retrieve URLs for all books writtenin English and listed as public domain or under CC BY or CC BY-SA licenses. For each matchedbook, we collect its contents directly from the publicly available web version provided by PressBooks.\n\nOERCommonsOERCommons is an online platform where educators share open-access instruc-tional materials—such as textbooks, lesson plans, problem sets, course syllabi, and worksheets—withthe goal of expanding access to affordable education. To collect the openly licensed content availableon OERCommons, we construct a search query to retrieve English-language content released into thepublic domain or under CC BY or CC BY-SA licenses. The resulting documents are converted toplain text directly from the HTML pages hosted on the OERCommons website.\n\n31\n\nLibreTextsLibreTexts is an online platform that provides a catalog of over 3,000 open-accesstextbooks. To collect openly licensed content from LibreTexts we gather links to all textbooks inthe catalog and check each textbook section for a license statement indicating that it is in the publicdomain or under a CC BY, CC BY-SA, or the GNU Free Documentation License. We extract plaintext from these textbook sections directly from the HTML pages hosted on the LibreTexts website.\n\nB.7Wikis\n\nWikis are collaboratively maintained websites that organize information around specific topics ordomains. Their crowd-sourced nature, coupled with community moderation and citation requirements,often results in text that is both informative and well-structured. Prominent examples such asWikipedia have become staples in large-scale language model pretraining corpora due to their breadthof coverage and high quality. In addition, most major wikis are distributed under open licenses suchas CC BY and CC BY-SA. The Common Pile includes content from a range of openly licensed wikisto provide models with structured and well-researched informational text.\n\nWikimediaWe downloaded the official database dumps from March 2025 of the English-languagewikis that are directly managed by the Wikimedia foundation (see Appendix F for a complete list).These database dumps include the wikitext—Mediawiki’s custom markup language—for each page aswell as talk pages, where editors discuss changes made to a page. We only use the most recent versionof each page. We converted wikitext to plain text using wtf_wikipedia after light adjustments informatting to avoid errors in section ordering caused by a bug. Before parsing, we converted wikitextmath into LATEX math using our custom code. Finally, any remaining HTML tags were removed viaregexes.\n\nWikiteamThere are many wikis on the internet that are not managed by the Wikimedia foundation,but do use their MediaWiki software to power their wiki. Many of these wikis have been archivedby wikiteam, a collection of volunteers that create unofficial database dumps of wikis and uploadthem to the Internet Archive. We download all dumps made by wikiteam when the metadata indicatesthe wiki was licensed under CC BY, CC BY-SA, or released into the public domain on the InternetArchive in September of 2024. This results in downloading approximately 330,000 wikis. Whenmultiple dumps of the same wiki exists, we use the most recent dump. The wikitext was convertedto plain text following the same steps as with Wikimedia wikis. After preprocessing, we removeddocuments from wikis that appeared to contain large amounts of license laundering, e.g. those thatwere collections of song lyrics or transcripts.\n\nB.8Source Code\n\nSource code has become an increasingly important component of large-scale language model pretrain-ing corpora, as it enables models to learn syntax, program structure, and problem solving strategiesuseful for both code generation and reasoning tasks. Thanks to the Free and Open Source Software(FOSS) movement, code also happens to be one of the most openly licensed forms of text, withmany software repositories distributed under open licenses such as MIT, BSD, Apache 2.0, and theGNU Free Documentation License (GFDL). The Common Pile includes high-quality, openly licensedsource code from large-scale public code datasets and documentation standards, enabling modelstrained on it to perform better on coding and technical writing tasks.\n\nThe Stack V2The Stack V2 [113] consists of a mixture of openly licensed and unlicensed work.We use the tooling that the Software Heritage Foundation and BigCode created to build our dataset.In particular, we relied on the license detection performed by the creators of Stack V2. When multiplelicenses are detected in a single repository, we make sure that all of them meet our definition of“openly licensed”.\n\nPython Enhancement ProposalsPython Enhancement Proposals, or PEPs, are design documentsthat generally provide a technical specification and rationale for new features of the Python program-ming language. There are been 661 PEPs published. The majority of PEPs are published in the PublicDomain, but 5 were published under the “Open Publication License” and omitted. PEPs are long,highly-polished, and technical in nature and often include code examples paired with their prose.PEPs are authored in ReStructured Text; we used pandoc, version 3.5, to convert them to plain text.\n\n32\n\nB.9Transcribed Audio Content\n\nA historically underutilized source of text data is speech transcribed from audio and video content.Spoken language in educational videos, speeches, and interviews provide an opportunity for modelsto learn conversational speech patterns.\n\nCreative Commons YouTubeYouTube is large-scale video-sharing platform where users have theoption of uploading content under a CC BY license. To collect high-quality speech-based textualcontent and combat the rampant license laundering on YouTube, we manually curated a set of over2,000 YouTube channels that consistently release original openly licensed content containing speech.The resulting collection spans a wide range of genres, including lectures, tutorials, reviews, videoessays, speeches, and vlogs. From these channels, we retrieved over 1.1 million openly licensedvideos comprising more than 470,000 hours of content. Finally, each video was transcribed to textusing the Whisper speech recognition model [138].\n\nB.10Web Text\n\nThe success of modern LLM pre-training relies on text scraped indiscriminately from the web, asweb text covers an extremely diverse range of textual domains. In the Common Pile, we restrict thisapproach to only include web content with clear public domain status or open license statements.\n\nCreative Commons Common CrawlWe sourced text from 52 Common Crawl snapshots, coveringabout half of Common Crawl snapshots available to date and covering all years of operations ofCommon Crawl up to 2024. We found a higher level of duplication across this collection, suggestingthat including more snapshots would lead to a modest increase in total token yield. From thesesnapshots, we extract HTML content using FastWarc [15]. Then, using a regular expression adaptedfrom the C4Corpus project [68], we retain only those pages where a CC BY, CC BY-SA, or CC0license appears. To ensure license accuracy, we manually verified the top 1000 domains by contentvolume, retaining only the 537 domains with confirmed licenses where the Creative Commonsdesignation is applied to all text content rather than only embedded media or a subset of the texton the domain. We extract the main content of these documents and remove boilerplate usingResiliparse [14]. We perform URL-level exact deduplication and use Bloom filters to remove near-duplicates with 80% ngram overlap. We also employ rule-based filters matching Dolma [167];namely, we use C4-derived heuristics [142] to filter pages containing Javascript, Lorem Ipsum, andcurly braces {}. We also apply all Gopher rules [141] to remove low-quality pages. We provide moredetails on the composition of this subset in Appendix G.\n\nFoodistaFoodista is a community-maintained site with recipes, food-related news, and nutritioninformation. All content is licensed under CC BY. Plain text is extracted from the HTML using acustom pipeline that includes extracting title and author information to include at the beginning ofthe text. Additionally, comments on the page are appended to the article after we filter automaticallygenerated comments.\n\nNewsWe scrape the news sites that publish content under CC BY or CC BY-SA according toopennewswire. A full list of sites can be found in Appendix E. Plain text was extracted fromthe HTML using our custom pipeline, including extraction of the title and byline to include at thebeginning of each article.\n\nPublic Domain ReviewThe Public Domain Review is an online journal dedicated to explorationof works of art and literature that have aged into the public domain. We collect all articles publishedin the Public Domain Review under a CC BY-SA license.\n\nCAdditional insights on licensing\n\nThere are many standards we could have chosen for what licenses to include in our dataset. The opensource, knowledge, and culture movements have harmonized on the high level principles described insection 1: “open” means that permission is granted for content to be freely used, studied, modified,and shared for any purpose. This language is found in the Open Knowledge Definition we followas well as the Open Source Institute’s Open Definition, Creative Commons’s statement on OpenCulture, Wikimedia’s Acceptable licenses policy and more. Our work was also developed to beconsistent with the Open movement’s work in the specific context of AI technologies such as the\n\n33\n\nOpen Source Initiative’s Open Source AI Definition and in consultations with leading members ofthe community [9].\n\nC.1Why we can’t always trust automatic license detection\n\nThere are many reasons why identifying the licensing status of internet text with automatic toolingcan be challenging. In this section, we briefly discuss some major themes from our experience.\n\nThere are many ways to say the same thing.While there exist standards for how to express alicense, people don’t always follow those standards and failure to follow the standards doesn’t meanthat the license is invalid. For example, simple string matching on “CC BY” misses a huge amount ofCC BY licensed text because a very common way to denote Creative Commons licenses is using animage badge. Current web-processing tools are substantially stronger at identifying text than images,and the failure rate on sites using image badges is quite high.\n\nLack of understanding of licenses.Most people are not lawyers and do not understand the fulllegal scope and meaning of the licenses that they attempt to put on their text. Developers routinelytweak boilerplate to produce ambiguous language like (“Licensed under MIT-ish terms”) or writecontradictory statements (“All rights reserved / CC-BY”). In general, it is common for people to writequasi-legal language along side a more traditional license. Non-standard licenses require substantialamounts of work to interpret and are not always valid or meaningful.\n\nLicensing signals can be noisy.Even when a developer intends to clearly communicate a specificlicense, contradictions and errors can occur in practice. For example, Longpre et al. [107] found thatthere were substantial disagreements between the terms of service of a website and the restrictionsfound in a robots.txt file. We have not yet found a reliable way to have an automatic system identifylicensed text and therefore frequently resort to manual review by humans.\n\nDList of Data Provenance Initiative sources\n\nThe openly licensed supervised datasets included in the Common Pile are listed in Table 1. Thesedatasets were identified and collected using metadata from the Data Provenance Initiative. For moreinformation on these datasets, consult the Data Provenance Initiative Dataset Explorer.\n\nTable 1: Supervised datasets included in the Common Pile from the Data Provenance Initiativecollection.\n\nCollectionDataset IdentifierLicenses\n\nAgentInstructAgentInstruct-alfworld[164]MIT License\n\nHelpSteerHelpSteer[184]CC BY 4.0\n\nAya Datasetaya-english[165]Apache License 2.0\n\nCommitPackFTcommitpackft-abap[165]MIT License\n\nCommitPackFTcommitpackft-agda[165]MIT License, BSD 3-Clause License\n\nCommitPackFTcommitpackft-apl[165]MIT License, ISC License\n\nCommitPackFTcommitpackft-arc[165]MIT License\n\nCommitPackFTcommitpackft-aspectj[165]Apache License 2.0, BSD 3-ClauseLicense, MIT License\n\nCommitPackFTcommitpackft-ats[165]Apache License 2.0, MIT License\n\nCommitPackFTcommitpackft-blitzmax[165]MIT License\n\nCommitPackFTcommitpackft-bluespec[165]MIT License\n\nCommitPackFTcommitpackft-boo[165]MIT License\n\nContinued on next page\n\n34\n\nCollectionDataset IdentifierLicenses\n\nCommitPackFTcommitpackft-brainfuck[165]Apache License 2.0, BSD 2-ClauseLicense, MIT License\n\nCommitPackFTcommitpackft-bro[165]MIT License, BSD 3-Clause License\n\nCommitPackFTcommitpackft-cartocss[165]MIT License\n\nCommitPackFTcommitpackft-chapel[165]Apache License 2.0, BSD 3-ClauseLicense, MIT License\n\nCommitPackFTcommitpackft-clean[165]Apache License 2.0, MIT License\n\nCommitPackFTcommitpackft-coldfusion[165]Apache License 2.0, MIT License\n\nCommitPackFTcommitpackft-creole[165]Apache License 2.0, MIT License\n\nCommitPackFTcommitpackft-crystal[165]Apache License 2.0, MIT License\n\nCommitPackFTcommitpackft-dns-zone[165]MIT License, BSD 3-Clause License\n\nCommitPackFTcommitpackft-dylan[165]MIT License\n\nCommitPackFTcommitpackft-eiffel[165]MIT License\n\nCommitPackFTcommitpackft-emberscript[165]Apache License 2.0, BSD 3-ClauseLicense, MIT License\n\nCommitPackFTcommitpackft-fancy[165]MIT License, BSD 3-Clause License\n\nCommitPackFTcommitpackft-flux[165]Apache License 2.0, MIT License\n\nCommitPackFTcommitpackft-forth[165]MIT License\n\nCommitPackFTcommitpackft-g-code[165]Apache License 2.0, BSD 3-ClauseLicense, MIT License\n\nCommitPackFTcommitpackft-gdscript[165]Apache License 2.0, CC0 1.0, MITLicense\n\nCommitPackFTcommitpackft-genshi[165]Apache License 2.0, MIT License\n\nCommitPackFTcommitpackft-graphql[165]Apache License 2.0, BSD 3-ClauseLicense, CC0 1.0, MIT License\n\nCommitPackFTcommitpackft-harbour[165]MIT License\n\nCommitPackFTcommitpackft-hlsl[165]Apache License 2.0, MIT License\n\nCommitPackFTcommitpackft-http[165]Apache License 2.0, MIT License\n\nCommitPackFTcommitpackft-idris[165]MIT License, BSD 3-ClauseLicense, BSD 2-Clause License\n\nCommitPackFTcommitpackft-igor-pro[165]MIT License, BSD 3-Clause License\n\nCommitPackFTcommitpackft-inform-7[165]MIT License, BSD 3-Clause License\n\nCommitPackFTcommitpackft-ioke[165]MIT License\n\nCommitPackFTcommitpackft-isabelle[165]MIT License, BSD 2-Clause License\n\nCommitPackFTcommitpackft-jflex[165]MIT License\n\nCommitPackFTcommitpackft-json5[165]MIT License, BSD 3-ClauseLicense, BSD 2-Clause License\n\nCommitPackFTcommitpackft-jsonld[165]Apache License 2.0, BSD 3-ClauseLicense, CC0 1.0, MIT License\n\nCommitPackFTcommitpackft-krl[165]MIT License\n\nCommitPackFTcommitpackft-latte[165]MIT License\n\nCommitPackFTcommitpackft-lean[165]Apache License 2.0, MIT License\n\nCommitPackFTcommitpackft-lfe[165]Apache License 2.0, MIT License\n\nContinued on next page\n\n35\n\nCollectionDataset IdentifierLicenses\n\nCommitPackFTcommitpackft-lilypond[165]MIT License\n\nCommitPackFTcommitpackft-liquid[165]Apache License 2.0, CC0 1.0, MITLicense\n\nCommitPackFTcommitpackft-literate-agda[165]MIT License\n\nCommitPackFTcommitpackft-literate-coffeescript[165]MIT License\n\nCommitPackFTcommitpackft-literate-haskell[165]MIT License, BSD 3-Clause License\n\nCommitPackFTcommitpackft-llvm[165]Apache License 2.0, BSD 3-ClauseLicense, BSD 2-Clause License,MIT License\n\nCommitPackFTcommitpackft-logos[165]Apache License 2.0, BSD 3-ClauseLicense, BSD 2-Clause License,MIT License, ISC License\n\nCommitPackFTcommitpackft-lsl[165]MIT License, BSD 3-Clause License\n\nCommitPackFTcommitpackft-maple[165]MIT License, BSD 3-Clause License\n\nCommitPackFTcommitpackft-mathematica[165]MIT License, CC0 1.0\n\nCommitPackFTcommitpackft-metal[165]Apache License 2.0, MIT License\n\nCommitPackFTcommitpackft-mirah[165]Apache License 2.0, MIT License\n\nCommitPackFTcommitpackft-monkey[165]Apache License 2.0, MIT License\n\nCommitPackFTcommitpackft-moonscript[165]MIT License\n\nCommitPackFTcommitpackft-mtml[165]MIT License\n\nCommitPackFTcommitpackft-mupad[165]Apache License 2.0, BSD 3-ClauseLicense, MIT License\n\nCommitPackFTcommitpackft-nesc[165]MIT License\n\nCommitPackFTcommitpackft-netlinx[165]MIT License\n\nCommitPackFTcommitpackft-ninja[165]Apache License 2.0, BSD 3-ClauseLicense, MIT License\n\nCommitPackFTcommitpackft-nit[165]Apache License 2.0, MIT License\n\nCommitPackFTcommitpackft-nu[165]Apache License 2.0, MIT License\n\nCommitPackFTcommitpackft-ooc[165]MIT License\n\nCommitPackFTcommitpackft-openscad[165]MIT License, CC0 1.0, BSD2-Clause License\n\nCommitPackFTcommitpackft-oz[165]MIT License, BSD 2-Clause License\n\nCommitPackFTcommitpackft-pan[165]Apache License 2.0, MIT License\n\nCommitPackFTcommitpackft-piglatin[165]Apache License 2.0, MIT License\n\nCommitPackFTcommitpackft-pony[165]MIT License, BSD 2-Clause License\n\nCommitPackFTcommitpackft-propeller-spin[165]MIT License\n\nCommitPackFTcommitpackft-pure-data[165]MIT License\n\nCommitPackFTcommitpackft-purebasic[165]MIT License, BSD 3-Clause License\n\nCommitPackFTcommitpackft-purescript[165]Apache License 2.0, BSD 3-ClauseLicense, MIT License\n\nCommitPackFTcommitpackft-ragel-in-ruby-host[165]MIT License\n\nCommitPackFTcommitpackft-rebol[165]Apache License 2.0, MIT License\n\nContinued on next page\n\n36\n\nCollectionDataset IdentifierLicenses\n\nCommitPackFTcommitpackft-red[165]MIT License, BSD 2-Clause License\n\nCommitPackFTcommitpackft-rouge[165]Apache License 2.0, MIT License\n\nCommitPackFTcommitpackft-sage[165]MIT License\n\nCommitPackFTcommitpackft-sas[165]MIT License\n\nCommitPackFTcommitpackft-scaml[165]MIT License, BSD 2-Clause License\n\nCommitPackFTcommitpackft-scilab[165]MIT License, BSD 3-Clause License\n\nCommitPackFTcommitpackft-slash[165]Apache License 2.0, MIT License\n\nCommitPackFTcommitpackft-smt[165]MIT License, BSD 3-Clause License\n\nCommitPackFTcommitpackft-solidity[165]Apache License 2.0, MIT License\n\nCommitPackFTcommitpackft-sourcepawn[165]Apache License 2.0, MIT License\n\nCommitPackFTcommitpackft-squirrel[165]MIT License\n\nCommitPackFTcommitpackft-ston[165]MIT License\n\nCommitPackFTcommitpackft-systemverilog[165]Apache License 2.0, BSD 3-ClauseLicense, MIT License\n\nCommitPackFTcommitpackft-unity3d-asset[165]Apache License 2.0, BSD 3-ClauseLicense, BSD 2-Clause License,MIT License, ISC License, CC0 1.0\n\nCommitPackFTcommitpackft-uno[165]MIT License\n\nCommitPackFTcommitpackft-unrealscript[165]MIT License\n\nCommitPackFTcommitpackft-urweb[165]MIT License, BSD 3-Clause License\n\nCommitPackFTcommitpackft-vcl[165]Apache License 2.0, BSD 3-ClauseLicense, MIT License\n\nCommitPackFTcommitpackft-xbase[165]Apache License 2.0, MIT License\n\nCommitPackFTcommitpackft-xpages[165]Apache License 2.0, MIT License\n\nCommitPackFTcommitpackft-xproc[165]Apache License 2.0, MIT License\n\nCommitPackFTcommitpackft-yacc[165]MIT License, ISC License, BSD2-Clause License\n\nCommitPackFTcommitpackft-zephir[165]MIT License\n\nCommitPackFTcommitpackft-zig[165]MIT License\n\nDolly 15kdolly-brainstorming[165]CC BY-SA 3.0\n\nDolly 15kdolly-classification[165]CC BY-SA 3.0\n\nDolly 15kdolly-closedqa[165]CC BY-SA 3.0\n\nDolly 15kdolly-creative_writing[165]CC BY-SA 3.0\n\nDolly 15kdolly-infoextract[165]CC BY-SA 3.0\n\nDolly 15kdolly-openqa[165]CC BY-SA 3.0\n\nDolly 15kdolly-summarization[165]CC BY-SA 3.0\n\nDialogStudiods-ABCD[165]Apache License 2.0, MIT License\n\nDialogStudiods-ATIS[165]Apache License 2.0, CC BY 4.0\n\nDialogStudiods-ATIS-NER[165]Apache License 2.0, CC BY 4.0\n\nDialogStudiods-AirDialogue[165]Apache License 2.0\n\nDialogStudiods-AntiScam[165]Apache License 2.0, CC0 1.0\n\nDialogStudiods-BANKING77[165]Apache License 2.0, CC BY 4.0\n\nContinued on next page\n\n37\n\nCollectionDataset IdentifierLicenses\n\nDialogStudiods-BANKING77-OOS[165]Apache License 2.0, CC BY 4.0\n\nDialogStudiods-BiTOD[165]Apache License 2.0\n\nDialogStudiods-CLINC-Single-Domain-OOS-banking[165]Apache License 2.0, CC BY 3.0\n\nDialogStudiods-CLINC-Single-Domain-OOS-credit_cards[165]Apache License 2.0, CC BY 3.0\n\nDialogStudiods-CLINC150[165]Apache License 2.0, CC BY-SA 3.0\n\nDialogStudiods-CaSiNo[165]Apache License 2.0, CC BY 4.0\n\nDialogStudiods-CoQA[165]Apache License 2.0, MIT License\n\nDialogStudiods-CoSQL[165]Apache License 2.0, CC BY-SA 4.0\n\nDialogStudiods-ConvAI2[165]Apache License 2.0\n\nDialogStudiods-CraigslistBargains[165]Apache License 2.0, MIT License\n\nDialogStudiods-DART[165]Apache License 2.0, MIT License\n\nDialogStudiods-DSTC8-SGD[165]Apache License 2.0, CC BY-SA 4.0\n\nDialogStudiods-DialogSum[165]Apache License 2.0, MIT License\n\nDialogStudiods-Disambiguation[165]Apache License 2.0, MIT License\n\nDialogStudiods-FeTaQA[165]Apache License 2.0, CC BY-SA 4.0\n\nDialogStudiods-GECOR[165]Apache License 2.0, CC BY 4.0\n\nDialogStudiods-GrailQA[165]Apache License 2.0\n\nDialogStudiods-HDSA-Dialog[165]Apache License 2.0, MIT License\n\nDialogStudiods-HH-RLHF[165]Apache License 2.0, MIT License\n\nDialogStudiods-HWU64[165]Apache License 2.0, CC BY-SA 3.0\n\nDialogStudiods-HybridQA[165]Apache License 2.0, MIT License\n\nDialogStudiods-KETOD[165]Apache License 2.0, MIT License\n\nDialogStudiods-MTOP[165]Apache License 2.0, CC BY-SA 4.0\n\nDialogStudiods-MULTIWOZ2_2[165]Apache License 2.0, MIT License\n\nDialogStudiods-MulDoGO[165]Apache License 2.0, CDLAPermissive 1.0\n\nDialogStudiods-MultiWOZ_2.1[165]Apache License 2.0, MIT License\n\nDialogStudiods-Prosocial[165]Apache License 2.0, MIT License\n\nDialogStudiods-RESTAURANTS8K[165]Apache License 2.0, CC BY 4.0\n\nDialogStudiods-SGD[165]Apache License 2.0, CC BY-SA 4.0\n\nDialogStudiods-SNIPS[165]Apache License 2.0\n\nDialogStudiods-SNIPS-NER[165]Apache License 2.0\n\nDialogStudiods-SParC[165]Apache License 2.0, CC BY-SA 4.0\n\nDialogStudiods-SQA[165]Apache License 2.0, CC BY-SA 4.0\n\nDialogStudiods-STAR[165]Apache License 2.0, MIT License\n\nDialogStudiods-Spider[165]Apache License 2.0, CC BY-SA 4.0\n\nDialogStudiods-TOP[165]Apache License 2.0, CC BY-SA\n\nDialogStudiods-TOP-NER[165]Apache License 2.0, CC BY-SA\n\nDialogStudiods-Taskmaster1[165]Apache License 2.0, CC BY 4.0\n\nContinued on next page\n\n38\n\nCollectionDataset IdentifierLicenses\n\nDialogStudiods-Taskmaster2[165]Apache License 2.0, CC BY 4.0\n\nDialogStudiods-Taskmaster3[165]Apache License 2.0, CC BY 4.0\n\nDialogStudiods-ToTTo[165]Apache License 2.0, CC BY-SA 3.0\n\nDialogStudiods-TweetSumm[165]Apache License 2.0, CC0 1.0\n\nDialogStudiods-WOZ2_0[165]Apache License 2.0\n\nDialogStudiods-WebQSP[165]Apache License 2.0, CC BY 4.0\n\nDialogStudiods-WikiSQL[165]Apache License 2.0, BSD 3-ClauseLicense\n\nDialogStudiods-WikiTQ[165]Apache License 2.0, CC BY-SA 4.0\n\nDialogStudiods-chitchat-dataset[165]Apache License 2.0, MIT License\n\nDialogStudiods-wizard_of_internet[165]Apache License 2.0, CC BY 4.0\n\nDialogStudiods-wizard_of_wikipedia[165]Apache License 2.0, CC BY 4.0\n\nFlan Collection (Chain-of-Thought)fc-cot-cot_gsm8k[34]MIT License\n\nFlan Collection (Chain-of-Thought)fc-cot-cot_strategyqa[59]CC BY-SA 3.0\n\nFlan Collection (Chain-of-Thought)fc-cot-stream_creak[125]MIT License, CC BY-SA 4.0\n\nFlan Collection (Chain-of-Thought)fc-cot-stream_esnli[21]MIT License, CC BY-SA 4.0\n\nFlan Collection (Flan 2021)fc-flan-drop[46]CC BY 4.0\n\nFlan Collection (Flan 2021)fc-flan-e2e_nlg[123]CC BY-SA 4.0\n\nFlan Collection (Flan 2021)fc-flan-natural_questions[89]Apache License 2.0, CC BY-SA 3.0\n\nFlan Collection (Flan 2021)fc-flan-quac[29]CC BY-SA 4.0\n\nFlan Collection (Flan 2021)fc-flan-squad_v1[147]CC BY-SA 4.0\n\nFlan Collection (Flan 2021)fc-flan-squad_v2[147]CC BY-SA 4.0\n\nFlan Collection (Flan 2021)fc-flan-trec[100]CC0 1.0\n\nFlan Collection (Flan 2021)fc-flan-true_case[100]CC0 1.0\n\nFlan Collection (Flan 2021)fc-flan-wiki_lingua_english_en[90]CC BY 3.0\n\nFlan Collection (Flan 2021)fc-flan-winogrande[158]Apache License 2.0, CC BY 4.0\n\nFlan Collection (Flan 2021)fc-flan-wnli[98]CC BY 4.0\n\nFlan Collection (Flan 2021)fc-flan-word_segment[98]CC0 1.0\n\nFlan Collection (Flan 2021)fc-flan-wsc[98]CC BY 4.0\n\nFlan Collection (P3)fc-p3-adversarial_qa[11]CC BY-SA 3.0\n\nFlan Collection (P3)fc-p3-cos_e[144]BSD 3-Clause License\n\nFlan Collection (P3)fc-p3-dbpedia_14[97]CC BY-SA 3.0\n\nFlan Collection (P3)fc-p3-hotpotqa[190]Apache License 2.0, CC BY-SA 4.0\n\nFlan Collection (P3)fc-p3-quarel[153]CC BY 4.0\n\nFlan Collection (P3)fc-p3-quartz[170]CC BY 4.0\n\nFlan Collection (P3)fc-p3-quoref[44]CC BY 4.0\n\nFlan Collection (P3)fc-p3-web_questions[13]CC BY 4.0\n\nFlan Collection (P3)fc-p3-wiki_bio[93]CC BY-SA 3.0\n\nFlan Collection (P3)fc-p3-wiki_hop[93]CC BY-SA 3.0\n\nfc-sni-adversarial_qa[11]CC BY-SA 3.0\n\nContinued on next page\n\n39\n\nCollectionDataset IdentifierLicenses\n\nfc-sni-adverserial_qa[11]MIT License\n\nfc-sni-air_dialogue[187]Apache License 2.0\n\nfc-sni-ancora_ca_ner[187]CC BY 4.0\n\nfc-sni-anem[124]MIT License, CC BY-SA 3.0\n\nfc-sni-argkpApache License 2.0, CC BY-SA 3.0\n\nfc-sni-asian_language_-treebank[151]CC BY 4.0\n\nfc-sni-atomic[75]CC BY 4.0\n\nfc-sni-bard[54]Apache License 2.0\n\nfc-sni-cedr[161]Apache License 2.0\n\nfc-sni-circa[112]CC BY-SA 4.0\n\nfc-sni-clue_cmrc2018[42]CC BY-SA 4.0\n\nfc-sni-coached_conv_pref[139]CC BY 4.0\n\nfc-sni-copa_hrBSD 2-Clause License\n\nfc-sni-crows_pairs[122]CC BY-SA 4.0\n\nfc-sni-cuad[71]CC BY 4.0\n\nfc-sni-defeasible_nli_atomic[155]MIT License\n\nfc-sni-disfl_qa[67]CC BY 4.0\n\nfc-sni-e_snli[21]MIT License\n\nfc-sni-gap[186]Apache License 2.0\n\nfc-sni-hotpotqa[190]Apache License 2.0, CC BY-SA 4.0\n\nfc-sni-human_ratings_of_natural_-language_generation_outputs[190]CC BY 4.0\n\nfc-sni-hybridqa[26]CC BY 4.0, MIT License\n\nfc-sni-iirc[53]CC BY 4.0\n\nfc-sni-jigsaw[53]CC0 1.0\n\nContinued on next page\n\n40\n\nCollectionDataset IdentifierLicenses\n\nfc-sni-librispeech_asr[127]CC BY 4.0\n\nfc-sni-logic2text[27]MIT License\n\nfc-sni-numeric_fused_head[49]MIT License\n\nfc-sni-offenseval_dravidian[49]CC BY 4.0\n\nfc-sni-open_pi[172]CC BY 4.0\n\nfc-sni-paper_reviews_data_set[172]CC BY 4.0\n\nfc-sni-poem_sentiment[163]CC BY 4.0\n\nfc-sni-propara[43]Apache License 2.0\n\nfc-sni-quarel[153]CC BY 4.0\n\nfc-sni-quartz[170]CC BY 4.0\n\nfc-sni-quoref[44]CC BY 4.0\n\nfc-sni-ro_sts_parallel[48]CC BY-SA 4.0\n\nfc-sni-schema_guided_dstc8[148]CC BY-SA 4.0\n\nfc-sni-scitail[86]Apache License 2.0\n\nfc-sni-scitailv1.1[86]Apache License 2.0\n\nfc-sni-semeval_2020_task4[86]CC BY-SA 4.0\n\nfc-sni-sms_spam_collection_v.1[86]CC BY 4.0\n\nfc-sni-splash[86]CC BY-SA 4.0\n\nfc-sni-squad2.0[147]CC BY-SA 4.0\n\nfc-sni-squad_1.1[146]CC BY-SA 4.0\n\nfc-sni-strategyqa[59]MIT License\n\nfc-sni-universal_dependencies___-english_dependency_treebank[59]CC BY-SA 4.0\n\nfc-sni-web_questions[13]CC BY 4.0\n\nfc-sni-wiki_hop[93]CC BY-SA 3.0\n\nContinued on next page\n\n41\n\nCollectionDataset IdentifierLicenses\n\nfc-sni-wikitext[116]CC BY-SA 3.0\n\nfc-sni-winograd_wsc[98]CC BY 4.0\n\nfc-sni-winomt[168]MIT License\n\nfc-sni-winowhy[194]MIT License\n\nfc-sni-wsc; enhanced_wsc[194]CC BY 4.0\n\nfc-sni-wsc_fiexed[194]CC BY-SA 3.0\n\nfc-sni-xcopa[134]CC BY 4.0\n\nfc-sni-xquad[6]CC BY-SA 4.0\n\nOpen Assistantoasst-en[88]Apache License 2.0, CC BY 4.0\n\nOpen Assistant OctoPackoasst-en-octopack[88]Apache License 2.0, CC BY 4.0\n\nOpen Assistant v2oasst2-en[88]Apache License 2.0\n\nOIGoig-unified_canadian_-parliament[88]Apache License 2.0\n\nOIGoig-unified_cuad[88]Apache License 2.0, CC BY 4.0\n\nOIGoig-unified_grade_school_math_-instructions[88]Apache License 2.0, MIT License\n\nOIGoig-unified_nq[88]Apache License 2.0, CC BY-SA 3.0\n\nOIGoig-unified_sqlv1[88]Apache License 2.0, CC BY-SA 4.0\n\nOIGoig-unified_sqlv2[88]Apache License 2.0, CC BY-SA 4.0\n\nOIGoig-unified_squad_v2_more_-neg[88]Apache License 2.0, CC BY-SA 4.0\n\nTasksource Instructtsi-balanced_copa[85]BSD 2-Clause License\n\nTasksource Instructtsi-breaking_nli[60]CC BY-SA 4.0\n\nTasksource Instructtsi-cladder[60]MIT License\n\nTasksource Instructtsi-condaqa[149]Apache License 2.0\n\nTasksource Instructtsi-conj_nli[157]MIT License\n\nTasksource Instructtsi-defeasible_nli-atomic[155]MIT License\n\nTasksource Instructtsi-defeasible_nli-snli[155]MIT License\n\nTasksource Instructtsi-dynasent-dynabench.dynasent.r1.all-r1[135]CC BY 4.0\n\nTasksource Instructtsi-dynasent-dynabench.dynasent.r2.all-r2[135]CC BY 4.0\n\nTasksource Instructtsi-fever_evidence_related-mwong_-_fever_related[177]CC BY-SA 4.0\n\nTasksource Instructtsi-few_nerd-supervised[45]CC BY-SA 4.0\n\nTasksource Instructtsi-fig_qa[102]MIT License\n\nTasksource Instructtsi-fracas[56]MIT License\n\nContinued on next page\n\n42\n\nCollectionDataset IdentifierLicenses\n\nTasksource Instructtsi-hyperpartisan_news[56]CC BY 4.0\n\nTasksource Instructtsi-lex_glue-case_hold[23]Apache License 2.0\n\nTasksource Instructtsi-lonli[174]MIT License\n\nTasksource Instructtsi-moral_stories-full[51]MIT License\n\nTasksource Instructtsi-neqa[51]CC BY 4.0\n\nTasksource Instructtsi-prostApache License 2.0\n\nTasksource Instructtsi-quote_repetitionCC BY 4.0\n\nTasksource Instructtsi-recast-recast_factualityCC BY-SA 4.0\n\nTasksource Instructtsi-recast-recast_megaveridicalityCC BY-SA 4.0\n\nTasksource Instructtsi-recast-recast_nerCC BY-SA 4.0\n\nTasksource Instructtsi-recast-recast_punsCC BY-SA 4.0\n\nTasksource Instructtsi-recast-recast_sentimentCC BY-SA 4.0\n\nTasksource Instructtsi-recast-recast_verbcornerCC BY-SA 4.0\n\nTasksource Instructtsi-recast-recast_verbnetCC BY-SA 4.0\n\nTasksource Instructtsi-redefine_mathCC BY 4.0\n\nTasksource Instructtsi-tracie[196]Apache License 2.0\n\nTasksource Instructtsi-truthful_qa-multiple_-choice[101]Apache License 2.0\n\nTasksource Instructtsi-vitaminc-tals__vitaminc[162]MIT License\n\nTasksource Instructtsi-winowhy[194]MIT License\n\nTasksource Symbol-Tuningtsy-breaking_nli[60]CC BY-SA 4.0\n\nTasksource Symbol-Tuningtsy-cladder[60]MIT License\n\nTasksource Symbol-Tuningtsy-condaqa[149]Apache License 2.0\n\nTasksource Symbol-Tuningtsy-conj_nli[157]MIT License\n\nTasksource Symbol-Tuningtsy-defeasible_nli-atomic[155]MIT License\n\nTasksource Symbol-Tuningtsy-defeasible_nli-snli[155]MIT License\n\nTasksource Symbol-Tuningtsy-dynasent-dynabench.dynasent.r1.all-r1[135]CC BY 4.0\n\nTasksource Symbol-Tuningtsy-dynasent-dynabench.dynasent.r2.all-r2[135]CC BY 4.0\n\nTasksource Symbol-Tuningtsy-fever_evidence_related-mwong__fever_related[177]CC BY-SA 4.0\n\nTasksource Symbol-Tuningtsy-fracas[56]MIT License\n\nTasksource Symbol-Tuningtsy-hyperpartisan_news[56]CC BY 4.0\n\nTasksource Symbol-Tuningtsy-lonli[174]MIT License\n\nTasksource Symbol-Tuningtsy-recast-recast_factuality[174]CC BY-SA 4.0\n\nTasksource Symbol-Tuningtsy-recast-recast_-megaveridicality[174]CC BY-SA 4.0\n\nTasksource Symbol-Tuningtsy-recast-recast_ner[174]CC BY-SA 4.0\n\nTasksource Symbol-Tuningtsy-recast-recast_puns[174]CC BY-SA 4.0\n\nTasksource Symbol-Tuningtsy-recast-recast_sentiment[174]CC BY-SA 4.0\n\nTasksource Symbol-Tuningtsy-recast-recast_verbcorner[174]CC BY-SA 4.0\n\nContinued on next page\n\n43\n\nCollectionDataset IdentifierLicenses\n\nTasksource Symbol-Tuningtsy-recast-recast_verbnet[174]CC BY-SA 4.0\n\nTasksource Symbol-Tuningtsy-tracie[196]Apache License 2.0\n\nTasksource Symbol-Tuningtsy-vitaminc-tals__vitaminc[162]MIT License\n\nTasksource Symbol-Tuningtsy-winowhy[194]MIT License\n\nEList of News sources\n\nThe Common Pile contains a variety of openly licensed news sources released under CC BY andCC BY-SA licenses. The sources licensed under CC BY include: 360info, Africa is a Country, AltNews, Balkan Diskurs, Factly, Freedom of the Press Foundation, Agenzia Fides, Global Voices,Meduza, Mekong Eye, Milwaukee Neighborhood News Service, Minority Africa, New CanadianMedia, SciDev.Net, The Solutions Journalism Exchange, Tasnim News Agency, and ZimFact. Thesources licensed under CC BY-SA include: Oxpeckers, Propastop, and The Public Record.\n\nFList of WikiMedia wikis\n\nOfficial Wikimedia wikis are released under a CC BY-SA license. The Common Pile includes thefollowing Wikimedia wikis: Wikipedia, Wikinews, Wikibooks, Wikiquote, Wikisource, Wikiversity,Wikivoyage, and Wiktionary.\n\nGCCCC Source Statistics\n\nWe provide additional statistics on the CCCC subset of the Common Pile, including the number ofunicode words and documents sourced from each Common Crawl snapshot, in Table 2.\n\nTable 2: Counts of words and documents extracted from 52 snapshots after filtering with our pipeline.\n\nSnapshotUnicode WordsDocuments\n\nCC-MAIN-2013-203,851,018,1975,529,294\n\nCC-MAIN-2013-484,544,197,2526,997,831\n\nCC-MAIN-2014-104,429,217,9416,682,672\n\nCC-MAIN-2014-154,059,132,8735,912,779\n\nCC-MAIN-2014-235,193,195,7658,253,690\n\nCC-MAIN-2014-354,254,690,9456,551,673\n\nCC-MAIN-2014-414,289,814,4496,558,170\n\nCC-MAIN-2014-423,986,284,7416,144,797\n\nCC-MAIN-2014-493,316,075,4524,699,472\n\nCC-MAIN-2014-524,307,765,2896,338,983\n\nCC-MAIN-2015-063,675,982,6795,181,955\n\nCC-MAIN-2015-113,932,442,9005,438,533\n\nCC-MAIN-2015-143,658,107,7654,954,273\n\nContinued on next page\n\n44\n\nSnapshotUnicode WordsDocuments\n\nCC-MAIN-2015-184,451,734,9466,319,757\n\nCC-MAIN-2015-224,285,945,3195,949,267\n\nCC-MAIN-2015-273,639,904,1284,975,152\n\nCC-MAIN-2016-071,588,496,7033,798,207\n\nCC-MAIN-2016-183,228,754,2004,446,815\n\nCC-MAIN-2016-223,217,827,6764,242,762\n\nCC-MAIN-2017-043,852,699,2135,239,605\n\nCC-MAIN-2017-094,186,915,4985,119,171\n\nCC-MAIN-2017-134,950,110,9315,923,670\n\nCC-MAIN-2017-174,684,050,8305,645,725\n\nCC-MAIN-2017-224,683,569,2785,514,717\n\nCC-MAIN-2017-264,744,689,1375,514,047\n\nCC-MAIN-2017-511,981,004,3062,529,289\n\nCC-MAIN-2018-134,816,417,9305,520,099\n\nCC-MAIN-2018-223,921,533,2514,401,956\n\nCC-MAIN-2018-264,506,583,9314,916,546\n\nCC-MAIN-2018-304,936,722,4035,282,886\n\nCC-MAIN-2018-343,865,953,9783,808,725\n\nCC-MAIN-2018-473,933,439,8413,637,947\n\nCC-MAIN-2018-514,745,124,4224,616,832\n\nCC-MAIN-2019-044,475,679,1904,140,277\n\nCC-MAIN-2019-094,287,868,8004,142,190\n\nCC-MAIN-2019-133,966,330,3483,849,631\n\nCC-MAIN-2019-304,179,526,1884,430,572\n\nCC-MAIN-2019-355,144,426,2705,048,106\n\nCC-MAIN-2019-394,572,972,4574,527,430\n\nCC-MAIN-2020-295,200,565,5014,984,248\n\nCC-MAIN-2020-344,458,827,9474,297,009\n\nCC-MAIN-2021-171,768,757,3861,824,942\n\nCC-MAIN-2021-394,599,961,6754,287,356\n\nCC-MAIN-2021-435,337,349,3315,304,846\n\nCC-MAIN-2021-493,980,018,7734,050,641\n\nCC-MAIN-2022-054,517,850,0194,503,863\n\nCC-MAIN-2023-065,135,614,2274,959,915\n\nCC-MAIN-2023-145,117,143,7654,675,097\n\nCC-MAIN-2023-235,461,486,8074,869,627\n\nCC-MAIN-2023-505,881,860,0144,901,306\n\nCC-MAIN-2024-105,164,171,5624,335,071\n\nCC-MAIN-2024-184,745,457,0543,949,186\n\nTotal221,715,271,483259,728,610\n\n45\n\nHPeS2o Source Statistics\n\nAdditional statistics on the composition of the peS2o subset of the Common Pile can be found inTable 3 and Table 4.\n\nTable 3: Distribution of licenses in the peS2o subset.\n\nLicenseTrain SplitValidation Split\n\nCC BY6,088,32537,754CC BY-SA120,1501,231CC036,373121Public domain10,0606\n\nTable 4: Distribution of papers across 23 fields of study, as identified by the Semantic ScholarAPI [87]. A paper may belong to one or more fields of study.\n\nField of StudyTrain SplitValidation Split\n\nMedicine2,435,24423,734\n\nBiology1,518,4788,879\n\nEnvironmentalScience993,4997,601\n\nEngineering656,0215,005\n\nComputer Science462,3203,003\n\nMaterials Science416,0453,166\n\nPhysics413,4611,285\n\nChemistry406,4292,781\n\nPsychology364,4412,126\n\nEducation220,0141,532\n\nBusiness193,536946\n\nEconomics185,716921\n\nAgricultural and FoodSciences333,7762,013\n\nSociology137,2571,535\n\nMathematics135,676199\n\nPolitical Science106,748378\n\nGeology67,258217\n\nGeography44,269257\n\nLinguistics41,737228\n\nHistory36,848192\n\nLaw30,888251\n\nPhilosophy27,518148\n\nArt26,65875\n\n46\n\nIGrowth rates of openly licensed data\n\nOver time, the volume of openly licensed data continues to grow as more creators release contentunder open licenses. In Figure 6, we quantify this growth between 2010 and 2024 by analyzingsubsets of the Common Pile for which reliable creation date metadata is available. We plot thecumulative proportion of data created up to various cutoff dates and find that approximately halfof the Common Pile (around 3.8TB) was created since 2020. This trend provides insight into thegrowing availability of openly licensed data and suggests a promising trajectory for future LLMstrained entirely on openly licensed sources.\n\n20102012201420162018202020222024Cutoff Date\n\n0\n\n20\n\n40\n\n60\n\n80\n\n100\n\nPercentage of Total Size\n\nQuantity of Openly Licensed Data Over Time\n\nAllCodeEducational Resources\n\nOtherOnline ForumsWeb\n\nWikisGovernmentAcademic Papers\n\nFigure 6: The amount of openly licensed text grows steadily over time. We visualize the cumulativeproportion of data created up to various cutoff dates for sources in the Common Pile with reliablecreation date metadata. This includes all sources except for the Caselaw Access Project, DataProvenance Initiative, and the sources covering early 20th century Public Domain books.\n\nJDetails on filtering pipelines\n\nIn subsection 4.1, we detail the steps used to produce the Comma v0.1 training dataset from theraw text in the Common Pile. These include applying filters based on language, text quality, length,likelihood, and toxicity; removing various forms of PII; and removal of source-specific boilerplatetext using regular expressions. The Common Pile contains a diverse range of sources and we thereforedesign separate filtering thresholds for each source. The exact source-specific thresholds used topost-process the Common Pile can be found in Table 5. Additionally, statistics on the pre- andpost-filtered sizes of each source can be found in Table 6.\n\nTable 5: Pre-processing pipelines applied to each source in the Common Pile to construct the Commadataset.\n\nSourceLanguage Text QualityDoc LengthLog-LikelihoodToxicityPIIRegex Filter\n\nArXiv Abstracts–––––YN\n\nArXiv Papers> 0.5––––YN\n\nBiodiversityHeritage Library> 0.5–> 100> -20–NY\n\nCaselaw AccessProject––> 100–> 0.1YN\n\nCC CommonCrawl> 0.5> 0.0001> 100–> 0.1YN\n\nContinued on next page\n\n47\n\nSourceLanguage Text QualityDoc LengthLog-LikelihoodToxicityPIIRegex Filter\n\nData ProvenanceInitiative–––––NN\n\nDatabase ofOpen AccessBooks\n\n> 0.5–> 200–> 0.1YN\n\nFoodista> 0.5–> 100––NN\n\nGitHub Archive> 0.5–> 100–> 0.1YN\n\nLibrary ofCongress–––> -20> 0.1NY\n\nLibreTexts> 0.5–> 700–> 0.1YN\n\nNews> 0.5–> 100––YN\n\nOERCommons> 0.5–> 300–> 0.1YN\n\npeS2o–––––YN\n\nPre-1929 Books–––> -20> 0.1NY\n\nPressBooks> 0.5–> 600–> 0.1YN\n\nProjectGutenberg> 0.5––> -20–NN\n\nPublic DomainReview––> 100––YN\n\nPubMed> 0.5–> 100––YN\n\nPEPs–––––YN\n\nRegulations.gov––> 100––YY\n\nStackExchange> 0.5––––YN\n\nUbuntu IRC> 0.5–> 100–> 0.1YN\n\nUK Hansard> 0.5––––YN\n\nUSGPO–––––NY\n\nUSPTO––> 100> -20–YN\n\nWikimedia> 0.5–> 100––YN\n\nWikiteam> 0.5–> 700–> 0.1YN\n\nCC YouTube> 0.5–> 100–> 0.1YN\n\nTable 6: Raw and filtered sizes of the Common Pile’s constituent datasets.\n\nDocument CountSize (GB)\n\nSourceRawFilteredRawFiltered\n\nArXiv Abstracts2,538,9352,504,6792.42.4\n\nArXiv Papers321,336304,0482119\n\nBiodiversity HeritageLibrary42,418,49815,111,3139635\n\nCaselaw Access Project6,919,2406,735,5257877\n\nContinued on next page\n\n48\n\nDocument CountSize (GB)\n\nSourceRawFilteredRawFiltered\n\nCC Common Crawl51,054,4126,852,13726058\n\nData Provenance Initiative9,688,2113,508,51873\n\nDirectory of Open AccessBooks474,445403,99212.512\n\nFoodista72,09065,6400.090.08\n\nGitHub Archive30,318,77423,358,58054.740.4\n\nLibrary of Congress135,500129,05247.835.6\n\nLibreTexts62,26940,0495.33.6\n\nNews172,308126,6730.40.3\n\nOERCommons9,3395,2490.10.05\n\npeS2o6,294,0206,117,280188.2182.6\n\nPre-1929 Books137,127124,89873.846.3\n\nPressBooks106,88154,4551.50.6\n\nProject Gutenberg71,81055,45426.220.1\n\nPublic Domain Review1,4121,4060.0070.007\n\nPubMed4,068,8673,829,689158.9147.1\n\nPEPs6566550.010.01\n\nRegulations.gov225,196208,3016.15.1\n\nStackExchange33,415,40030,987,814103.789.7\n\nStack V2218,364,13369,588,6074774.7259.9\n\nUbuntu IRC329,115234,9826.35.3\n\nUK Hansard51,55247,909109.6\n\nUSGPO2,732,6772,148,54874.536.1\n\nUSPTO20,294,15217,030,2311003.4661.1\n\nWikimedia63,969,93816,311,57490.557.4\n\nWikiteam219,139,36826,931,807437.513.7\n\nCC YouTube1,129,692998,10421.518.6\n\nTotal692,854,953233,817,1697557.91838.3\n\nKDetails on Comma’s pre-training data mixture\n\nWe estimated the quality of each source in the Common Pile by training a 1.7B-parameter model for28B tokens on each source individually and evaluating the resulting models on the set of “early signal”tasks from [132]. In doing so, we found that the amount of text in each source was poorly correlatedwith text quality, motivating the use of heuristic mixing weights to up-/down-weight different sourcesin our pre-training mix. In Table 7 we list the pre-training mixture weights for each of the sources inthe Common Pile.\n\n49\n\nTable 7: Overview of the data mixing used to up/down-weight individual sources in the CommonPile to construct the Comma v0.1-1T pre-training dataset. Comma v0.1-2T simply repeats this fullmixture twice.\n\nSourceSize (GB)RepeatsEffective Size(GB)Tokens(Billions)Percentage\n\nArXivAbstracts2.4614.43.60.360%\n\nArXiv Papers19.5611729.32.932%\n\nBiodiversityHeritageLibrary\n\n35.50.258.92.20.220%\n\nCaselawAccess Project77.5177.519.41.941%\n\nCC CommonCrawl58.16348.687.18.716%\n\nDataProvenanceInitiative\n\n3.4620.45.10.510%\n\nDatabase ofOpen AccessBooks\n\n12672181.801%\n\nFoodista0.0860.480.120.012%\n\nGitHubArchive40.46242.460.66.064%\n\nLibrary ofCongress35.60.258.92.20.220%\n\nLibreTexts3.6621.65.40.540%\n\nNews0.2561.50.380.038%\n\nOERCommons0.0560.30.080.008%\n\npeS2o182.661,095.6273.927.409%\n\nPre-1929Books46.3146.311.61.161%\n\nPressBooks0.663.60.90.090%\n\nProjectGutenberg20.1120.150.500%\n\nPublic DomainReview0.00760.040.010.001%\n\nPubMed147.11147.136.83.683%\n\nPEPs0.0160.060.020.002%\n\nRegulations.gov5.1630.67.60.761%\n\nStackExchange89.76538.2134.613.469%\n\nStack V2259.92519.813013.009%\n\nContinued on next page\n\n50\n\nSourceSize (GB)RepeatsEffective Size(GB)Tokens(Billions)Percentage\n\nUbuntu IRC5.3631.87.90.791%\n\nUK Hansard9.6657.614.41.441%\n\nUSGPO36.10.2592.30.230%\n\nUSPTO661.10.25165.341.34.133%\n\nWikimedia57.46344.486.18.616%\n\nWikiteam13.7454.813.71.371%\n\nCC YouTube18.6118.64.70.470%\n\nTotal1838.3–3997.4999.3100%\n\nLDetails on Comma’s cool-down data mixture\n\nFollowing Hu et al. [73], we end training with a “cool-down” where we train on 37.7B tokens ofhigh-quality data while linearly decaying the learning rate to 0. We provide the source mixtureweights for this cool-down phase in Table 8.\n\nTable 8: Overview of the data mixing used to up/down-weight individual sources in the CommonPile to construct the training distribution for Comma v0.1-1T’s cool-down phase. Comma v0.1-2Tsimply repeats this full mixture twice.\n\nSourceSize (GB)RepeatsEffective Size(GB)Tokens(Billions)Percentage\n\nArXiv Papers19.50.59.82.46.50%\n\nCC CommonCrawl58.10.317.44.411.63%\n\nDataProvenanceInitiative\n\n3.426.81.74.55%\n\nDatabase ofOpen AccessBooks\n\n12224616.04%\n\nFoodista0.0820.160.040.11%\n\nLibreTexts3.627.21.80.48%\n\nNews0.2520.50.130.33%\n\nOERCommons0.0520.10.030.07%\n\npeS2o182.60.118.34.612.18%\n\nPressBooks0.621.20.30.77%\n\nPublic DomainReview0.00720.0140.0040.01%\n\nContinued on next page\n\n51\n\nSourceSize (GB)RepeatsEffective Size(GB)Tokens(Billions)Percentage\n\nPEPs0.0120.020.0050.02%\n\nStackExchange89.70.2522.45.614.96%\n\nStack V2259.90.126.06.517.04%\n\nWikimedia57.40.4235.715.32%\n\nTotal679.4–149.937.5100%\n\nMDetails on small-scale data ablations\n\nIn subsection 4.3 we report results from a series of small-scale data ablations where we identicallytrained 1.7B parameter models on various openly licensed and unlicensed datasets and evaluate theirperformance on the “early signal” tasks from Penedo et al. [132] to compare their data quality againstthe Common Pile. In Figure 7 we show how the performance of these models evolve over the courseof their training run, highlighting that differences in data quality become apparent very early intraining. Additionally, we provide exact numerical results for each model in Table 9, showing thatthe Common Pile has higher data quality than any previously released openly licensed datasets andthe Pile, and nearly matches the data quality of the OSCAR dataset. To validate that this is not purelydue to the presence of high-quality supervised fine-tuning data from the Data Provenance Initiative(DPI) data source, we also perform an ablation on the Common Pile excluding the DPI data and findthat the final performance of this model is largely unchanged.\n\nFigure 7: A model trained on the Comma dataset consistently outperforms models trained onother corpora of openly licensed text and outperforms the Pile on all but two tasks. We trainidentical 1.7B parameter models on 28B tokens from each dataset following Penedo et al. [132].\n\nNAdditional Comma results\n\nWe provide exact numerical results for Comma v0.1-1T and -2T alongside baseline models resultsacross a variety of knowledge, reasoning, and coding tasks in Table 10 and Table 11 respectively. Wefind that particularly on knowledge-based benchmarks (such as MMLU) and coding benchmarks,\n\n52\n\nTable 9: Comma’s training dataset has higher quality than previous openly-licensed datasetsand unlicensed datasets like the Pile. In the small-scale (1.7B parameter) data ablation setting,we find that Comma’s training dataset yields better models than previous openly licensed datasetsand the Pile, and nearly matches the performance of models trained on OSCAR. Additionally, wefind that removing the high-quality supervised data from the Data Provenance Initiative has marginalaffect on the Comma dataset’s overall quality.\n\nDatasetARCMMLUHSOBQACSQAPIQASIQAAvg.\n\nKL3M31.826.329.928.426.858.238.036.2OLC33.127.533.827.427.759.438.537.3Common Corpus34.227.033.630.226.461.037.737.6Comma (no DPI)37.728.737.631.030.863.839.840.0Comma38.029.539.932.429.665.839.440.8\n\nThe Pile37.027.835.828.631.566.838.239.6OSCAR35.427.640.830.432.169.739.740.9FineWeb38.029.148.234.233.673.440.343.7\n\nComma v0.1-1T and -2T outperform baseline models trained on an equivalent amount (1T or 2Ttokens, respectively) of unlicensed text.\n\nTable 10: Comparison between Comma v0.1-1T and baseline models trained with similar resources (7billion parameters, 1 trillion tokens) across a variety of knowledge, reasoning, and coding benchmarks.\n\nModelARC-CARC-EMMLUBoolQHSOBQACSQAPIQASIQAHEvalMBPPAvg.\n\nRPJ-INCITE42.868.427.868.670.349.457.776.046.911.115.948.6LLaMA44.567.934.875.476.251.261.877.250.319.927.953.4StableLM50.865.445.271.775.648.257.277.048.223.132.054.0MPT46.570.530.274.277.648.663.377.349.127.333.254.3OpenLLaMA44.567.240.372.672.650.862.878.049.727.633.954.5Comma v0.1-1T52.868.442.475.762.647.059.470.850.836.535.554.7\n\nQwen357.274.577.086.177.050.866.478.255.094.567.571.3\n\nTable 11: Performance of Comma v0.1-2T and a variety of budget-matched baseline models.\n\nModelARC-CARC-EMMLUBoolQHSOBQACSQAPIQASIQAHEvalMBPPAvg.\n\nOLMo Twin45.267.528.271.773.448.061.877.948.518.227.551.6Llama 248.569.545.880.276.248.462.876.750.826.128.555.8Comma v0.1 2T45.871.849.878.664.446.264.072.552.344.241.557.4DeepSeekLLM49.567.748.571.774.152.066.677.851.643.143.858.8\n\nOAdditional training runs\n\nTo explore the sensitivity of our Comma v0.1 results to hyperparameter choices, we perform a seriesof additional 7B parameter/1T token training runs on AMD MI300A GPUs with slight alterationsto the training recipe. Due to both a desire to reach the same 1T token target rapidly, and the lowersingle-GPU throughput on the system available for these ablations, for all additional runs the thetraining batch size is 8.3M (223) versus the 2.1M (221) tokens per step of Comma v0.1. Unlessotherwise specified, we did not use the two phase training process described in subsection 4.4 (i.e. noseparate high-quality cooldown phase is run and we do not perform checkpoint averaging at the endof training and before evaluation).\n\nO.1Ablations at 1T Tokens\n\nWe first performed a set of training runs for 125,000 steps, resulting in 1.048T total tokens (referredto as “1T” for brevity).\n\n53\n\n“8M Batch”We perform a run with nearly the same training hyperparameters as Comma v0.1-1T,except with a larger 8M token batch size. We also use a single phase training setup; the base datamixture (Table 7) is run for the entire duration to 1T tokens. The learning rate schedule is 2,000 stepsof warm-up from 0 to a peak of 1e −3 with 123,000 steps of decay to a minimum of 1.8e −9.\n\n“Curriculum”In this experimental run, a different data mixture is used in each of three trainingstages of equal duration (we also use the modified hyperparameters from “8M Batch” ablationabove). The first stage of the curriculum comprises data from only the Common Pile’s largest sources(mostly USPTO, Table 13). The second stage uses the same data mixture as Comma v0.1’s mainpre-training phase (“phase I”), but run for only 1/3 of the duration. Finally, the third and last stage ofthe curriculum up-weights Common Pile’s highest quality, benchmark-relevant sources (Table 14).\n\nWe provide exact numerical results for Comma v0.1 and alternate Comma runs performed withdifferent hyperparameters and data mixture curricula across a variety of knowledge, reasoning, andcoding benchmarks in Table 12. We find that the 8M Batch and Curriculum ablations are roughlycomparable on average to the main Comma v0.1-1T run, with the notable exception that both ablationsslightly outperform Comma v0.1-1T on the coding benchmarks. We conclude that the benchmarkresults reported for Comma v0.1-1T in subsection 4.4 seem relatively robust to minor changes intraining hyperparameters, dataset mixture curriculum (assuming similar amounts of most data splitsappear at some time during training), and the software environment and GPU hardware used to trainthe model.\n\nTable 12: Comparison between our main Comma v0.1-1T training run and alternate runs performedwith different hyperparameters and data mixture curricula across a variety of knowledge, reasoning,and coding benchmarks. For “Main”, we report the performance of Comma v0.1-1T without averagingthe cooldown checkpoints so that it is a fair comparison.\n\nModelARC-CARC-EMMLUBoolQHSOBQACSQAPIQASIQAHEvalMBPPAvg.\n\nCurriculum45.269.141.474.760.846.859.170.548.638.134.653.58M Batch47.269.642.969.962.947.056.970.450.536.837.253.8Main50.868.440.272.962.346.259.571.051.232.134.653.6\n\nTable 13: Overview of the data mixing used to up/down-weight individual sources for the Stage 1 ofthe Curriculum ablation run. In this table we omit the size columns for brevity.\n\nSourceRepeatsTokens(Billions)Percentage\n\nUSPTO1.4125233.566.81%\n\nPre-1929Books5.6565.418.71%\n\nStack V2(HTML)11.312.83.65%\n\nUSGPO1.4112.83.65%\n\nLibrary ofCongress1.4112.63.59%\n\nBiodiversityHeritageLibrary\n\n1.4112.523.58%\n\nTotal–349.4100%\n\n54\n\nTable 14: Overview of the data mixing used to up/down-weight individual sources for the Stage 3 ofthe Curriculum ablation run. In this table we omit the size columns for brevity.\n\nSourceRepeatsTokens(Billions)Percentage\n\nStack V2163.818.519%\n\nDatabase ofOpen AccessBooks\n\n6185.230%\n\nWikimedia686.124.981%\n\nStackExchange2.556.116.259%\n\npeS2o145.613.241%\n\nCC CommonCrawl343.612.638%\n\nArXiv Papers524.47.063%\n\nDataProvenanceInitiative\n\n65.11.485%\n\nPressBooks60.870.251%\n\nLibreTexts60.540.157%\n\nNews60.370.108%\n\nFoodista60.120.036%\n\nOERCommons60.080.023%\n\nPEPs60.020.005%\n\nPublic DomainReview60.010.003%\n\nTotal–344.7100%\n\n55",
  "needs_ocr": false
}