{
  "doc_id": "b2e21930-9f82-4d33-b2ac-0b3c899b779e",
  "filename": "human_activity_recognition.pdf",
  "title": "A Public Domain Dataset for Human ActivityRecognition Using Smartphones",
  "metadata": {
    "title": "",
    "author": "",
    "subject": "",
    "producer": "Acrobat Distiller 10.1.3 (Windows)",
    "creationDate": "D:20130303122007+01'00'",
    "modDate": "D:20130801174605+02'00'",
    "num_pages": 6
  },
  "sections": [
    {
      "section_id": "5123f1d8-f725-4531-8895-5f5ffb37df7c",
      "heading": "A Public Domain Dataset for Human ActivityRecognition Using Smartphones",
      "text": "Davide Anguita1, Alessandro Ghio1, Luca Oneto1,Xavier Parra2 and Jorge L. Reyes-Ortiz1,2\n\n1- University of Genova - DITEN.Via Opera Pia 11A, I-16145, Genova, Italy.\n\n2- Universitat Polit`ecnica de Catalunya - CETpDRambla de l’Exposici´o 59-69, 08800, Vilanova i la Geltr´u, Spain.\n\nAbstract.Human-centered computing is an emerging research ﬁeld that aims to understandhuman behavior and integrate users and their social context with computer systems.One of the most recent, challenging and appealing applications in this frameworkconsists in sensing human body motion using smartphones to gather context infor-mation about people actions. In this context, we describe in this work an ActivityRecognition database, built from the recordings of 30 subjects doing Activities ofDaily Living (ADL) while carrying a waist-mounted smartphone with embeddedinertial sensors, which is released to public domain on a well-known on-line repos-itory. Results, obtained on the dataset by exploiting a multiclass Support VectorMachine (SVM), are also acknowledged.",
      "start_page": 1,
      "end_page": 1
    },
    {
      "section_id": "e56fa5c6-9c54-498c-be52-9eabc5f99bc9",
      "heading": "1Introduction",
      "text": "Human Activity Recognition (HAR) aims to identify the actions carried out by a persongiven a set of observations of him/herself and the surrounding environment. Recogni-tion can be accomplished by exploiting the information retrieved from various sourcessuch as environmental [1] or body-worn sensors [2, 3]. Some approaches have adapteddedicated motion sensors in different body parts such as the waist, wrist, chest andthighs achieving good classiﬁcation performance [4]. These sensors are usually un-comfortable for the common user and do not provide a long-term solution for activitymonitoring (e.g. sensor repositioning after dressing [5]).Smartphones are bringing up new research opportunities for human-centered ap-plications where the user is a rich source of context information and the phone is theﬁrsthand sensing tool. Latest devices come with embedded built-in sensors such asmicrophones, dual cameras, accelerometers, gyroscopes, etc. The use of smartphoneswith inertial sensors is an alternative solution for HAR. These mass-marketed devicesprovide a ﬂexible, affordable and self-contained solution to automatically and unobtru-sively monitor Activities of Daily Living (ADL) while also providing telephony ser-vices. Consequently, in the last few years, some works aiming to understand humanbehavior using smartphones have been proposed: for instance in [6], one of the ﬁrstapproaches to exploit an Android smartphone for HAR employing its embedded triax-ial accelerometers; additional results have also been presented in [7, 8]. Improvements",
      "start_page": 1,
      "end_page": 1
    },
    {
      "section_id": "d0d5e1a1-7262-4dfa-9565-521e8f9fedcf",
      "heading": "437",
      "text": "ESANN 2013 proceedings, European Symposium on Artificial Neural Networks, Computational  Intelligence and Machine Learning.  Bruges (Belgium), 24-26 April 2013, i6doc.com publ., ISBN 978-2-87419-081-0. Available from http://www.i6doc.com/en/livre/?GCOI=28001100131010.\n\nNo.StaticTime (sec)No.DynamicTime (sec)0Start (Standing Pos)07Walk (1)151Stand (1)158Walk (2)152Sit (1)159Walk Downstairs (1)123Stand (2)1510Walk Upstairs (2)124Lay Down (1)1511Walk Downstairs (1)125Sit (2)1512Walk Upstairs (2)126Lay Down (2)1513Walk Downstairs (3)1214Walk Upstairs (3)1215Stop0Total192\n\nTable 1: Protocol of activities for the HAR Experiment.\n\nare still expected in topics such as in multi-sensor fusion for better HAR classiﬁca-tion, standardizing performance evaluation metrics [9], and providing public data forevaluation.In the HAR research framework, some datasets have been released to the public do-main: the one of the Opportunity Project [10] is an example which has recorded a set ofADL in a sensor rich environment using 72 environmental and body sensors. Similarly,other works have provided public data, such as [11] and [12]. Publicly available datasetsprovide a freely available source of data across different disciplines and researchers inthe ﬁeld. For this reason, we present a new dataset that has been created using inertialdata from smartphone accelerometers and gyroscopes, targeting the recognition of sixdifferent human activities. Some results, obtained by exploiting a multi class SupportVector Machine (SVM) classiﬁer [13], are shown as well.",
      "start_page": 1,
      "end_page": 2
    },
    {
      "section_id": "6114dcd4-5d8b-44ca-ba4a-d052b2878166",
      "heading": "2Methodology",
      "text": "A set of experiments were carried out to obtain the HAR dataset. A group of 30 vol-unteers with ages ranging from 19 to 48 years were selected for this task. Each personwas instructed to follow a protocol of activities while wearing a waist-mounted Sam-sung Galaxy S II smartphone. The six selected ADL were standing, sitting, layingdown, walking, walking downstairs and upstairs. Each subject performed the protocoltwice: on the ﬁrst trial the smartphone was ﬁxed on the left side of the belt and on thesecond it was placed by the user himself as preferred. There is also a separation of 5seconds between each task where individuals are told to rest, this facilitated repeata-bility (every activity is at least tried twice) and ground trough generation through thevisual interface. The tasks were performed in laboratory conditions but volunteers wereasked to perform freely the sequence of activities for a more naturalistic dataset. Table1 shows experiment protocol details.\n\n2.1Signal Processing\n\nWe collected triaxial linear acceleration and angular velocity signals using the phoneaccelerometer and gyroscope at a sampling rate of 50Hz. These signals were pre-processed for noise reduction with a median ﬁlter and a 3rd order low-pass Butter-",
      "start_page": 2,
      "end_page": 2
    },
    {
      "section_id": "80172472-d5d7-4442-a941-a6dd3bd8b335",
      "heading": "438",
      "text": "ESANN 2013 proceedings, European Symposium on Artificial Neural Networks, Computational  Intelligence and Machine Learning.  Bruges (Belgium), 24-26 April 2013, i6doc.com publ., ISBN 978-2-87419-081-0. Available from http://www.i6doc.com/en/livre/?GCOI=28001100131010.\n\nNameTimeFreq.Body Acc11Gravity Acc10Body Acc Jerk11Body Angular Speed11Body Angular Acc10Body Acc Magnitude11Gravity Acc Mag10Body Acc Jerk Mag11Body Angular Speed Mag11Body Angular Acc Mag11\n\nTable 2: Time and frequency domain signals obtained from the smartphone sensors.\n\nworth ﬁlter with a 20 Hz cutoff frequency. This rate is sufﬁcient for capturing humanbody motion since 99% of its energy is contained below 15Hz [3]. The accelerationsignal, which has gravitational and body motion components, was separated using an-other Butterworth low-pass ﬁlter into body acceleration and gravity. The gravitationalforce is assumed to have only low frequency components, therefore we found from theexperiments that 0.3 Hz was an optimal corner frequency for a constant gravity signal.Additional time signals were obtained by calculating from the triaxial signals theeuclidean magnitude and time derivatives (jerk da/dt and angular acceleration dw/dt).The time signals were then sampled in ﬁxed-width sliding windows of 2.56 sec and50% overlap between them, since:\n\n• The cadence of an average person walking is within [90, 130] steps/min [14], i.e.a minimum of 1.5 steps/sec;\n\n• At least a full walking cycle (two steps) is preferred on each window sample;\n\n• People with slower cadence such as elderly and disabled should also beneﬁt fromthis method. We supposed a minimum speed equal to 50% of average humancadence;\n\n• Signals are also mapped in the frequency domain through a Fast Fourier Trans-form (FFT), optimized for power of two vectors (2.56sec × 50Hz = 128cycles).\n\nThus, a total of 17 signals were obtained with this method, which are listed in Table 2.\n\n2.2Feature Mapping\n\nFrom each sampled window described above a vector of features was obtained. Stan-dard measures previously used in HAR literature [15] such as the mean, correlation,signal magnitude area (SMA) and autoregression coefﬁcients [16] were employed forthe feature mapping. A new set of features was also employed in order to improve thelearning performance, including energy of different frequency bands, frequency skew-ness, and angle between vectors (e.g. mean body acceleration and y vector). Table 3contains the list of all the measures applied to the time and frequency domain signals.A total of 561 features were extracted to describe each activity window. In order toease the performance assessment, the dataset has been also randomly partitioned into",
      "start_page": 2,
      "end_page": 3
    },
    {
      "section_id": "79626ba3-c84f-43a0-b482-0ff7fc00335c",
      "heading": "439",
      "text": "ESANN 2013 proceedings, European Symposium on Artificial Neural Networks, Computational  Intelligence and Machine Learning.  Bruges (Belgium), 24-26 April 2013, i6doc.com publ., ISBN 978-2-87419-081-0. Available from http://www.i6doc.com/en/livre/?GCOI=28001100131010.\n\nFunctionDescriptionmeanMean valuestdStandard deviationmadMedian absolute valuemaxLargest values in arrayminSmallest value in arraysmaSignal magnitude areaenergyAverage sum of the squaresiqrInterquartile rangeentropySignal EntropyarCoeffAutorregresion coefﬁcientscorrelationCorrelation coefﬁcientmaxFreqIndLargest frequency componentmeanFreqFrequency signal weighted averageskewnessFrequency signal SkewnesskurtosisFrequency signal KurtosisenergyBandEnergy of a frequency intervalangleAngle between two vectors\n\nTable 3: List of measures for computing feature vectors.\n\ntwo independent sets, where 70% of the data were selected for training and the remain-ing 30% for testing. The Human Activity Recognition dataset has been made availablefor public use and it is presented as raw inertial sensors signals and also as feature vec-tors for each pattern. It has been submitted as the Human Activity Recognition usingSmartphones dataset in the UCI Machine Learning Repository [17] and can be accessedfollowing this link (information concerning the licensing and usage of the data can beretrieved in the readme ﬁle included):\n\narchive.ics.uci.edu/ml/datasets/Human+Activity+Recognition+Using+Smartphones",
      "start_page": 3,
      "end_page": 4
    },
    {
      "section_id": "251cd7e8-af30-4b7e-8018-bd95496a1a78",
      "heading": "3Experimental results",
      "text": "We conducted some experiments on the HAR dataset to acknowledge future users withsome results. For this purpose, we exploit well-known and state-of-the-art SupportVector Machine (SVM) [13] binary classiﬁers, which are generalized to the multiclasscase through a One-Vs-All (OVA) approach: the SVM hyperparameters are selectedthrough a 10-fold Cross Validation procedure and Gaussian kernels are used for ourexperiments.The classiﬁcation results using the multiclass SVM (MC-SVM) for the 6 ADL arepresented in Table 4. They show an overall accuracy of 96% for the test data composedof 2947 patterns. Similar work on HAR using special purpose sensors have showncomparable performance (90%-96%), such as in [3] where a system developed by col-lecting data from 6 volunteers for the classifaciton of 12 ADL using a waist-mountedtriaxial accelerometer provided an accuracy of 90.8%, and similarly in [18] where achest-mounted accelerometer was used for classifying 5 ADL obtained a recognitionperformance of 93.9%. This allows to argue that the use of smartphones, in addition tobe more unobtrusive and less invasive than other special purpose solutions (e.g. wear-able sensors), is a feasible way to walk for effectively performing HAR. It is also worth",
      "start_page": 4,
      "end_page": 4
    },
    {
      "section_id": "3a3afa0e-d012-498c-9c6b-6313f7f5a804",
      "heading": "440",
      "text": "ESANN 2013 proceedings, European Symposium on Artificial Neural Networks, Computational  Intelligence and Machine Learning.  Bruges (Belgium), 24-26 April 2013, i6doc.com publ., ISBN 978-2-87419-081-0. Available from http://www.i6doc.com/en/livre/?GCOI=28001100131010.\n\nWKWUWDSTSDLDRecallWalking4921300099%W. Upstairs18451200096%W. Downstairs4641000098%Sitting02043257088%Standing00014518097%Laying Down00000537100%Precision96%98%99%97%90%100%96%\n\nTable 4: Confusion Matrix of the classiﬁcation results on the test data using the multi-class SVM. Rows represent the actual class and columns the predicted class. Activitynames on top are abbreviated.\n\nunderlining that the MC-SVM model outperforms by 7% the classiﬁer learned on ourprevious dataset described in [19], where only acceleration data from the smartphonewere taken into account for the recognition: this suggests that the new features, in-troduced in the publicly available dataset as depicted in Section 2.2, allow to ease thelearning process.The classiﬁcation performance for each class is also shown in terms of recall andprecision measures, with the sitting activity having lowest recall equal to 88%. In par-ticular, there is a noticeable misclassiﬁcation overlap between this activity and standingattributed to the physical location of the device and its difﬁculty to categorize them: fu-ture works will have to investigate the necessary steps in order to improve the discrim-ination of these non-dynamic activities (e.g. introduction of new features, for examplederived by gyroscopes).",
      "start_page": 4,
      "end_page": 5
    },
    {
      "section_id": "e83272e5-2ede-4dcb-b430-037f3aef6b89",
      "heading": "4Conclusions",
      "text": "In this paper we introduced a new publicly available dataset for HAR using smartphonesand acknowledged some results using a multiclass Support Vector Machine approach.The multiclass SVM employed for the classiﬁcation of smartphone inertial data showeda recognition performance similar to previous work that have used special purpose sen-sors, therefore strengthening the application of these devices for HAR purposes. Wealso highlighted an improvement on the classiﬁcation performance of the learned modelusing this new dataset against the previous version, which had a reduced set of features.However, rooms for improvements exist: while dynamic activities can be efﬁcientlyclassiﬁed thanks to the newly introduced features in the released dataset, non-dynamicactions still present misclassiﬁcation overlaps. This requires further study of availableinputs and revision of the HAR process pipeline phases. Finally, computational com-plexity aspects such as battery life and real time processing for the application will beassessed in our forthcoming works.\n\nAcknowledgments. This work was supported in part by the Erasmus Mundus JointDoctorate in Interactive and Cognitive Environments, which is funded by the EACEAAgency of the European Commission under EMJD ICE FPA n 2010-0012.",
      "start_page": 5,
      "end_page": 5
    },
    {
      "section_id": "02f5ce75-6042-419a-925e-909829995411",
      "heading": "441",
      "text": "ESANN 2013 proceedings, European Symposium on Artificial Neural Networks, Computational  Intelligence and Machine Learning.  Bruges (Belgium), 24-26 April 2013, i6doc.com publ., ISBN 978-2-87419-081-0. Available from http://www.i6doc.com/en/livre/?GCOI=28001100131010.",
      "start_page": 5,
      "end_page": 5
    },
    {
      "section_id": "e0732a3a-9b3f-4a62-b67d-886aceebcfbb",
      "heading": "References",
      "text": "[1] R. Poppe. Vision-based human motion analysis: An overview. Computer Vision and Image Under-standing, 108(1-2):4–18, 2007.\n\n[2] P. Lukowicz, J.A. Ward, H. Junker, M. St¨ager, G. Tr¨oster, A. Atrash, and T. Starner. Recognizing work-shop activity using body worn microphones and accelerometers. Proceedings of the 2nd Int ConferencePervasive Computing, pages 18–22, 2004.\n\n[3] D.M. Karantonis, M.R. Narayanan, M. Mathie, N.H. Lovell, and B.G. Celler. Implementation of areal-time human movement classiﬁer using a triaxial accelerometer for ambulatory monitoring. IEEETransactions on Information Technology in Biomedicine, 10(1):156–167, 2006.\n\n[4] R. Nishkam, D. Nikhil, M. Preetham, and M.L. Littman. Activity recognition from accelerometerdata. In Proceedings of the Seventeenth Conference on Innovative Applications of Artiﬁcial Intelligence,pages 1541–1546, 2005.\n\n[5] L. Bao and S.S. Intille. Activity recognition from user-annotated acceleration data. In T. Kanade,J. Kittler, J.M. Kleinberg, F. Mattern, J.C. Mitchell, O. Nierstrasz, C. Pandu Rangan, B. Steffen, D. Ter-zopoulos, D. Tygar, M.Y. Vardi, and A. Ferscha, editors, Pervasive Computing, pages 1–17. 2004.\n\n[6] J.R. Kwapisz, G.M. Weiss, and S.A. Moore. Activity recognition using cell phone accelerometers.SIGKDD Explorations Newsletter, 12(2):74–82, 2011.\n\n[7] T. Brezmes, J.L. Gorricho, and J. Cotrina. Activity recognition from accelerometer data on a mobilephone. Distributed Computing, Artiﬁcial Intelligence, Bioinformatics, Soft Computing, and AmbientAssisted Living, pages 796–799, 2009.\n\n[8] W. Wu, S. Dasgupta, E.E. Ramirez, C. Peterson, and G.J. Norman. Classiﬁcation accuracies of physicalactivities using smartphone motion sensors. Journal of Medical Internet Research, 14(5), 2012.\n\n[9] M.F.A. bin Abdullah, A.F.P. Negara, M.S. Sayeed, D.J. Choi, and K.S. Muthu. Classiﬁcation algorithmsin human activity recognition using smartphones. International Journal of Computer and InformationEngineering, 6:77–84, 2012.\n\n[10] D. Roggen, A. Calatroni, M. Rossi, T. Holleczek, K. F¨orster, G. Tr¨oster, P. Lukowicz, D. Bannach,G. Pirkl, and A. Ferscha. Collecting complex activity data sets in highly rich networked sensor envi-ronments. In Proceedings of the 7th International Conference on Networked Sensing Systems 2010,2010.\n\n[11] E.M. Tapia, S.S. Intille, L. Lopez, and K. Larson. The design of a portable kit of wireless sensors fornaturalistic data collection. In Proceedings of PERVASIVE 2006, pages 117–134, 2006.\n\n[12] S. Dernbach, B. Das, N.C. Krishnan, B.L. Thomas, and D.J. Cook. Simple and complex activity recog-nition through smart phones. In 2012 8th International Conference on Intelligent Environments, pages214–221, 2012.\n\n[13] C. Cortes and V. Vapnik. Support-vector networks. Machine learning, 20(3):273–297, 1995.\n\n[14] C. BenAbdelkader, R. Cutler, and L. Davis. Stride and cadence as a biometric in automatic personidentiﬁcation and veriﬁcation. In Proceedings of the Fifth IEEE International Conference on AutomaticFace and Gesture Recognition, pages 372–377, 2002.\n\n[15] J.Y. Yang, J.S. Wang, and Y.P. Chen. Using acceleration measurements for activity recognition: Aneffective learning algorithm for constructing neural classiﬁers. Pattern recognition letters, 29(16):2213–2220, 2008.\n\n[16] A.M. Khan, Y.-K. Lee, S.Y. Lee, and T.-S. Kim. Human activity recognition via an accelerometer-enabled-smartphone using kernel discriminant analysis. In Proceedings of the 5th International Con-ference on Future Information Technology, pages 1–6, 2010.\n\n[17] A. Frank and A. Asuncion. UCI machine learning repository, 2010.\n\n[18] Y. Hanai, J. Nishimura, and T. Kuroda. Haar-like ﬁltering for human activity recognition using 3daccelerometer.In Digital Signal Processing Workshop and 5th IEEE Signal Processing EducationWorkshop, 2009. DSP/SPE 2009. IEEE 13th, pages 675 –678, jan. 2009.\n\n[19] D. Anguita, A. Ghio, L. Oneto, X. Parra, and J.L. Reyes-Ortiz. Human activity recognition on smart-phones using a multiclass hardware-friendly support vector machine. In Proceedings of the Interna-tional Workshop of Ambient Assited Living, 2012.",
      "start_page": 6,
      "end_page": 6
    },
    {
      "section_id": "bfd8a05e-0df7-468a-a321-c5d729b34cf4",
      "heading": "442",
      "text": "ESANN 2013 proceedings, European Symposium on Artificial Neural Networks, Computational  Intelligence and Machine Learning.  Bruges (Belgium), 24-26 April 2013, i6doc.com publ., ISBN 978-2-87419-081-0. Available from http://www.i6doc.com/en/livre/?GCOI=28001100131010.",
      "start_page": 6,
      "end_page": 6
    }
  ],
  "tables": [
    {
      "table_id": "e8e5cf99-ab62-4dc8-99c0-02799f0ea827",
      "page": 2,
      "csv_path": "data\\processed\\tables\\human_activity_recognition_p2_table1.csv",
      "rows": 3,
      "cols": 6,
      "raw": [
        [
          "No.",
          "Static",
          "Time(sec)",
          "No.",
          "Dynamic",
          "Time(sec)"
        ],
        [
          "0\n1\n2\n3\n4\n5\n6",
          "Start(StandingPos)\nStand(1)\nSit(1)\nStand(2)\nLayDown(1)\nSit(2)\nLayDown(2)",
          "0\n15\n15\n15\n15\n15\n15",
          "7\n8\n9\n10\n11\n12\n13\n14\n15",
          "Walk(1)\nWalk(2)\nWalkDownstairs(1)\nWalkUpstairs(2)\nWalkDownstairs(1)\nWalkUpstairs(2)\nWalkDownstairs(3)\nWalkUpstairs(3)\nStop",
          "15\n15\n12\n12\n12\n12\n12\n12\n0"
        ],
        [
          "",
          null,
          null,
          null,
          "Total",
          "192"
        ]
      ]
    },
    {
      "table_id": "18b940a6-71fd-433f-91fe-7f3b426a0cc4",
      "page": 3,
      "csv_path": "data\\processed\\tables\\human_activity_recognition_p3_table1.csv",
      "rows": 2,
      "cols": 3,
      "raw": [
        [
          "Name",
          "Time",
          "Freq."
        ],
        [
          "BodyAcc\nGravityAcc\nBodyAccJerk\nBodyAngularSpeed\nBodyAngularAcc\nBodyAccMagnitude\nGravityAccMag\nBodyAccJerkMag\nBodyAngularSpeedMag\nBodyAngularAccMag",
          "1\n1\n1\n1\n1\n1\n1\n1\n1\n1",
          "1\n0\n1\n1\n0\n1\n0\n1\n1\n1"
        ]
      ]
    },
    {
      "table_id": "1bc41624-5e42-4cb5-aaa5-294760e6cb53",
      "page": 4,
      "csv_path": "data\\processed\\tables\\human_activity_recognition_p4_table1.csv",
      "rows": 2,
      "cols": 2,
      "raw": [
        [
          "Function",
          "Description"
        ],
        [
          "mean\nstd\nmad\nmax\nmin\nsma\nenergy\niqr\nentropy\narCoeff\ncorrelation\nmaxFreqInd\nmeanFreq\nskewness\nkurtosis\nenergyBand\nangle",
          "Meanvalue\nStandarddeviation\nMedianabsolutevalue\nLargestvaluesinarray\nSmallestvalueinarray\nSignalmagnitudearea\nAveragesumofthesquares\nInterquartilerange\nSignalEntropy\nAutorregresioncoefficients\nCorrelationcoefficient\nLargestfrequencycomponent\nFrequencysignalweightedaverage\nFrequencysignalSkewness\nFrequencysignalKurtosis\nEnergyofafrequencyinterval\nAnglebetweentwovectors"
        ]
      ]
    },
    {
      "table_id": "ef87c3eb-06f9-4a22-b8a1-59f98edbfb01",
      "page": 5,
      "csv_path": "data\\processed\\tables\\human_activity_recognition_p5_table1.csv",
      "rows": 3,
      "cols": 3,
      "raw": [
        [
          "",
          "WK WU WD ST SD LD",
          "Recall"
        ],
        [
          "Walking\nW.Upstairs\nW.Downstairs\nSitting\nStanding\nLayingDown",
          "492 1 3 0 0 0\n18 451 2 0 0 0\n4 6 410 0 0 0\n0 2 0 432 57 0\n0 0 0 14 518 0\n0 0 0 0 0 537",
          "99%\n96%\n98%\n88%\n97%\n100%"
        ],
        [
          "Precision",
          "96% 98% 99% 97% 90% 100%",
          "96%"
        ]
      ]
    }
  ],
  "figures": [],
  "references": "[1] R. Poppe. Vision-based human motion analysis: An overview. Computer Vision and Image Under-standing, 108(1-2):4–18, 2007.\n\n[2] P. Lukowicz, J.A. Ward, H. Junker, M. St¨ager, G. Tr¨oster, A. Atrash, and T. Starner. Recognizing work-shop activity using body worn microphones and accelerometers. Proceedings of the 2nd Int ConferencePervasive Computing, pages 18–22, 2004.\n\n[3] D.M. Karantonis, M.R. Narayanan, M. Mathie, N.H. Lovell, and B.G. Celler. Implementation of areal-time human movement classiﬁer using a triaxial accelerometer for ambulatory monitoring. IEEETransactions on Information Technology in Biomedicine, 10(1):156–167, 2006.\n\n[4] R. Nishkam, D. Nikhil, M. Preetham, and M.L. Littman. Activity recognition from accelerometerdata. In Proceedings of the Seventeenth Conference on Innovative Applications of Artiﬁcial Intelligence,pages 1541–1546, 2005.\n\n[5] L. Bao and S.S. Intille. Activity recognition from user-annotated acceleration data. In T. Kanade,J. Kittler, J.M. Kleinberg, F. Mattern, J.C. Mitchell, O. Nierstrasz, C. Pandu Rangan, B. Steffen, D. Ter-zopoulos, D. Tygar, M.Y. Vardi, and A. Ferscha, editors, Pervasive Computing, pages 1–17. 2004.\n\n[6] J.R. Kwapisz, G.M. Weiss, and S.A. Moore. Activity recognition using cell phone accelerometers.SIGKDD Explorations Newsletter, 12(2):74–82, 2011.\n\n[7] T. Brezmes, J.L. Gorricho, and J. Cotrina. Activity recognition from accelerometer data on a mobilephone. Distributed Computing, Artiﬁcial Intelligence, Bioinformatics, Soft Computing, and AmbientAssisted Living, pages 796–799, 2009.\n\n[8] W. Wu, S. Dasgupta, E.E. Ramirez, C. Peterson, and G.J. Norman. Classiﬁcation accuracies of physicalactivities using smartphone motion sensors. Journal of Medical Internet Research, 14(5), 2012.\n\n[9] M.F.A. bin Abdullah, A.F.P. Negara, M.S. Sayeed, D.J. Choi, and K.S. Muthu. Classiﬁcation algorithmsin human activity recognition using smartphones. International Journal of Computer and InformationEngineering, 6:77–84, 2012.\n\n[10] D. Roggen, A. Calatroni, M. Rossi, T. Holleczek, K. F¨orster, G. Tr¨oster, P. Lukowicz, D. Bannach,G. Pirkl, and A. Ferscha. Collecting complex activity data sets in highly rich networked sensor envi-ronments. In Proceedings of the 7th International Conference on Networked Sensing Systems 2010,2010.\n\n[11] E.M. Tapia, S.S. Intille, L. Lopez, and K. Larson. The design of a portable kit of wireless sensors fornaturalistic data collection. In Proceedings of PERVASIVE 2006, pages 117–134, 2006.\n\n[12] S. Dernbach, B. Das, N.C. Krishnan, B.L. Thomas, and D.J. Cook. Simple and complex activity recog-nition through smart phones. In 2012 8th International Conference on Intelligent Environments, pages214–221, 2012.\n\n[13] C. Cortes and V. Vapnik. Support-vector networks. Machine learning, 20(3):273–297, 1995.\n\n[14] C. BenAbdelkader, R. Cutler, and L. Davis. Stride and cadence as a biometric in automatic personidentiﬁcation and veriﬁcation. In Proceedings of the Fifth IEEE International Conference on AutomaticFace and Gesture Recognition, pages 372–377, 2002.\n\n[15] J.Y. Yang, J.S. Wang, and Y.P. Chen. Using acceleration measurements for activity recognition: Aneffective learning algorithm for constructing neural classiﬁers. Pattern recognition letters, 29(16):2213–2220, 2008.\n\n[16] A.M. Khan, Y.-K. Lee, S.Y. Lee, and T.-S. Kim. Human activity recognition via an accelerometer-enabled-smartphone using kernel discriminant analysis. In Proceedings of the 5th International Con-ference on Future Information Technology, pages 1–6, 2010.\n\n[17] A. Frank and A. Asuncion. UCI machine learning repository, 2010.\n\n[18] Y. Hanai, J. Nishimura, and T. Kuroda. Haar-like ﬁltering for human activity recognition using 3daccelerometer.In Digital Signal Processing Workshop and 5th IEEE Signal Processing EducationWorkshop, 2009. DSP/SPE 2009. IEEE 13th, pages 675 –678, jan. 2009.\n\n[19] D. Anguita, A. Ghio, L. Oneto, X. Parra, and J.L. Reyes-Ortiz. Human activity recognition on smart-phones using a multiclass hardware-friendly support vector machine. In Proceedings of the Interna-tional Workshop of Ambient Assited Living, 2012.\n\n442\n\nESANN 2013 proceedings, European Symposium on Artificial Neural Networks, Computational  Intelligence and Machine Learning.  Bruges (Belgium), 24-26 April 2013, i6doc.com publ., ISBN 978-2-87419-081-0. Available from http://www.i6doc.com/en/livre/?GCOI=28001100131010.",
  "raw_text": "A Public Domain Dataset for Human ActivityRecognition Using Smartphones\n\nDavide Anguita1, Alessandro Ghio1, Luca Oneto1,Xavier Parra2 and Jorge L. Reyes-Ortiz1,2\n\n1- University of Genova - DITEN.Via Opera Pia 11A, I-16145, Genova, Italy.\n\n2- Universitat Polit`ecnica de Catalunya - CETpDRambla de l’Exposici´o 59-69, 08800, Vilanova i la Geltr´u, Spain.\n\nAbstract.Human-centered computing is an emerging research ﬁeld that aims to understandhuman behavior and integrate users and their social context with computer systems.One of the most recent, challenging and appealing applications in this frameworkconsists in sensing human body motion using smartphones to gather context infor-mation about people actions. In this context, we describe in this work an ActivityRecognition database, built from the recordings of 30 subjects doing Activities ofDaily Living (ADL) while carrying a waist-mounted smartphone with embeddedinertial sensors, which is released to public domain on a well-known on-line repos-itory. Results, obtained on the dataset by exploiting a multiclass Support VectorMachine (SVM), are also acknowledged.\n\n1Introduction\n\nHuman Activity Recognition (HAR) aims to identify the actions carried out by a persongiven a set of observations of him/herself and the surrounding environment. Recogni-tion can be accomplished by exploiting the information retrieved from various sourcessuch as environmental [1] or body-worn sensors [2, 3]. Some approaches have adapteddedicated motion sensors in different body parts such as the waist, wrist, chest andthighs achieving good classiﬁcation performance [4]. These sensors are usually un-comfortable for the common user and do not provide a long-term solution for activitymonitoring (e.g. sensor repositioning after dressing [5]).Smartphones are bringing up new research opportunities for human-centered ap-plications where the user is a rich source of context information and the phone is theﬁrsthand sensing tool. Latest devices come with embedded built-in sensors such asmicrophones, dual cameras, accelerometers, gyroscopes, etc. The use of smartphoneswith inertial sensors is an alternative solution for HAR. These mass-marketed devicesprovide a ﬂexible, affordable and self-contained solution to automatically and unobtru-sively monitor Activities of Daily Living (ADL) while also providing telephony ser-vices. Consequently, in the last few years, some works aiming to understand humanbehavior using smartphones have been proposed: for instance in [6], one of the ﬁrstapproaches to exploit an Android smartphone for HAR employing its embedded triax-ial accelerometers; additional results have also been presented in [7, 8]. Improvements\n\n437\n\nESANN 2013 proceedings, European Symposium on Artificial Neural Networks, Computational  Intelligence and Machine Learning.  Bruges (Belgium), 24-26 April 2013, i6doc.com publ., ISBN 978-2-87419-081-0. Available from http://www.i6doc.com/en/livre/?GCOI=28001100131010.\n\nNo.StaticTime (sec)No.DynamicTime (sec)0Start (Standing Pos)07Walk (1)151Stand (1)158Walk (2)152Sit (1)159Walk Downstairs (1)123Stand (2)1510Walk Upstairs (2)124Lay Down (1)1511Walk Downstairs (1)125Sit (2)1512Walk Upstairs (2)126Lay Down (2)1513Walk Downstairs (3)1214Walk Upstairs (3)1215Stop0Total192\n\nTable 1: Protocol of activities for the HAR Experiment.\n\nare still expected in topics such as in multi-sensor fusion for better HAR classiﬁca-tion, standardizing performance evaluation metrics [9], and providing public data forevaluation.In the HAR research framework, some datasets have been released to the public do-main: the one of the Opportunity Project [10] is an example which has recorded a set ofADL in a sensor rich environment using 72 environmental and body sensors. Similarly,other works have provided public data, such as [11] and [12]. Publicly available datasetsprovide a freely available source of data across different disciplines and researchers inthe ﬁeld. For this reason, we present a new dataset that has been created using inertialdata from smartphone accelerometers and gyroscopes, targeting the recognition of sixdifferent human activities. Some results, obtained by exploiting a multi class SupportVector Machine (SVM) classiﬁer [13], are shown as well.\n\n2Methodology\n\nA set of experiments were carried out to obtain the HAR dataset. A group of 30 vol-unteers with ages ranging from 19 to 48 years were selected for this task. Each personwas instructed to follow a protocol of activities while wearing a waist-mounted Sam-sung Galaxy S II smartphone. The six selected ADL were standing, sitting, layingdown, walking, walking downstairs and upstairs. Each subject performed the protocoltwice: on the ﬁrst trial the smartphone was ﬁxed on the left side of the belt and on thesecond it was placed by the user himself as preferred. There is also a separation of 5seconds between each task where individuals are told to rest, this facilitated repeata-bility (every activity is at least tried twice) and ground trough generation through thevisual interface. The tasks were performed in laboratory conditions but volunteers wereasked to perform freely the sequence of activities for a more naturalistic dataset. Table1 shows experiment protocol details.\n\n2.1Signal Processing\n\nWe collected triaxial linear acceleration and angular velocity signals using the phoneaccelerometer and gyroscope at a sampling rate of 50Hz. These signals were pre-processed for noise reduction with a median ﬁlter and a 3rd order low-pass Butter-\n\n438\n\nESANN 2013 proceedings, European Symposium on Artificial Neural Networks, Computational  Intelligence and Machine Learning.  Bruges (Belgium), 24-26 April 2013, i6doc.com publ., ISBN 978-2-87419-081-0. Available from http://www.i6doc.com/en/livre/?GCOI=28001100131010.\n\nNameTimeFreq.Body Acc11Gravity Acc10Body Acc Jerk11Body Angular Speed11Body Angular Acc10Body Acc Magnitude11Gravity Acc Mag10Body Acc Jerk Mag11Body Angular Speed Mag11Body Angular Acc Mag11\n\nTable 2: Time and frequency domain signals obtained from the smartphone sensors.\n\nworth ﬁlter with a 20 Hz cutoff frequency. This rate is sufﬁcient for capturing humanbody motion since 99% of its energy is contained below 15Hz [3]. The accelerationsignal, which has gravitational and body motion components, was separated using an-other Butterworth low-pass ﬁlter into body acceleration and gravity. The gravitationalforce is assumed to have only low frequency components, therefore we found from theexperiments that 0.3 Hz was an optimal corner frequency for a constant gravity signal.Additional time signals were obtained by calculating from the triaxial signals theeuclidean magnitude and time derivatives (jerk da/dt and angular acceleration dw/dt).The time signals were then sampled in ﬁxed-width sliding windows of 2.56 sec and50% overlap between them, since:\n\n• The cadence of an average person walking is within [90, 130] steps/min [14], i.e.a minimum of 1.5 steps/sec;\n\n• At least a full walking cycle (two steps) is preferred on each window sample;\n\n• People with slower cadence such as elderly and disabled should also beneﬁt fromthis method. We supposed a minimum speed equal to 50% of average humancadence;\n\n• Signals are also mapped in the frequency domain through a Fast Fourier Trans-form (FFT), optimized for power of two vectors (2.56sec × 50Hz = 128cycles).\n\nThus, a total of 17 signals were obtained with this method, which are listed in Table 2.\n\n2.2Feature Mapping\n\nFrom each sampled window described above a vector of features was obtained. Stan-dard measures previously used in HAR literature [15] such as the mean, correlation,signal magnitude area (SMA) and autoregression coefﬁcients [16] were employed forthe feature mapping. A new set of features was also employed in order to improve thelearning performance, including energy of different frequency bands, frequency skew-ness, and angle between vectors (e.g. mean body acceleration and y vector). Table 3contains the list of all the measures applied to the time and frequency domain signals.A total of 561 features were extracted to describe each activity window. In order toease the performance assessment, the dataset has been also randomly partitioned into\n\n439\n\nESANN 2013 proceedings, European Symposium on Artificial Neural Networks, Computational  Intelligence and Machine Learning.  Bruges (Belgium), 24-26 April 2013, i6doc.com publ., ISBN 978-2-87419-081-0. Available from http://www.i6doc.com/en/livre/?GCOI=28001100131010.\n\nFunctionDescriptionmeanMean valuestdStandard deviationmadMedian absolute valuemaxLargest values in arrayminSmallest value in arraysmaSignal magnitude areaenergyAverage sum of the squaresiqrInterquartile rangeentropySignal EntropyarCoeffAutorregresion coefﬁcientscorrelationCorrelation coefﬁcientmaxFreqIndLargest frequency componentmeanFreqFrequency signal weighted averageskewnessFrequency signal SkewnesskurtosisFrequency signal KurtosisenergyBandEnergy of a frequency intervalangleAngle between two vectors\n\nTable 3: List of measures for computing feature vectors.\n\ntwo independent sets, where 70% of the data were selected for training and the remain-ing 30% for testing. The Human Activity Recognition dataset has been made availablefor public use and it is presented as raw inertial sensors signals and also as feature vec-tors for each pattern. It has been submitted as the Human Activity Recognition usingSmartphones dataset in the UCI Machine Learning Repository [17] and can be accessedfollowing this link (information concerning the licensing and usage of the data can beretrieved in the readme ﬁle included):\n\narchive.ics.uci.edu/ml/datasets/Human+Activity+Recognition+Using+Smartphones\n\n3Experimental results\n\nWe conducted some experiments on the HAR dataset to acknowledge future users withsome results. For this purpose, we exploit well-known and state-of-the-art SupportVector Machine (SVM) [13] binary classiﬁers, which are generalized to the multiclasscase through a One-Vs-All (OVA) approach: the SVM hyperparameters are selectedthrough a 10-fold Cross Validation procedure and Gaussian kernels are used for ourexperiments.The classiﬁcation results using the multiclass SVM (MC-SVM) for the 6 ADL arepresented in Table 4. They show an overall accuracy of 96% for the test data composedof 2947 patterns. Similar work on HAR using special purpose sensors have showncomparable performance (90%-96%), such as in [3] where a system developed by col-lecting data from 6 volunteers for the classifaciton of 12 ADL using a waist-mountedtriaxial accelerometer provided an accuracy of 90.8%, and similarly in [18] where achest-mounted accelerometer was used for classifying 5 ADL obtained a recognitionperformance of 93.9%. This allows to argue that the use of smartphones, in addition tobe more unobtrusive and less invasive than other special purpose solutions (e.g. wear-able sensors), is a feasible way to walk for effectively performing HAR. It is also worth\n\n440\n\nESANN 2013 proceedings, European Symposium on Artificial Neural Networks, Computational  Intelligence and Machine Learning.  Bruges (Belgium), 24-26 April 2013, i6doc.com publ., ISBN 978-2-87419-081-0. Available from http://www.i6doc.com/en/livre/?GCOI=28001100131010.\n\nWKWUWDSTSDLDRecallWalking4921300099%W. Upstairs18451200096%W. Downstairs4641000098%Sitting02043257088%Standing00014518097%Laying Down00000537100%Precision96%98%99%97%90%100%96%\n\nTable 4: Confusion Matrix of the classiﬁcation results on the test data using the multi-class SVM. Rows represent the actual class and columns the predicted class. Activitynames on top are abbreviated.\n\nunderlining that the MC-SVM model outperforms by 7% the classiﬁer learned on ourprevious dataset described in [19], where only acceleration data from the smartphonewere taken into account for the recognition: this suggests that the new features, in-troduced in the publicly available dataset as depicted in Section 2.2, allow to ease thelearning process.The classiﬁcation performance for each class is also shown in terms of recall andprecision measures, with the sitting activity having lowest recall equal to 88%. In par-ticular, there is a noticeable misclassiﬁcation overlap between this activity and standingattributed to the physical location of the device and its difﬁculty to categorize them: fu-ture works will have to investigate the necessary steps in order to improve the discrim-ination of these non-dynamic activities (e.g. introduction of new features, for examplederived by gyroscopes).\n\n4Conclusions\n\nIn this paper we introduced a new publicly available dataset for HAR using smartphonesand acknowledged some results using a multiclass Support Vector Machine approach.The multiclass SVM employed for the classiﬁcation of smartphone inertial data showeda recognition performance similar to previous work that have used special purpose sen-sors, therefore strengthening the application of these devices for HAR purposes. Wealso highlighted an improvement on the classiﬁcation performance of the learned modelusing this new dataset against the previous version, which had a reduced set of features.However, rooms for improvements exist: while dynamic activities can be efﬁcientlyclassiﬁed thanks to the newly introduced features in the released dataset, non-dynamicactions still present misclassiﬁcation overlaps. This requires further study of availableinputs and revision of the HAR process pipeline phases. Finally, computational com-plexity aspects such as battery life and real time processing for the application will beassessed in our forthcoming works.\n\nAcknowledgments. This work was supported in part by the Erasmus Mundus JointDoctorate in Interactive and Cognitive Environments, which is funded by the EACEAAgency of the European Commission under EMJD ICE FPA n 2010-0012.\n\n441\n\nESANN 2013 proceedings, European Symposium on Artificial Neural Networks, Computational  Intelligence and Machine Learning.  Bruges (Belgium), 24-26 April 2013, i6doc.com publ., ISBN 978-2-87419-081-0. Available from http://www.i6doc.com/en/livre/?GCOI=28001100131010.\n\nReferences\n\n[1] R. Poppe. Vision-based human motion analysis: An overview. Computer Vision and Image Under-standing, 108(1-2):4–18, 2007.\n\n[2] P. Lukowicz, J.A. Ward, H. Junker, M. St¨ager, G. Tr¨oster, A. Atrash, and T. Starner. Recognizing work-shop activity using body worn microphones and accelerometers. Proceedings of the 2nd Int ConferencePervasive Computing, pages 18–22, 2004.\n\n[3] D.M. Karantonis, M.R. Narayanan, M. Mathie, N.H. Lovell, and B.G. Celler. Implementation of areal-time human movement classiﬁer using a triaxial accelerometer for ambulatory monitoring. IEEETransactions on Information Technology in Biomedicine, 10(1):156–167, 2006.\n\n[4] R. Nishkam, D. Nikhil, M. Preetham, and M.L. Littman. Activity recognition from accelerometerdata. In Proceedings of the Seventeenth Conference on Innovative Applications of Artiﬁcial Intelligence,pages 1541–1546, 2005.\n\n[5] L. Bao and S.S. Intille. Activity recognition from user-annotated acceleration data. In T. Kanade,J. Kittler, J.M. Kleinberg, F. Mattern, J.C. Mitchell, O. Nierstrasz, C. Pandu Rangan, B. Steffen, D. Ter-zopoulos, D. Tygar, M.Y. Vardi, and A. Ferscha, editors, Pervasive Computing, pages 1–17. 2004.\n\n[6] J.R. Kwapisz, G.M. Weiss, and S.A. Moore. Activity recognition using cell phone accelerometers.SIGKDD Explorations Newsletter, 12(2):74–82, 2011.\n\n[7] T. Brezmes, J.L. Gorricho, and J. Cotrina. Activity recognition from accelerometer data on a mobilephone. Distributed Computing, Artiﬁcial Intelligence, Bioinformatics, Soft Computing, and AmbientAssisted Living, pages 796–799, 2009.\n\n[8] W. Wu, S. Dasgupta, E.E. Ramirez, C. Peterson, and G.J. Norman. Classiﬁcation accuracies of physicalactivities using smartphone motion sensors. Journal of Medical Internet Research, 14(5), 2012.\n\n[9] M.F.A. bin Abdullah, A.F.P. Negara, M.S. Sayeed, D.J. Choi, and K.S. Muthu. Classiﬁcation algorithmsin human activity recognition using smartphones. International Journal of Computer and InformationEngineering, 6:77–84, 2012.\n\n[10] D. Roggen, A. Calatroni, M. Rossi, T. Holleczek, K. F¨orster, G. Tr¨oster, P. Lukowicz, D. Bannach,G. Pirkl, and A. Ferscha. Collecting complex activity data sets in highly rich networked sensor envi-ronments. In Proceedings of the 7th International Conference on Networked Sensing Systems 2010,2010.\n\n[11] E.M. Tapia, S.S. Intille, L. Lopez, and K. Larson. The design of a portable kit of wireless sensors fornaturalistic data collection. In Proceedings of PERVASIVE 2006, pages 117–134, 2006.\n\n[12] S. Dernbach, B. Das, N.C. Krishnan, B.L. Thomas, and D.J. Cook. Simple and complex activity recog-nition through smart phones. In 2012 8th International Conference on Intelligent Environments, pages214–221, 2012.\n\n[13] C. Cortes and V. Vapnik. Support-vector networks. Machine learning, 20(3):273–297, 1995.\n\n[14] C. BenAbdelkader, R. Cutler, and L. Davis. Stride and cadence as a biometric in automatic personidentiﬁcation and veriﬁcation. In Proceedings of the Fifth IEEE International Conference on AutomaticFace and Gesture Recognition, pages 372–377, 2002.\n\n[15] J.Y. Yang, J.S. Wang, and Y.P. Chen. Using acceleration measurements for activity recognition: Aneffective learning algorithm for constructing neural classiﬁers. Pattern recognition letters, 29(16):2213–2220, 2008.\n\n[16] A.M. Khan, Y.-K. Lee, S.Y. Lee, and T.-S. Kim. Human activity recognition via an accelerometer-enabled-smartphone using kernel discriminant analysis. In Proceedings of the 5th International Con-ference on Future Information Technology, pages 1–6, 2010.\n\n[17] A. Frank and A. Asuncion. UCI machine learning repository, 2010.\n\n[18] Y. Hanai, J. Nishimura, and T. Kuroda. Haar-like ﬁltering for human activity recognition using 3daccelerometer.In Digital Signal Processing Workshop and 5th IEEE Signal Processing EducationWorkshop, 2009. DSP/SPE 2009. IEEE 13th, pages 675 –678, jan. 2009.\n\n[19] D. Anguita, A. Ghio, L. Oneto, X. Parra, and J.L. Reyes-Ortiz. Human activity recognition on smart-phones using a multiclass hardware-friendly support vector machine. In Proceedings of the Interna-tional Workshop of Ambient Assited Living, 2012.\n\n442\n\nESANN 2013 proceedings, European Symposium on Artificial Neural Networks, Computational  Intelligence and Machine Learning.  Bruges (Belgium), 24-26 April 2013, i6doc.com publ., ISBN 978-2-87419-081-0. Available from http://www.i6doc.com/en/livre/?GCOI=28001100131010.",
  "needs_ocr": false
}